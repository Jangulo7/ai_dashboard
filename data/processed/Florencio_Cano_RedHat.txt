# Master Class Master Class. Florencio Cano - RedHat-20250709_183602-Grabación de la reunión transcript # Transcripción generada automáticamente por OpenAI Whisper # Archivo original: Master Class. Florencio Cano - RedHat-20250709_183602-Grabación de la reunión.mp4 (759.8 MB) # Procesado en 32 segmentos el 2025-07-12 13:10:12 # Segmentos transcritos exitosamente: 32/32 # Segmentos fallidos: 0 # Cobertura estimada: 100.0% Vosotros por chat, por temas personales y tal. Y va, no quiero que nadie se conozca, porque yo es que vengo, voy casi siempre menos este día que no pude ir. A ver, las clases son presenciales, excepto los miércoles, que son online. Este módulo en concreto es un módulo donde estamos trayendo profesores de muchos sitios. Es necesario que vengáis. No es un tema de un, pues si puedo voy. No, no. Es un esfuerzo extra y por venir. Y os digo más, el viernes, hasta hoy mismo a las 4 de la tarde, la jueza que iba a venir me ha dicho que no sabía si podía, que tenía que mover una cosa, que ya le he dicho, bueno, vamos a ver lo que puedes hacer. Y al final va a venir, llega el juez a las 12 de la noche, nos da la clase y se va, la clase se da hasta las 7 y media. Y a las 7 y media coge otro avión para volverse a Canarias, porque viene de Canarias. Es decir, si realmente el viernes hay cuatro personas, yo creo que ya es para hacérnoslo mirar seriamente, porque después de todo el esfuerzo que estamos haciendo y el que están haciendo, ya te digo, es una jueza especializada en temas de IA, que ha escrito varios artículos de IA y que viene de Canarias exclusiva solo a dar la clase. Yo voy a organizar una reunión aparte para que al menos, antes de nada, para que hable con la gente de la sindicatura, con abogados y tal, con temas de IA y por hacerle un poco la pelota entre nosotros. Se está grabando, pero ella no lo va a ver. Por hacerle un poco la pelota habría que hacer esfuerzo y va a cobrar lo mismo que cualquier otro. Es decir, no le pagamos el viaje ni tal. Yo le dije lo que había y se ha ofrecido. Con lo cual, os pido por favor que el viernes hagáis, el viernes y el sábado, evidentemente, pero el viernes en concreto hagáis un esfuerzo extra para venir, ¿vale? Y también por avisaros, acabará a las siete y media porque la llevaré al aeropuerto. ¿Vale? ¿Vale? Más cosas. Que alguno me lo habéis preguntado también. La idea es que, como sabéis, acabamos este sábado con este máster, con este módulo, perdón. Entonces, tenéis una semana para entregármelo. Aquellos que tengáis, porque me habéis dicho, ostras, estoy con la presentación y tenéis toda la razón. Y yo en esto aprieto pero no ahogo. Desde el principio os lo dije. Os doy una semana más, ¿vale? Es decir, y las notas las pondré a la semana siguiente. Es decir, mira, os lo digo exactamente. Había dicho que me lo teníais que entregar el 18. Me podéis entregar el resumen de las ponencias junto con una evaluación personal de la que más os ha impactado para vuestro CFM, para vuestro trabajo, lo que sea, el 25. ¿Vale? Repito, ampliamos una semana hasta el 25 para que podáis respirar un poco y os centréis en el trabajo final de máster, en la presentación, porque os lo jugáis todo a esta. Aquí vamos a valorar muy mucho la presentación. Yo ya he estado hablando con Radu cómo enfocarla y tal. Os aconsejo que con vuestro responsable TFM os juntéis y le hagáis la presentación a él o a ella, ¿vale? ¿Dudas, consultas? ¿No? Pues, nada, Florencio, todo tuyo. Gracias por estos minutos. La idea es hacer un descanso más o menos como hacemos siempre, ¿no? A las 8 por ahí y parar unos 15 minutillos y luego acabar sobre las 9 o por ahí, ¿vale? Una cosa que también me ha dicho Florencio, perdonad, sí que si vais a hablar tal, sí que mejor levantar la mano y si él no lo ve, yo os iré dando paso, ¿vale? Porque no sé si tiene 2 pantallas. ¿Tienes 2 pantallas, Florencio? Sí, sí que tengo 2. Creo que lo podría ver. Si no, yo te comentaría, ¿vale? Y lo dicho, ya sabéis que a mí me gusta que os pongáis la cámara, aunque sea en online, pero si vais a hablar, ponedos la cámara. Si vais a preguntar y no os dejéis de preguntar por no poneros la cámara, que yo no sé qué manía tenéis todos de estar con las letritas, ¿vale? Tampoco os voy a coger del cuello para que lo pongáis. ¿Cómo os cuesta, macho? Vale, eh, Florencio, todo tuyo. FLORENCIO DÍAZ-SÁNCHEZ Muy bien, eh, os he compartido la pantalla, ¿podéis ver ya? FLORENCIO DÍAZ-SÁNCHEZ Sí. FLORENCIO DÍAZ-SÁNCHEZ La pantalla. OK. Vale, pues, nada, voy a hablar de seguridad, de seguridad del software, ¿vale? Yo he visto también la presentación de Jesús y yo creo que no hay prácticamente ningún solape, solamente el tema de la seguridad de fondo. Bueno, yo soy Florencio, trabajo en Red Hat, os voy a contar un poco de mí y también un poco el enfoque de la charla relacionado con esto, ¿vale? Ahí también tenéis mi correo personal que si me queréis enviar algún correo en algún momento, alguna consulta, alguna duda, algo que os pueda aportar, pues, no lo dudéis de escribirme que, vamos, me gusta, ¿vale? Bueno, yo estudié ingeniería superior en informática en la Universidad Politécnica de Valencia. Me gusta la seguridad desde, vamos a decir, casi siempre, desde el 98, el 99. ¿Valencia? Se cayó. Uy, se ha quedado frito, espero que no se ha quedado. No estamos escuchando el audio. No, no, se ha quedado frito, ¿no? Vosotros no escucháis nada, ¿no? Se quedó... Sí, se quedó... Está fuera. A ver si lo han hackeado. Ahora. ¿Ahora me oís? Sí, sí, sí. Estaba diciendo que ves si te habían hackeado. Pero se había cortado, se ha cortado. Vale, vale. Pero sí, ahora me oís bien. Te oímos perfecto y te vemos bien. Ah, vale, vale. Entonces, ¿y ahora? ¿Y se había cortado antes? ¿No? ¿Sí? Sí, sí, sí, sí. Ah, yo no he notado nada de que se hubiera cortado. Sí. Pues vale, rápidamente lo... lo retomo, ¿vale? Florencio, estudié Ingeniería Superior en Informática. A mí me gusta la seguridad desde antes de entrar en la carrera, ¿vale? Ya eso, vi unas guías que se llamaban guías del hacker mayormente inofensivo. Me hicieron gracia y a partir de ahí aprendí Linux, aprendí... ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? ¿Qué? No sé, estábamos enseñando como es un poco la tecnología que estoy utilizando, ¿no? Y a mí me encanta... Bueno, y luego esos productos que decía eran como más pequeños, pero luego rejas sobre todo donde se ve es en proporcionar el software donde se van a ejecutar los modelos, ¿no? Los modelos, pues, en local desde el punto de vista que sea. Es decir, puede ser en una instancia de una empresa en la nube o en sus servidores, en su CPD, pero los va a ejecutar sobre algo. Y, entonces, está Red Hat Enterprise Linux, que es el sistema operativo ya con los drivers optimizado para poder hacer inferencia de modelos. O OpenShift AI, que es, pues, el clúster de Kubernetes ya con herramientas, optimizado para ejecutar modelos en un clúster. Entonces, yo lo que me preocupo en mi día a día es de hablar con estos equipos, entender los riesgos de seguridad que hay, aconsejarles o pedirles que hagan las cosas de una determinada manera, sobre todo desde el punto de vista de la inteligencia artificial y para que esos productos, pues, sean lo más seguros posibles para nuestros clientes o para los usuarios, ¿no? Una cosa interesante también que no, o sea, yo, por ejemplo, no era, lo había oído, pero yo no era consciente hasta antes de entrar en Red Hat. Todos los productos de Red Hat realmente son de código abierto, son open source. Todos estos productos, hay repositorios en GitHub que están ahí. Otra cosa es que el display, claro, Red Hat los paquetiza y los presenta de una forma en los repositorios, que también comentaré un poco, luego están relacionados con la seguridad, pero el software este es abierto. Entonces, yo trabajo en una empresa que desarrolla software o trabaja con comunidades que desarrollan software y luego ese software les hace cosas, los mantiene, les puede hacer añadidos, los paquetiza y los provee a clientes. Y, entonces, ahora esos productos tienen inteligencia artificial o se basan en inteligencia artificial, ¿vale? Entonces, dice, oye, la seguridad, tú te preocupas de la seguridad de estos productos. Entonces, yo cuando empiezo a pensar en la seguridad de la inteligencia artificial, empiezo a ver que hay distintas, o sea, que se le llama seguridad de la inteligencia artificial, pero hay distintas patas o distintos aspectos de la seguridad de la inteligencia artificial. Entonces, hay 2 muy claros, que uno es la seguridad de la inteligencia artificial, que la inteligencia artificial se asegura y otro es usar la inteligencia artificial para mejorar la seguridad, ¿vale? Hay 2 patas bien diferenciadas, pero luego dentro de la seguridad de la inteligencia artificial hay varias cosas. Una preocupación es o un aspecto es que, vamos a decir, los malos, los atacantes, no ataquen nuestra inteligencia artificial, o sea, nuestro software que tenemos haciendo inteligencia artificial, pues, que no lo ataquen desde el punto de vista de inteligencia artificial, que no lo ataquen usando la inteligencia artificial, ¿no? O problemas en la inteligencia artificial. Y luego una empresa que desarrolla software en general, pues, que cuando nuestros clientes o nuestros usuarios usen este software, que no tengan problemas de seguridad o que sean mínimos cuando usen la parte de inteligencia artificial del software, ¿vale? Eso sería la primera parte o un aspecto. Otro aspecto es que los malos no usen la inteligencia artificial para hacer cosas malas, ¿no? Es decir, no usen la inteligencia artificial para construir una bomba, ¿no? Por ejemplo, el típico ejemplo. Eso sería otro aspecto. Y luego otro aspecto que es parecido al primero, pero tiene matices, que es que por culpa de usar inteligencia artificial tengamos problemas de seguridad. Para mí un ejemplo es el hecho de que una inteligencia artificial genere código que tiene vulnerabilidades de seguridad, ¿vale? En el primer caso, que los malos no ataquen nuestra IA, los malos pueden atacar nuestra IA o pueden conseguir algo de nuestra IA si tiene una vulnerabilidad de seguridad que hay que corregir y se puede corregir. Ese es el primer aspecto. Y ya os anticipo que este primer punto es donde yo voy a centrar la charla y donde yo estoy más especializado. Mi día a día está en este primer punto. En esta charla que hicimos al principio del máster, me centré más en el tercer punto, ¿no? En el problemas de seguridad derivados de usar la IA. Y luego el último punto, lo que os comentaba, ¿no? IA para la seguridad, para mejorar la seguridad. Bueno, pues nos vamos a centrar, me voy a centrar hoy en yo, que a mí me mandan o me pongo en los zapatos de alguien que está en una empresa y se tiene que preocupar por la la seguridad de su software, tanto porque lo va a usar como porque lo está desarrollando, su empresa lo desarrolla para sus usuarios o sus clientes, ¿no? Entonces, voy a hablar alrededor de esa preocupación o de esos riesgos. Entonces, hay un aspecto importante que quiero comentar para introducir luego cómo llego a los riesgos de seguridad de la IA, ¿no? Que es la superficie de ataque, que seguramente conocéis el concepto, ¿vale? Y si no, pues, lo comento. Que es la superficie de ataque es un concepto muy importante cuando hablamos de seguridad y muy importante desde el punto de vista de la persona. Porque sabéis que en seguridad muchas veces en las empresas se distinguen 2 equipos grandes, ¿no? Hay un equipo que ataca, ¿no? Que le suelen llamar el red team, que ataca y de esa forma demuestra que hay fallos de seguridad en la empresa que se tienen que corregir. Y luego hay un equipo que les suelen llamar el blue team que es el que defiende, ¿vale? Que es el que pone medidas de seguridad para que los malos no puedan atacarlo. Entonces, la superficie de ataque, por supuesto, desde el punto de vista del atacante es importante, pero desde el punto de vista del defensor es fundamental porque es lo que el defensor, donde el defensor se tiene que fijar para poner medidas de seguridad. La superficie de ataque es todos esos puntos de interacción que un malo tiene para interactuar con un sistema, ¿no? Y el ejemplo que se pone muchas veces es el de una, imaginemos, un chalet, ¿no? Una casa, una montaña. Dime, ¿sí, Jenni? JENNI FUENTES DE INTÉRPRETE 1. Disculpa, no sé si cambiaste la diapo, pero al menos tu cámara está freeze. MIGUEL BURGUETE. ¿La cámara o la diapositiva? JENNI FUENTES DE INTÉRPRETE 1. Es que no sé si cambiaste la diapo. MIGUEL BURGUETE. La cámara está congelada, sí. JENNI FUENTES DE INTÉRPRETE 1. Claro, ¿tu cámara? Entonces, no sé si también la diapo, porque ahí no sé identificar si la diapo está cambiada o no. Porque estamos viendo. MIGUEL BURGUETE. La diapositiva habla de la superficie de ataque. MIGUEL BURGUETE. No, no la vimos. JENNI FUENTES DE INTÉRPRETE 1. No, no estaba viendo, sí. MIGUEL BURGUETE. ¿No? MIGUEL BURGUETE. Sí, se ha congelado. MIGUEL BURGUETE. No. JENNI FUENTES DE INTÉRPRETE 1. Mmm, vale, pues, a ver. Es raro que vaya el vídeo y no la diapositiva, ¿no? MIGUEL BURGUETE. No, el vídeo no va, ¿eh? O sea, no, no. MIGUEL BURGUETE. No, el vídeo tampoco, pero el audio, vale. MIGUEL BURGUETE. El audio sí, pero a ti te hemos congelado. Tiene que ser algo de la propia conexión o el propio Teams. JENNI FUENTES DE INTÉRPRETE 1. A lo mejor te conviene no poner el vídeo, solo audio y la diapo. MIGUEL BURGUETE. El Teams, sí. MIGUEL BURGUETE. Espera un segundo, voy a parar de compartir, un segundo. MIGUEL BURGUETE. Ana, ya te veo dentro, que no te veía. Una aventura con el Teams, ¿eh? A ver, voy a cambiar la conexión, igual me desconecto, si es así me vuelvo a conectar, ¿vale? A ver. ANA MARTÍNEZ SANTOS. Sí, yo no sé qué pasó con el Teams, pero me botó y no me dejaba iniciar. Lo más gracioso es que el Teams como chat sí me funcionaba, el calendario sí me funcionaba, veía la reunión, pero al hacer clic no se quedaba. De hecho, todavía lo tengo aquí, el Teams. Está como dando vuelta, como que está trabajando, pero no conecta, no sé. MIGUEL BURGUETE. No quiere trabajar. ANA MARTÍNEZ SANTOS. Hace mucho calor. MIGUEL BURGUETE. Se quiere ir de vacaciones como los de OpenAI, que les han obligado ahora a tener una semana de vacaciones. ANA MARTÍNEZ SANTOS. Ah, ¿sí? Ah, porque yo creo que van a sacar un anuncio potente. MIGUEL BURGUETE. No, lo que pasa es que se están yendo todos a meta y les ha fichado 8 de los investigadores más importantes y están haciendo jornadas de 80 horas a la semana. Y han dicho, ostras, se me está yendo la gente, pues voy a ver si los cuido un poco. ANA MARTÍNEZ SANTOS. Es que ni Cristiano ni Messi ganan así. MIGUEL BURGUETE. Se han ido con alguno con 100 millones de dólares, pero ¿eso es la verdad? ¿100 millones de dólares? ANA MARTÍNEZ SANTOS. Sí, el mismo Sam Alman lo dijo. MIGUEL BURGUETE. 100 millones de dólares, pues por un investigador, que me parece bien que gane más, sinceramente. A mí, por primera vez en la historia de la humanidad, me parece bien que alguien que se dedique a investigar gane más dinero que uno que le pegue un balón, ¿vale? Por muy bien que le pegue, que yo a Messi lo quiero mucho. ANA MARTÍNEZ SANTOS. Yo escuché que eso era como una estrategia de marketing o una estrategia para evitar que los demás investigadores se les vayan. Entonces, claro, si tú dices que los que ya se fueron los contrataron por 100 millones, tú no vas a querer que te den menos. MIGUEL BURGUETE. O sí, dame 5. ANA MARTÍNEZ SANTOS. Dame 5, vale. MIGUEL BURGUETE. Hay veces que es dame 5 y en lugar de trabajar 80 horas, que trabajes 60. ANA MARTÍNEZ SANTOS. Fue agridulce porque yo decía, cuando tuve unas decepciones con el tema de investigación hace poco, y decía, pero qué horrible, qué asco uno dedicándose con tantas horas de trabajo para nada. Y después veo esta noticia y digo, bueno, quizá me tengo que ir a Estados Unidos. MIGUEL BURGUETE. Yo estuve investigando. ANA MARTÍNEZ SANTOS. Y si pagan por investigar. MIGUEL BURGUETE. Estuve investigando también temas de gestión inteligente del tráfico. No tenía nada que ver con inteligencia artificial. Pero bueno, estábamos haciendo un protocolo nuevo para, bueno, en caso de accidente, hacer ciertos temas. Y claro, yo fui con una promesa de un contrato de año y medio ampliado a dos años. Empecé con contratos de un mes, porque si no, llevaba la financiación y dije, pero me han dejado el otro trabajo que tengo para alquiler. Si no llegara el dinero, ¿qué hago? ¿Cómo vivo yo? Y nada, aguanté. Al final siempre me pagaron todos los meses, pero por esa incertidumbre. Y a mí en el mundo de la investigación me encanta. Pero dije, mira, me voy a la empresa privada, que tendré un poquito más de estabilidad y nada, la verdad que bien. Yo también. Me pasó igual. Yo siento que mi pasión siempre ha sido investigar. Y de hecho, estuve trabajando en una universidad años atrás y ellos me ofrecieron la cátedra ya definitiva, pero vi lo que pagaban y vi lo que me ofrecían en el mercado y fue como mira, me gusta mucho, pero tengo que vivir una vida. No, no, es que es así, es así, es que no. Claro, dicen que si no llega a la financiación, dice ya, pero si no llega a la financiación, eso lo tienes que decir al principio del contrato. O sea, cuando vas a contactar con el otro Camilas, no? No, y el tema es que yo quería investigar, pero me ofrecían la cátedra, pero la cátedra es para que tú hagas docencia. Pero los proyectos de investigación había que postular aparte y a ver si te llegaba algo alguna vez. Entonces dije bueno, si no voy a estar haciendo lo que me gusta, porque voy a estar trabajando por un sueldo que es menos del tercio de lo que me pagan en el privado. Entonces al final me fui al mundo privado. Pero ahora me gustaría volver a la investigación ahora que ya viví todo lo privado. Y me gustaría hacer el camino contrario. Claro que lo veo difícil ahora. Perdón, ahora me veis o tampoco sigue congelado. Ahora sigue congelado y no vemos la pantalla. Tampoco estás compartiendo, pero se ve en blanco, completamente en blanco. Qué raro. Pues puedes abrir Tims desde el navegador. No tengo. Ese es el navegador. Ese es el navegador. Y reiniciar. Ha reiniciado ya? Sí, pues voy a. No, lo que he probado es otra conexión, pero voy a probar a salir y volver a entrar. Vale. Pero además te has quedado así con la boquita de piñón. Así yo noté que había algo raro porque cuando empezó con la parte de superficie ataque y yo de estar mostrando algo y sí, pues si nosotros pasamos una incidencia, lo que contaba antes aprovecho también para contarlo. Yo diseñé una parte diseñé e implante una parte de seguridad de certificados de mercado donde al final todos los dispositivos finales en los almacenes, los que se utilizaban para hacer el piquín y todo esto. Descargamos unos certificados y la propia wifi va con un certificado vinculado al dispositivo. Eso implante una PKI y todo esto. Cuando empezamos no se veía nada. Mira, ya estás, Florencia, ya te vemos bien. Y al final tenían un certificado, digamos raíz caduca. Entonces si caducaba y no se hacía bien el renove, pues se podían bloquear todos los terminales. Y justo cuando salía al poco tiempo, pues bueno, un tema de un refresco no se creía que había empezado a petar, claro, los almacenes se paran. O sea, fue. Ya hemos hablado, Florencia, de esto varias veces antes de que yo me fuera de ahí, porque esto lo implante como cuatro años antes de entrar en ciber, tres años antes de entrar en ciber. Implante desde la parte de gestión de proyectos de infraestructura y fue. Pues sí, sí, sí. La famosa PKI. Desde entonces no se me ha olvidado y luego tuve una incidencia muy, muy gorda que ahí Florencia y yo estaba coordinando un equipo y ahí Florencia y todo su equipo de ciber vino y nos ayudó donde se mezclaban las nóminas. Fue gordísimo. Eso sí que es grande. Sí, pero pero no que se mezclaban de cara a que cada uno cobrara, sino que tú veías en tu nómina, podías ver la tuya y la de otro. O sea, tú cobras lo que tenías que cobrar, pero podías ver. Pero claro, datos privados y personal. Y las envidias en el mundo laboral, Ego Rius. En el Mercadona pasa una cosa que es que Mercadona pasa que toda la gente al mismo o sea a un nivel y es el 90 por ciento de la gente, el 99 por ciento de la gente cobra lo mismo al mismo nivel. Eso está muy bien. Entonces una persona de un rango C el primer año cobra lo mismo que otro C de primer año o C de segundo año, ahí hasta 5 a 5 escalones en cada grupo. Entonces en esa parte de ver el sueldo de otro, en ese sentido no era tan problemático, pero estaba hecho en sí. Y cuentas también. Bueno, había y todo era porque calculaban al final vinculaban el PDF de la nómina con el momento en el que digamos tú pedías que se imprime ese con el instante el segundo. Qué pasa? Que claro, eso pasó de pedir una al segundo a pedir 100 al segundo que era la encina. A lo mejor eran 10, pero ya se mezclaban todas. Y eso es porque se cambió el servicio de arriba y atacaba a la misma. Entonces nadie previ. Nadie predijo o tuvo la previsión de decir, vamos a revisar el proceso de cabo a rabo para ver cómo se está calculando guardando el pdf, porque al final el propio webservice cuando recuperaba el pdf, claro, lo recuperaba junto, pero bueno, ya está. Estaba contando un poco de esa experiencia, que son las dos incidencias más gordas que hemos tenido, creo yo, en nuestra vida y vamos tranquilo, ya se te ve bien. Esa como incidencias, pero como problemas muchos, pero bueno. Eso es lo que nos curte, por eso estamos curtidos. Vale, pues entonces, ahora veis la transparencia de la superficie de ataque. Vale, pues lo comentaba, la superficie de ataque son estos puntos donde el malo iba a poner este ejemplo de la casa, es decir, hay una casa, queremos entrar a robar y entonces la casa tiene puertas, tiene ventanas, esas puertas y esas ventanas son los puntos de interacción del malo, por donde podría entrar y el defensor, pues lo que conociendo esa superficie de ataque, lo que tiene que hacer es, pues en la puerta la voy a proteger así, voy a poner una cerradura de este tipo y la puerta va a ser de este tipo para que no se pueda caer, no la puedan tirar, en las ventanas voy a poner rejas o voy a poner detectores de no sé qué. Entonces, eso es lo que se hace también con un sistema informático, una página web, donde el malo puede interactuar con la página web o con un sistema de IA. Entonces, bueno, aquí digo, ¿qué pasa si el riesgo, o sea, qué pasa con el riesgo si la superficie de ataque es grande? Hay más riesgo, independientemente de la seguridad que haya, cuantas más ventanas, más puertas, en principio más riesgo. Y lo contrario si es pequeña, la superficie de ataque cuanto más pequeña, pues más seguro. Pero, ¿podría existir un sistema sin superficie de ataque? No, eso sería lo ideal desde el punto de vista de seguridad, pero claro, una casa sin puertas ni ventanas, pues no sería muy usable, ¿no? Entonces, no se puede, no es el problema que tenemos en seguridad, que las cosas tienen que funcionar, la IA tiene que funcionar, los sistemas que usan IA tienen que funcionar, pero hay que reducir la posibilidad de que un malo las ataque. Hay que reducir la superficie de ataque y luego hay que proteger cada uno de esos puntos de entrada. Entonces, yo cuando me planteo esto, veo 3 escenarios, ¿vale? Ahora lo veis los cambios, ¿verdad? Veis la slide con los escenarios, ¿vale? Pues, entonces, hay 3 escenarios que el primero es, también lo podemos ver como 3 personas, ¿no? La primera persona es el que va a crear un modelo de inteligencia artificial desde cero, ¿no? Nos ponemos en los zapatos del ingeniero de meta que va a desarrollar la nueva versión de Lama. El segundo escenario es el desarrollador que hace una aplicación y en esa aplicación incluye modelos, ¿vale? Porque esto, quiero decir, esto al principio es una confusión clara, vosotros que ya estáis con todo este tiempo viendo inteligencia artificial, ChatGPT no es un modelo, ChatGPT es una aplicación que usa un modelo, incluso tú puedes elegir usar otro modelo, ¿no? Entonces, se hacen aplicaciones que usan modelos. Este es el segundo escenario. Y luego el tercer escenario es otro distinto, es el escenario en el que, por ejemplo, una empresa se baja Lama y lo ejecuta en local o en su instancia en la nube. Es decir, hace uso de modelos o de aplicaciones que usan modelos. Entonces, cada uno tiene unos riesgos diferentes. Desde el punto de vista del malo, se diría que yo como hacker, para hackear al creador de modelos voy a pensar en atacarle de una manera, al desarrollador de aplicaciones le voy a atacar de otra manera y a los usuarios o empresas que ejecutan modelos les voy a atacar de otra manera, ¿vale? Entonces, yo ahora me pongo en los zapatos de la persona que yo tengo esta preocupación, ¿no? Porque hago ese software para estas personas a ver cómo, qué es lo que un malo podría hacer y cómo reduzco la probabilidad de que eso pase, ¿vale? Y, idealmente, cómo la elimino. Pero en seguridad es muy difícil eliminar la probabilidad, es muy difícil reducir a cero la probabilidad de que pase algo. Siempre existe alguna probabilidad, ¿no? Pero lo que tratamos es de aplicar medidas de seguridad para reducir esta probabilidad. Entonces, ahora pensamos en cada rol. El rol del creador de modelos, pues, ¿qué hace? Se baja a librerías, ¿no? De internet, ¿no? Se baja a PyTorch, se baja a TensorFlow. el Lanchain, Spacey, ¿no? Se baja a librerías, las ejecuta el local para construir su modelo y también, sus modelos y también utiliza programas, ¿no? IDEs de desarrollo, por ejemplo, Visual Code Studio, Cursor, ¿vale? Lo que sea utiliza esto para crear los modelos. Entonces, ya vemos que un atacante lo que intentaría sería, en estas librerías, introducir puertas traseras, atacar los repositorios donde se almacenan estas librerías, para que cuando se las baje el malo lo tengamos, o encontrar fallos de seguridad en ese software, ¿no? Un fallo de seguridad en Visual Code Studio, que cuando con el plugin de GitHub van a un repositorio que yo he construido, se explota una vulnerabilidad y accedo a sus ficheros, por ejemplo, ¿no? Entonces, encuentro vulnerabilidades en este software y, además, intento poner puertas traseras en este software. Así es como atacaría al desarrollador, al creador de modelos, al desarrollador de aplicaciones. Algo parecido, aquí lo que hace el desarrollador de aplicaciones es que ya se bajan modelos preentrenados, ¿no? Se bajan modelos de Hacking Face o se bajan modelos del catálogo de Olama o de otros sitios. Se bajan modelos y los alinea, le hace fine tuning para su aplicación, ¿no? Y luego, por ejemplo, pues, puede que coja estos modelos alineados y los sube a Hacking Face, ¿no? Hacking Face hay muchos modelos que se han, si les ha hecho fine tuning y están basados en otros modelos, ¿no? Se coge un modelo de Lama o un modelo Mistral y se les hace fine tuning y hacen una cosa concreta, ¿no? Pues, esto es lo que hace este perfil. Entonces, ¿qué necesita esta persona? ¿Dónde lo podríamos atacar? En que esos modelos que se descarga sean seguros, ¿no? Que no se baje un modelo de Hacking Face o de Olama, que tenga malware, ¿no? Al final, en Hacking Face cualquiera puede subir un modelo. Pues, te lo bajas y puede tener malware ese modelo, puede tener una puerta trasera, puede tener código malicioso. Hay un millón o casi 900,000 modelos subidos a Hacking Face. Que no sé quién los revisará, pero, vamos, en el tiempo que llega a Hacking Face, es impresionante. Pues, voy a hablar de eso. Una parte importante de la charla va de la seguridad de esos modelos, ¿no? Y cogiendo mucho el ejemplo de Lama y de Hacking Face. Porque, y lo introduzco ya, ¿no? Ya que estamos hablando de esto. Yo me encontraba en foros de expertos de seguridad una cosa que era que se decía, es que los modelos son solo pesos. Los modelos son solo pesos, son solo datos, ¿no? Floats o enteros, floats, ¿no? Y, entonces, ahí no puede haber nada malo. No te preocupes, te puedes bajar un modelo que ahí no hay riesgo. Entonces, a mí eso, claro, a ver, cuando empiezas nadie nace aprendido, ¿no? Pero eso me chirriaba, ¿no? Decía, no, no, no me acaba de cuadrar, ¿no? Porque si los modelos fueran solo pesos, y yo sé que un modelo de estos, Large Language Model, es como un conjunto de Deep Neural Networks, ¿no? Que pueden tener, además, distintas arquitecturas, ¿no? Distintas capas, distintas, claro, si solo son pesos, es que todos los modelos son iguales, ¿no? Y solo cambia el peso. Y dices, yo creo que no, ¿no? Yo leía y decía, oye, es que el de Granite de IBM tiene tantas capas y están interconectadas así y el otro usa otra conexión así, no parece que sean iguales. Y, además, pensaba otra cosa. Si son todos iguales, ¿por qué Olama, este software para ejecutar modelos, no puede ejecutar el último modelo que ha salido? Si es igual que todos los demás, que son solo los pesos, pues, cualquier software de estos podría ejecutar cualquier modelo. Entonces, ahí es donde empezó una investigación. Y también la muestro, creo que es interesante, para conocer más estos modelos que nos bajamos de internet. Y también para que veáis desde donde una preocupación por la seguridad de esos modelos. Y algo que la propia gente de seguridad está diciendo, si tienes curiosidad y rascas, te das cuenta que no es así. No es lo que se está diciendo. Ya hay más donde investigar. Entonces, hay riesgos en los modelos que te bajas, ¿no? Y ahora lo veremos. También cuando salió DeepSeek, se habló otra cosa que es, oye, cuidado que DeepSeek te roba los datos, ¿no? Como viene de China y tal, que eso es como la otra banda, ¿no? Pasamos de decir que no hay que preocuparse por nada a que te roban los datos del ordenador. Entonces, hablaré un poco de. de eso. Porque también era una preocupación para mí, ¿no? Red Hat también trabaja en proyectos de, en registros de modelos, ¿no? Una especie de hacking face open source, ¿no? Que tú te puedes bajar y tener en local, ¿no? Para que una empresa tenga sus modelos ahí en local. Pues, entonces, ¿qué pasa con esos modelos que subes ahí? Bueno. Y luego, la última persona es las empresas que están ejecutando estos modelos, que tienen ese mismo riesgo, ¿no? Se bajan modelos de sitios y los ejecutan en local. ¿Tienen riesgo o no tienen riesgo? ¿Por culpa de yo bajarme un modelo malo me pueden hackear o realmente no? ¿O que el propio modelo puede tener un malware, el modelo? O sea, yo me puedo bajar un virus con un modelo. ¿O otra cosa? Si yo cargo un modelo en Olama, ¿puedo romper Olama? ¿Puede ser que yo, es como, si yo abro un fichero Excel malicioso, puedo estar ejecutando un virus? Si yo abro un modelo malicioso con Olama, ¿puedo estar ejecutando un virus? Pues, eso es lo que yo me preguntaba, ¿no? Entonces, bueno, hemos visto 3 personas y 3 grupos de preocupaciones distintas, ¿vale? Preocupación por la seguridad de las aplicaciones, Visual Code Studio, por ejemplo, de las dependencias de PyTorch, TensorFlow, hablaré un poquito de eso. La seguridad de los modelos en sí, de los modelos que nos bajamos, que es lo que más voy a hablar. Y, luego, la seguridad en sí de las plataformas, que comentaré un poquito, ¿vale? ¿Qué seguridad aportan las plataformas? La plataforma es donde tú ejecutas los modelos. ¿Te puedo interrumpir un poquito, Florencia? Sí, sí, por favor. Que me parece genial que hables de eso porque Jen y yo hemos comentado muchas veces que nos preocupa el que en IA tú, hay muchas librerías que son como estándar y que todo el mundo las baja. Y nosotras decimos, bueno, ¿y quién revisa esto? ¿Cómo sé yo si lo que estoy bajando es seguro o no? Entonces, primera persona que escucho que tiene esa preocupación y, además, allá que lo llevó y lo investigó. Y, con respecto a Hacking Face, yo estoy subiendo los modelos que tengo. Yo ya he subido 3, me parece, pero tengo como 8 más que tengo que subir. Y, sí, me doy cuenta al hacer el fine tuning lo que tú puedes hacer. Y, también, el tema es que mucha gente que consulta los modelos no entiende cómo funciona esto. No entiende por qué. Porque es súper engañoso porque tú utilizas Yama, Mistral o cualquier otro modelo conocido y ese es tu modelo base. Y lo que tú haces es un adaptador. Entonces, tú, cuando le pones el nombre, tú le pones Yama, Mistral. Y, claro, la gente, al verlo, piensa que está bajando un modelo oficial, el que sale ahí de la empresa y no. Y resulta que tú estás bajando un adaptador hecho por XXX y no te das ni cuenta. Entonces, si tú no entiendes cómo funciona Hacking Face, la verdad es que tienes un tema. Ahora, que haya contaminación con los modelos como de base no es imposible, pero, obviamente, cuando hay una gran empresa detrás es más difícil. Pero sí que también yo he visto que hay modelos de como organizaciones que no es nada demasiado oficial y que sacan muchos modelos y que son muy, muy buenos. Entonces, tú te ves muy tentada a utilizar esos modelos como base y hacer un fine tuning más ligero. Pero también a mí me surge esa duda, bueno, ¿y este modelo es seguro o no? Porque yo a esta persona no la conozco. Y yo sí tengo cierto temor con todos los investigadores de China que suelen subir cosas que son muy convenientes, pero yo digo, ¿hasta qué punto esto es seguro o no? No tengo capacidad de evaluar muchas veces. No es que no haya usado, también he usado, pero sí que yo creo que Hacking Face, para quien no está familiarizado, es muy engañoso. Que tú piensas que está usando modelos oficiales y no es así. J.M.L.: Pero Hacking Face sí que lo hemos visto en el máster, ¿no? Eso lo explicó Pablo, del Vecchio. PAULA MONTAÑÁNZUZO A TRAVÉS DE INTÉRPRETE O sea, sí, sí, se vio, pero yo diría que se vio por encima. Creo que todavía, si no lo conoces bien, de hecho, nosotros hicimos fine tuning, pero ligero, por los problemas de cómputo que hay, pero ahí son unos más pesados. Y, claro, tú ahí entiendes realmente cómo funcionan los archivos, los modelos, qué es lo que tienes que subir a Hacking Face. Entonces, ahí te das cuenta lo fácil que es confundirse. Yo misma buscando modelo me daba cuenta de que, ah, este modelo, y después lo miro y como, no, esto no es oficial. Entonces, sí, no, te confundí. J.M.L.: Y hay 2 partes, en lo que has dicho y es clave, que hay 2 preocupaciones, habrá más, ¿no? Pero 2 preocupaciones claras cuando te bajas un modelo de Hacking Face. Una es que el propio modelo, vamos a decir, lo que genera el modelo, cuando tú lo haces inferencia y le pasas un prompt y te da una respuesta, que eso de alguna manera esté. manipulado, imagina que es un modelo que genera código y de alguna manera lo han entrenado para generar código con puertas traseras, eso es una preocupación que es mucho la que estamos hablando de es un modelo que se llama lama pero se le hace fine tuning y a lo mejor el fine tuning es para que él conteste de una manera maliciosa, pero hay otra preocupación que yo es la que considero que está, a ver, todo es muy nuevo y está todo poco investigado, pero es que hay otra que es la que yo no oía hablar nada, que es, vale, pero yo cuando me bajo un modelo de Hugging Face, ¿qué me bajo exactamente? ya no hablo de lo que genera cuando cuando habla, sino que me estoy bajando, que me estoy bajando solamente un fichero donde hay pesos y eso se cargan en holama y ya tengo un modelo que habla o me estoy bajando más cosas y tú si has subido modelos pues ya sabes parte de esto o todo lo sabes muy bien pero hay mucha gente que es que directamente no sabe pues todos los formatos que hay para bajarse modelos, lo que contienen esos formatos y tal, ¿si Jenny? Sí, bueno, dos temas, una consulta que te voy a hacer después que te hable el primero y del primero opinar respecto también a considerar de que los temas de procedimientos como no es, por ejemplo, el procedimiento de descargar algún modelo y leerse o fijarse en todos los ficheros y efectivamente es como simplemente casi un phishing o no, en el sentido de que cuando tú ves un phishing tú puedes identificar rápidamente si lo va a eliminar y no hace ningún clic y nada, pero en el caso, por ejemplo, de que tú no lo identifiques, claro, es fácil hacer clic y bajo esa perspectiva y por eso hablo del tema de los procedimientos, cuando uno descarga modelos como que al principio no podrá revisar y antes de ejecutar el modelo local uno mirará los archivos, pero lo hace con todo y cada uno de los archivos que descargan, no, entonces como hay tanto modelo nuevo tú no vas a saber si hay alguno que efectivamente haga, cuando haga la ejecución, por ejemplo, la descarga de algún archivo malicioso y claro, a la larga lo mejor ese modelo va a ser bañado, en fin, pero los primeros que caigan no lo van a hacer, entonces ese es el primer comentario y el segundo comentario era respecto a la clasificación que estabas mostrando, porque en mi caso yo estoy haciendo de mi DFME una investigación del deterioro, el impacto y deterioro de los modelos explicables de Machine Learning en el caso de envenenamiento de datos en los sectores críticos de seguridad, defensa y salud, entonces en tu clasificación no sé dónde tú estarías viendo la parte de los datos, porque más bien como un insumo en este caso, más que las librerías y los modelos, entonces dentro de tu clasificación, como poner atención ahí, quería saber dónde lo incorporas tú. Sí, yo esa parte la diría como, los puntos que he puesto al principio serían el tercero, en los riesgos por usar un modelo, se ha relacionado con lo que el modelo genera, no en sí con los ficheros, con la infraestructura, sino más bien con lo que el modelo genera, que es otro tipo de medidas de seguridad, que digo, no hablo tanto aquí, pero es la explicabilidad, es lo que tú comentas, es la clave, el tema de guardrails, guardrails durante la ejecución, que vigilen lo que se les pase, lo que devuelven y luego técnicas de regularización, distintas técnicas que hay para reducir la probabilidad de que un modelo envenenado proporcione esos datos, y técnicas para intentar descubrir que se han manipulado los pesos para darle importancia artificialmente a algo. He leído cosas, pero sobre todo yo aquí en la charla de hoy me voy a centrar en la parte de eso, de técnicamente, si cuando me bajo un modelo puede tener un virus o puede ser un modelo malicioso, en el sentido que rompa o lama o rompa la plataforma para ejecutar comandos. También una cosa interesante es que tú con los adaptadores, cuando tú generas adaptadores, es decir, haces un fine tuning y subes tu adaptador, tú puedes sin darte cuenta estar propagando un virus, porque si tú de base utilizas un modelo que tiene algún tipo de malware, luego tú haces el adaptador y subes tu adaptador, pero la persona que usa ese adaptador carga el modelo base, entonces tú no te puedes dar ni cuenta y estás propagando un virus. Claro, ahora veremos que, o sea, si entendemos virus por código que se ejecuta y hace cosas malas, por ejemplo, borrarte ficheros del ordenador y pedirte un ransom o algo así, sólo algunos tipos de modelos, porque hay distintos tipos de modelos en HackingFace, tienen esa posibilidad a día de hoy y otros no, depende del formato con el que te bajes el modelo. Si es la parte del modelo envenenado, es decir, el Entrenamiento en sí está envenenado, esto sí que es general. Cualquier modelo puede estar entrenado de una manera maliciosa para generar datos malos. Pero lo que es en sí bajarte unos ficheros de modelo que tengan un virus, entendiéndolo como código ejecutable malicioso, solo algunos formatos. Y aquí es un poco, os voy a comentar cómo sin saber por dónde iba, empecé a investigarlo y lo que vi, ¿no? Y un poco los distintos tipos de ficheros que hay, ¿vale? ¿Cuáles, también así intuitivamente, cuáles sabemos que pueden ser más peligrosos y cuáles menos a día de hoy? Entonces, bueno, estas son las preguntas que me planteaba. Y, entonces, empecé a investigarlo desde el software de inferencia. Es decir, el software de inferencia que, os lo digo, porque muchas veces se habla mucho de la utilización de modelos que se sirven como servicio, ¿vale? Es decir, un modelo GPT que tiene un API y tú como servicio lo usas o demás, pero hay muchas empresas o organizaciones tipo Hugging Face, también Red Hat, que su visión es que va a haber más bien muchos modelos medianos y pequeños muy especializados y que las empresas usarán más esto que grandes modelos que hagan todo, ¿vale? Ahora mismo hay una, están esas 2 visiones, pero muchas empresas, sobre todo de entornos críticos, sí que directamente tiran por la parte de usar sus modelos en sus propias instancias en la nube o en sus propios servidores en local, ¿vale? Entonces, para ejecutar estos modelos en local se utiliza este tipo de software, ¿no? Aquí os he puesto algunos. Olama, que es el que hace meta, ¿no? Tú lo instalas en tu ordenador y puedes ejecutar un modelo. Con Hugging Face, con el API de Hugging Face, también puedes hacerlo. En Hugging Face tú directamente ves las líneas de Python que puedes ejecutar para bajarte un modelo y ejecutarlo en tu ordenador. VLLM es un software de código abierto que Red Hat recientemente ha comprado la empresa que, sobre todo, hacía el desarrollo de este software. Y es la apuesta de Red Hat, ¿no? Igual que está OpenShift de Kubernetes, pues va a haber un producto de Red Hat basado en VLLM que Red Hat va a mantener, soportar y tal. También sirve para hacer esto, para hacer inferencia de modelos. KSERV, que es un modelo, es también open source, pero es para Kubernetes o para OpenShift. Vale, yo aquí lo que hice, lo que he investigado es partiendo de Olama y de Hugging Face API. Entonces, lo que vamos a ver es, ¿qué te bajas exactamente cuando te bajas un modelo con Olama o qué te bajas exactamente cuando te bajas un modelo usando el API de Hugging Face? ¿Vale? Y si es lo que te bajas, puede tener código. Porque si tiene código, puede tener malware. La otra cosa, ya veríamos las condiciones, quién lo revisa y demás. Y si no tiene código, si realmente solo tiene pesos, ¿cómo puede ser? ¿Dónde está el código? Porque código tiene que haber. Para ejecutar un modelo, entonces, vemos primero en Olama, ¿vale? Olama tiene un catálogo de modelos. O sea, Olama no soporta todos los modelos, soporta algunos. Y eso ya es un indicador, como comentaba antes, de que no pueden ser todos los modelos iguales. Porque si no, Olama soportaría todos los modelos. Cuando salió DeepSeq, ya lo soportaría, ¿no? Porque es igual, pero no lo es. Entonces, hay algo que los diferencia. Entonces, yo lo que hice fue descargarme, usar Olama para ejecutar un modelo, el Lama 3.2 de un billón de parámetros. Y ya en el catálogo veo una información que pone Arc, ¿no? Suena a arquitectura, arquitectura Lama, ¿no? Esto cuadra. O sea, parece que hay large language models que tienen diferentes arquitecturas. Y si tienen diferentes arquitecturas, es que tienen que tener diferente código. Y el código tiene que estar en algún lado. Mi hilo de pensamiento es ese. Entonces, me bajo un modelo Lama que ya en el catálogo me está diciendo que tiene una arquitectura Lama. Vale, entonces, me lo descargo y veo que se descarga. Esto, si habéis trabajado con contenedores, os sonará. O sea, realmente, cuando tú descargas un modelo con Olama, se descarga como layers, como diferentes capas. Igual que cuando se descarga un modelo de diferentes tamaños. Hay una capa, la primera, que tiene 2 gigas, ¿vale? Entonces, parece que ahí hay material, ¿no? Entonces. VALERIA ARLANTIZ TORRES. Disculpa, Florencio, me parece que nuevamente no estás mostrando algo en pantalla porque estamos viendo solamente la lista de software. FLORENCIO LASTRE. Sí, y te gusta quedar bloqueado. ¿Ya? Pues, a ver. Voy a probar lo que decíais de quitar la cámara. A ver. Vale, nos la quitamos todos también. Con el saco que doy yo con la cámara, vamos a quitarlo a todos a ver si. Vale, si veis que cambia, me lo decís, ¿vale? Para saber que. Vale, ahora no está cambiando, ¿no? No. Voy a parar de compartir y compartir otra vez. Voy a probar a compartir toda la pantalla en vez de. En todas maneras, se te queda bloqueada la imagen y entiendo que es, no sé, o algo de que no está funcionando bien el Teams vía navegador. Ahora parece que, a ver, a ver, a ver, se ve en blanco, ¿eh? ¿En blanco? Sí. Charlie vuelve a entrar, si no, como has hecho antes y a ver si sigue. Vale. Curioso. Estamos hackeando el ordenador a Florencia. Ha ido bien durante un rato, pero luego dice, ya, está aquí. Tiene que ser algún cache de memoria o algo que se queda, se queda ahí. A lo mejor, efectivamente, es su internet, ¿no? Sí, sí. Entiendo que, bueno, está utilizando el sistema operativo Red Hat y a lo mejor no se llevará muy bien con el Teams. Red Hat y Microsoft ahí. Y una cosa que le pregunté, cuando IBM compró Red Hat, Carlos, que son dos compañías antagónicas. Cada una está en una punta. Si había notado algún cambio. Y parece ser que, a ver, algo se nota, pero que en principio están manteniendo al final la solidez y la filosofía de Red Hat. Que eso es muy importante, ¿no? Porque una de las cosas principales que ha triunfado Red Hat es por eso, ¿no? Porque desde el punto de vista de IBM necesitaba sangre fresca. Y hizo bien en comprarla porque estaba, de todo lo que ha sido, yo creo que está metiendo la parte de Watson, que lleva tantos años intentando introducirla. En hecho, fue uno de los primeros en ponerle un nombre a temas de inteligencia artificial. Y ahí sigue. Vale, ahora sí que se te ve. Estás con el micro apagado, pero sí que se te ve bien. A ver, ¿ahora me oís? Sí, sí, pero de todas maneras, yo os diría, vamos a quitarnos la cámara, si te parece bien, Florencia. A ver si ahora se aguanta un poquito más. Y comparte y a ver si así podemos. Vale, ahora voy a ver de compartir esto. Vale, sí, ahora se ve perfectamente. He cambiado de navegador también, porque yo suelo usar siempre Firefox y no me suele dar problemas, pero, bueno, estoy ahora con Chrome. Vale, perfecto. Vale, pues, entonces, comentaba que cuando te bajas el modelo con Olama, pues, se hace una descarga de eso muy parecida cuando te bajas de un contenedor. Aquí, si veis, se está bajando a distintos blobs de bytes, que son conjuntos de bytes. Algunos 96 bytes, muy pequeño, pero hay uno de 2 gigas. Bueno, entonces, se ha descargado esto, 2 gigas. Entonces, bueno, lo ejecuto también. Lo ejecuto, veo que sí, que he hecho una descarga primero y ahora hago una ejecución y se ejecuta normal. Entonces, algo se ha descargado y, entonces, empiezo a mirar en mi ordenador a ver lo que realmente me he descargado. Entonces, ¿qué hemos descargado cuando he hecho este pool con Olama? Pues, bueno, hay un directorio en nuestro ordenador que es user-sare-olama.olamamodels que tiene esta estructura. Tiene un directorio blobs y luego tiene un directorio manifests. Esto es muy parecido a los contenedores. Los blobs son esas distintas capas que se ha bajado y en el directorio manifests, pues, tiene ahí una jerarquía de directorios y, al final, hay un fichero latest, ¿vale? Entonces, bueno, voy a ver ese fichero latest que contiene y veo que es un manifest, pues, muy parecido a lo mismo cuando se baja un contenedor. Y aquí lo que hay es, la definición en este manifest es como un inventario de las distintas capas que se ha bajado, ¿no? O sea, para. A recapitular, me he bajado un modelo con Olama y se ha bajado distintos ficheros, que son bytes, conjuntos de bytes. Hay uno en concreto, que es este que está recuadrado, que tiene más de 2 gigas. Y, además, le llama, tiene un media type, le llama image model, ¿vale? Entonces, eso es lo más grande que me he descargado cuando me he descargado el modelo. Entonces, ahí viene la pregunta, entonces, ¿qué es exactamente este fichero? Si yo lo abro, ¿habrá directamente un array, un vector de bytes, que son los pesos del modelo, o hay algo más? Entonces, porque si hay algo más, y vuelvo a decirlo, hay código, pues, me podría haber bajado un virus cuando me he hecho ese Olama pool. Entonces, voy a investigar este fichero. Vale, es este fichero. Y veo que, pues, este fichero lo abro, digamos, la interpretación en hexadecimal. Y veo que empiezan los ficheros muchas veces, los primeros bytes de un fichero, que es una cosa que le llaman magic number, que es una cabecera al inicio de un fichero que indica el tipo de fichero. Esto pasa en muchos formatos, no en todos, pero muchos pasa. Entonces, aquí, en este fichero que yo me he descargado con Olama, sí que, efectivamente, justamente los primeros bytes son gguf, que a mí ya me sonaba que era un formato para algo relacionado con modelos, ¿no? Pero a partir de aquí lo investigué más. Entonces, me he bajado un fichero de 2 gigas que es un fichero gguf, ¿vale? Entonces, busco información del fichero de este formato, del formato gguf. Entonces, encuentro esta información. Dice que el formato gguf, al contrario que los formatos de solo tensores, es decir, tensores, son estas matrices que contienen los pesos de los modelos. Dice, al contrario que los formatos de solo tensores, es decir, gguf no es un formato que solo tenga pesos. Tiene algo más. Vamos a ver qué más tiene. Dice, al contrario, como, por ejemplo, el formato 6 tensores, vale, 6 tensores es otro formato y un poco lo comentaré luego también. Y, además, dice, 6 tensores es el formato recomendado por Hugging Face, ¿vale? O sea, esto es un formato, yo me he bajado un formato con Olama que no es el que recomienda Hugging Face. Y dice, gguf codifica conjuntamente tensores, es decir, los pesos y metadatos. Entonces, ya tenemos que yo me he bajado un fichero en local, me lo he bajado de Olama. No ya de Hugging Face todavía, no estamos hablando de Hugging Face, estamos hablando de con Olama, con la herramienta de Olama y del catálogo de meta de Olama. Me he bajado un modelo en formato gguf y es un formato que tiene pesos y tiene metadatos. Vale, pues, ya si tienen metadatos, significa que esos metadatos van a ser leídos por alguien, ¿no? En esta imagen aquí también se ven, hay un metadato general.arquitecture. Lama. Esto está dentro del contenido de este fichero, que yo también lo veía en el catálogo de Olama, si os acordáis que he visto Arc Lama. También tiene un general name, Lama versión 2. El tamaño del contexto 4,096 y tiene otros metadatos. Curioso, ¿vale? Pues, estos datos van a ser leídos por alguien. En seguridad ya sabemos que parsear datos, es decir, leer un fichero, leer cada campo de ese fichero y guardar esos datos en sitios, eso es el origen de muchas vulnerabilidades de seguridad. Hay muchas vulnerabilidades de seguridad relacionadas con, yo aquí hago un programa que abre el gguf y va a leer el campo general.architecture y en vez de llama, aquí hay 3 millones de as. Pues, eso puede romper el programa y puede generar un problema de seguridad, ¿vale? Entonces, ya sabemos que la primera, digamos, en la frente. Me he bajado un modelo de Olama y no tiene solo pesos, tiene pesos y metadatos. Y donde hay metadatos ya hay para investigar, ¿vale? Este es el formato gguf, que es interesante, ¿no? También significa que me bajo un modelo de internet que contiene datos que yo puedo abrir y mirarlo, ¿no? Y me da información sobre el modelo. Bueno, y yo digo, vale, de momento sé que tiene metadatos, pero yo estoy buscando el código, ¿no? A mí lo que me pica es que debe haber código en algún sitio. Código que diferencie. Este modelo de otro modelo. Entonces, estoy buscando ese código. Entonces, con este comando abro el gguf y extraigo los metadatos. Y encuentro unos campos, bueno, encuentro campos de metadatos, ¿no? También habla del tokenizador que se usa, que es gpt2, ¿no? O Lama, el modelo Lama utiliza el tokenizador de gpt2. Interesante. Pero hay un campo de metadatos en gguf que es este. Y yo digo, genial, ya he encontrado el código. Veo aquí ifs, endif, veo comparaciones, veo asignaciones, set, system, guión, bajo, message. Veo un parseo de una cadena. Aquí veo otra asignación, aquí veo código. Entonces, vale, digo, en el fichero gguf hay código, pero este código no me parece que defina la arquitectura de un modelo. Me parece otra cosa, ¿no? Además, se llama chat template. Entonces, empiezo a investigarlo y me doy cuenta que esto es, para modelos que se pueden usar como chats, el formato del chat, bueno, cómo se habla con el modelo tiene un formato y eso está definido aquí, dentro del fichero gguf. Y este código es un código en un lenguaje de programación que se llama Jinja. Y, en concreto, para los casos de los modelos de, o sea, gguf en general utiliza el lenguaje Jinja para estos templates de chat. Y, en el caso de Olama, usa además Go template. No Go, pero un lenguaje relacionado con Go que se llama Go template. Entonces, es decir, me he bajado un modelo, no son solo pesos, y tiene código. Y me lo he bajado de Olama, con Olama. Ahora, ¿este código puede tener malware? Bueno, pues estuve investigando y no es conocido, no se sabe que se pueda meter malware en este código, ¿vale? Entonces, ahí baja un poco la posibilidad. Siempre existe, vamos a decir, en investigación, en seguridad, esto puede que luego se encuentre algo, pero pinta que no. Este lenguaje no es tan rico como para poder hacer un virus. Este lenguaje es más para parsear, digamos, vistas o vistas o, pues eso, el formato de un chat, ¿no? Para establecer un formato. Pero no tanto para código que, por ejemplo, pueda acceder al disco, pueda guardar cosas en el disco y cargarlas. Este lenguaje no tiene esa capacidad. Entonces, parece que no. Pero lo que sí que se me ocurre, al final, como es un lenguaje de programación que alguien tiene que leer y procesar, pues si yo adrede este lenguaje, hace cosas raras, a lo mejor puedo romper o Lama, ¿no? Cuando Lama carga este gguf, si yo lo he manipulado, a lo mejor rompo Lama y genero un problema de seguridad. Eso podría pasar. PAULA MONTAÑAZO SÁNCHEZ POR INTÉRPRETE Un ciclo infinito fácilmente lo podéis meter ahí, ¿no? Por ejemplo, se podría probar un ciclo infinito, eso es muy interesante, sería una prueba muy interesante. Algo, digamos, tan simple, entre comillas, como eso para empezar y eso ya sería una demostración de que algo malo puedes hacer con ese código, ¿no? Vale, entonces, digo, vale, yo me he bajado un fichero para probar un modelo, es un gguf, ya he avanzado algo, sé que no solamente son pesos. Me he hablado, además, del formato gguf y el formato 6tensors, pero ahora quiero hacer un poco lo mismo con otro modelo para ver, para comparar, ¿no? Para comparar a ver si es que, a lo mejor, todos los modelos tienen arquitectura Lama, no lo sé. Entonces, y también como de deepseq salió y se hablaba mucho de deepseq, pues, digo, voy a probar con deepseq a ver qué me encuentro. Entonces, muy similar, ¿vale? Me bajo deepseq, lo mismo, hay una forma de bajarse que se ve exactamente lo mismo, me bajo distintos ficheros. Hay uno que tiene un giga, lo guarda en el mismo directorio. Ahora, si miro en ese directorio, veo los 2 modelos, el deepseq R1 y el Lama 3.2 que me he bajado antes. En el deepseq R1 voy al fichero, lo abro y veo, efectivamente, que me dice cuál es el fichero concreto de los que me he bajado, que es el más grande, que tiene ese nombre. Y vuelvo a mirar y vuelve a ser el ggweb, ¿eh? Es decir, parece que los modelos que me he bajado antes. bajo con Olama son gguf. Lo extrapolo de 2 ejemplos, ¿vale? Igual hay otros formatos, pero yo en principio me encuentro que 2 que me bajo, 2 son gguf. Y hago lo mismo, leo el fichero gguf, veo que la arquitectura es de Windows, que no es DeepSeek, es también chino, está relacionado, pero son 2 modelos distintos, pero tienen la misma arquitectura. No, es una cosa que yo también me doy cuenta que no sabía. Estos LLM grandes que te bajas de internet pueden usar arquitecturas de otros modelos, ¿vale? Es decir, eso como realmente son open source, y ahora hablaremos un poco, ahora veremos eso, pero pueden usar arquitecturas de otros modelos. No tienen cada uno su propia arquitectura. Hay arquitecturas que son compartidas, pero ya no es Lama, es otra arquitectura que es Cuendos. Pero, Florencio, una pregunta sobre esto de arquitectura. Al final, ¿esto es el tipo de arquitectura? Y no sé si lo conoces, pero yo lo desconozco. ¿Es el tipo de arquitectura que están utilizando de cara a Transformer o no se refiere a esto? FLORENCIO DÍAZ-RUIZ Sí, todos se basan, o sea, yo ya no soy muy especialista de las distintas arquitecturas, pero sí que te puedo decir que todas estas utilizan Transformers. Lo que pasa es que luego hay variaciones en cómo se interconectan ciertas capas para que haya, digamos, feedback. O sea, no es exactamente la misma interconexión entre las distintas capas de la red neuronal. Hay pequeñas variaciones. Está súper, que esto es para investigarlo también, está súper interesante la investigación que has hecho. Esto es para publicar 4 artículos, ¿eh? Sí, esto, bueno, fui con esta charla a la ruta de Diablé y ahora me voy a Ámsterdam a final de mes a seguir hablando de esto. Joder, privilegio. Porque yo creo que es algo, además, vamos a ver, que lo estáis viendo, que no es complicado, pero es ir rascando paso a paso y que no había mucho de esto. Es que, además, lo contrario, la gente estaba hablando de que aquí no había nada que investigar. Que, hombre, yo creo que sí que hay para investigar aquí, ¿eh? Y además, y os voy a enseñar más cosas, ¿vale? No, eso es una locura que se diga que no hay nada que investigar si está todo por investigar. Solo para complementar, cuando se habla de LLM, yo entiendo que estamos hablando de transformers, pero no son los únicos riesgos que tenemos. Porque yo, por lo que sabía, porque como te decía, con Jenny siempre nos preguntamos qué tan seguro es lo que bajamos, porque somos como bien adversas al riesgo de ese tipo de cosas. Y por lo que yo sabía, tenemos riesgos en los archivos Pico, que son los de PyTorch y que solemos usarlo con transformer. Pero, por ejemplo, cuando usamos redes neuronales, utilizamos TensorFlow y ahí los H5 también son archivos que pueden tener ejecución de código. Y ahí no tenemos transformers. Entonces, no es que sea solo asociado a transformers, solo para poner ese tipo de cosas. Correcto. Correcto. Yo entiendo, además, que sí, o sea, es decir, H5 yo no lo he mirado, ¿vale? Pero Pico sí, de hecho, esta parte de la charla se llama riesgos más allá de PQL. Y PQL, comento al final, porque es el primero el que inició esto. O sea, PQL es el primer tipo de formato que era fácil, que contuviera código malicioso. Y, de hecho, se encontraron casos en HackingFace con PQL y que tenían malware. Pero luego había otros formatos que se supone que ya no les pasaba eso, pero, que es lo que os enseño, que hay ciertas cosas para investigar y tal. Pero sí, y H5, ese no lo he mirado, lo había oído, pero no lo he investigado yo. Pero sí, hay para mirar. Vale, y en DeepSeek lo mismo. Es decir, está esto, está la arquitectura, está el tokenizador y también tiene el chat template también con código. Yo no le veo nada especialmente raro a este chat template, pero, bueno, otra vez nos encontramos código aquí. Vale. Y, vale, aquí nos quedamos con la idea de Cuenchu, ¿vale? O sea, utiliza la arquitectura Cuenchu, que yo creo que es una palabra clave importante y ahora os enseñaré, porque yo luego busco, ¿no? Y es otro modelo que en principio no tiene nada que ver con DeepSeek, ¿vale? Pero usa la misma arquitectura. Vale, entonces, yo de aquí me surge otra idea. Digo, vale, tengo distintas arquitecturas y parece, ¿no? Aquí un poco, vamos a decir, un poco de decepción, porque yo sí que esperaba encontrar el código en este formato gguf, ¿no? Que yo me estaba bajando estos modelos y tiene que haber código, tiene que estar en algún sitio, pero no está aquí. Y digo, ¿podría ser que el código esté en el propio Olama? Porque aquí hay 2 partes, el modelo y Olama. Entonces, si no está en el modelo, una cosa primera que pienso. es que se lo baja de internet el código cuando Olama ejecuta el modelo, ¿vale? Entonces, esto aquí no lo he documentado, pero yo investigo esto primero. Es mi primera hipótesis, que se lo baja de internet. El propio código específico de Quenchu se lo baja de internet cuando lo ejecuto este modelo con Olama. Pero me doy cuenta que no. Es decir, lo ejecuto, pongo Wireshark, utilizo distintas herramientas para ver la conexión de red y los tamaños y demás y no. Entonces, digo, a ver, si no se lo baja en tiempo de ejecución, no se lo ha bajado cuando ha hecho el pull, a lo mejor es que el código está en Olama. Pero digo, no puede ser, ¿no? No puede ser que el código esté en Olama por varias razones. Una es porque si el código de Quenchu está en Olama, tendrá que tener el código de Quenchu, tendrá que tener el código de Lama y tendrá que tener el código de otras arquitecturas que haya. Y, luego, en Olama, lo cual no me cuadra mucho, ¿no? Que el código de Quenchu esté en Olama, que son 2 proyectos distintos. Y, aparte, entonces, ¿qué pasa? Que Hugging Face API, VLM, KSERV, lo que hemos visto al principio, también tienen el código de Quenchu, de Lama y de otras arquitecturas. O sea, cada motor de inferencia está repitiendo el código. Yo pienso, no puede ser, pero vamos a ver. Entonces, yo busco referencias a Quenchu dentro del código fuente de Olama. Olama es código abierto. Me lo descargo de GitHub y busco Quenchu en el código. Ves, ahí está el repositorio arriba. Hago un clon, me lo bajo, y hago un grep. Y cuento cuántas veces aparece. Y Quenchu aparece 98 veces dentro del código fuente de Olama. Digo, ostras, pues parece que sí. Y, además, empiezo a ver el código y veo que, efectivamente, es código que no parece código trivial. Es que está especificando la arquitectura. Está especificando las hidden layers, temas relacionados con los embeddings. Y, entonces, empiezo a investigar el código de Olama. Olama tiene un fichero, olama.go, que ahí es donde, en esa línea 104, donde lee del gguf, donde lee el metadato. Entonces, veis que hay código Go que está leyendo el metadato. Entonces, ahí podría haber un problema de seguridad. Hay un free de un pointer que, digamos, el tema de liberar punteros, reservar punteros y tal, está muy relacionado con la seguridad. Bueno, el caso es que veo y me encuentro, por ejemplo, esto dentro del código de Olama. Es decir, montones de arquitecturas, 100 y pico arquitecturas. Creo que luego hago un ls y lo digo, 100 y pico arquitecturas, todas dentro del código de Olama. Entonces, ya he encontrado el código. El código fuente de Quenchu, el código que es diferente para Quenchu y para Lama y para Granite y para Mistral, que tienen diferentes arquitecturas, el código fuente que las diferencia está dentro del propio Olama, dentro del propio código fuente de Olama. Aquí veis, hay un case que dice, oye, si esto es Quenchu, lo que estás haciendo inferencia, esta es la arquitectura que vas a usar. Ves, tiene casos para, dependiendo el tipo de modelo, para FII 2, por ejemplo, veis abajo, que es un modelo de Microsoft, todo código específico y que se repite, o sea, que es diferente para cada, no para cada modelo, pero para cada arquitectura de modelo. Y hay entre 100 y 150, creo que vi, distintas arquitecturas dentro del código fuente de Olama. Entonces, yo de momento, con los ficheros que me he bajado de Olama, que son gguf, no veo una posibilidad sencilla de yo, cuando me baje un modelo, que el modelo tenga un virus. Porque no veo código ejecutable. El código ejecutable, sí. DIANE PORRADO LEE EL CHAT. ¿Estaríamos diciendo más o menos que Olama está funcionando como un orquestador de arquitecturas de los modelos? Porque él está definiendo cuál es la arquitectura conforme el modelo que se va a leer. Eso es lo que, en resumen, estaría haciendo Olama en este caso, porque veo que es Olama el que define la arquitectura que se va a utilizar conforme el modelo. PABLO SANGUINETTI LEE EL CHAT. Olama es un código fuente abierto, ¿no? Entonces, yo imagino que si la gente que hace DeepSeq quiere que DeepSeq se pueda ejecutar en Olama, le hacen un pull request a Olama con su código, con el código que Olama tiene que implementar para poder ejecutar DeepSeq, que es la arquitectura de DeepSeq. Entonces, lo que yo quiero creer es que la gente de Olama revisa ese código y dice, OK, nada raro, para adentro. Y lo incluyen en Olama. Digamos que Olama tiene él el código que necesita cada modelo para ejecutarse. No es que lo orqueste, sino que el propio código que define, el propio código de Ptorch que define las capas que tiene que tener, cómo se deben interconectar y demás, ese código está dentro de Olama. ¿Cómo llega a estar dentro de Olama? Pues, yo creo que es eso, que los propios ingenieros que crean los modelos lo contribuyen a Olama o a lo mejor los propios ingenieros de Olama crean el código específico. Pero la cuestión es que alguien mete en Olama el código del modelo que quieren que Olama pueda ejecutar. O sea, el código de DeepSeq, el código fuente de DeepSeq, no te lo descargas cuando te descargas DeepSeq. Si lo vas a ejecutar con Olama, ¿vale? El código fuente de DeepSeq está ya en Olama. Por eso es que Olama lo soporta. Si no tuviera el código, cuando Olama no soporta DeepSeq o no soporta un modelo, es porque no tiene el código todavía, no tiene el código de su arquitectura todavía. Esto, ¿vale? ¿Y por qué? Dices, ¿y por qué tiene el código Olama y no te bajas el código cuando te bajas el modelo? Pues, pienso que a lo mejor lo querían hacer o por optimización, porque quieren asegurarse que el código funcione de una manera concreta dentro de Olama, por optimización o también puede ser por seguridad, ¿no? Porque si te bajaras el código cada vez, de cada modelo, habría más riesgo de que te bajes malware. Si el código está dentro del proyecto Olama, pues, como ya hay gente que lo mantiene, que lo monitoriza y demás, es más difícil que alguien cuele ahí un virus. Posible, pero más difícil. CARLA NUDEL ARESTADO Vale, gracias. Sí, eso me queda más claro y creo que, efectivamente, o sea, si bien también es un punto único de falla, pero también te da más seguridad porque centraliza. CARLOS ANAYA LOZANO Correcto, es un punto único de fallo y, además, y de riesgo, ¿no? Es decir, si hackean Olama, pues, claro, lo que pasa es que ese código de esos modelos, bueno, al revés. Es decir, aquí también tiene una ventaja de seguridad. Si hackean, o sea, para hackear el código fuente de esos modelos, tienen que hackear Olama. Y cuando lo hagan, solo hackean Olama. Digamos que, como ahora veremos, DeepSeq para ser ejecutado en HackingFace es otro código que representará lo mismo, ¿no? Porque la arquitectura será igual, pero es otro código diferente. Entonces, si hackean Olama, no hackean HackingFace, ¿no? Digamos que el código de DeepSeq ejecutado en Olama y el código de DeepSeq ejecutado en HackingFace es diferente. Entonces, eso yo lo veo un problema de mantenimiento y una pesadilla a nivel de desarrollo. Pero imagino que tiene esa ventaja de seguridad, pero, claro, lo veo que eso, supongo que se ha creado así, pues, o por intereses o por inercia que se empezó creando separado. Lo lógico habría sido cada modelo en un solo sitio, ¿no? Y ejecutado por distintos motores. Pero no es así. Cada motor tiene su propio código para cada arquitectura de cada modelo. AUDIENCIA 1. Seguramente es hasta que salga algún protocolo, así como MSP o para las herramientas. AUDIENCIA 2. Sí, o que se estabilice más. Porque también creo que cada motor de inferencia hace ciertas optimizaciones. Y es, por ejemplo, Olama creo que está optimizado para poder ejecutarse en CPU, ¿no? Porque quiere que cualquiera lo pueda ejecutar en su ordenador. Entonces, modelos cuantizados hacen ciertas cosas. Y HackingFace es de otra forma, VLLM es de otra forma. Entonces, pienso que también puede ir por ahí la cosa. Que ese código que se implementa de la arquitectura de cada modelo, a lo mejor, se les hace ciertas cosas para ese motor específico de inferencia. No sé las razones exactamente. Sé que cuando lo investigué me encontré esto, que tienen el código de Quenchu dentro de Olama. AUDIENCIA 1. O sea, yo creo que el que sea así está bien. Porque si no, imagínate los riesgos que tendrían. al descargar los modelos. Creo que si funciona de esta forma es más seguro para nosotros porque cuando estamos descargando un modelo solamente estamos descargando pesos y metadatos pero se supone que ni los safe tensor ni estos archivos los que acaba de mostrar el formato gguf deberían poder ejecutar código entonces me parece que está bien que sea que sea así y también lo otro es que al tener hola malo las arquitecturas creo que también da cierta seguridad porque no cualquiera podría como meter una arquitectura que no esté como soportada por programa es decir ellos te aseguran que en el fondo no van a ejecutar un código que no encaje con las arquitecturas aprobadas entonces me parece que desde el punto de vista de seguridad está bien porque si no yo me imagino que estaríamos todos súper infectados a esta altura si no existiera algo así sí yo creo yo estoy de acuerdo con lo que sabemos hoy como yo creo que sí que es así es decir creo que es tienes a decir es bueno no es malo tiene sentido y dice las ventajas pero vamos a ver algunos casos también que se salen de esta norma lo primero de generación de imagen pasó no que estaba el modelo checkpoint y en ese podrían poner malware y después se generó el nuevo modelo que es extensor que justamente se creó para para evitar esto se extensos ahora lo comentaré porque cuando uso el api dejado en feis se usan más estándares extensos y extensos es el más seguro es más seguro incluso que gguf por lo que comento no porque gguf como tiene metadatos sí que podría estar mal formado adrede o sea que esos metadatos tengan caracteres raros por ejemplo y cuando olama los lee que se rompa de hecho esto ha pasado o sea se ha reportado vulnerabilidades en 2024 por ejemplo una vulnerabilidad que por un fichero gguf especialmente construido lo habría olama y se generaba y se explotaba esta vulnerabilidad que era un buffer overflow entonces esto ya ha sucedido si es un fichero más plano con menos palancas no como se extensos esto es más difícil que pase vale entonces pero vamos a ver el caso de jardín feis hemos visto olama olama trabaja con ficheros modelos gguf que tiene este tema de los metadatos ya hemos visto que el riesgo no es cero porque hay metadatos que se tienen que leer y procesar y además hay código no que es ninja o go template hay cierto riesgo de momento no hemos visto mucho no parece que te puedes bajar un virus dentro de un modelo de olama vale de momento no no vamos a ver en jardín feis en jardín feis que ya lo conocéis vale yo esto lo he hecho el ejemplo con granite que es un modelo que ha entrenado ibm vale ibm compró red hat hace un tiempo para que os suene y porque hablo también de ibm estas son las instrucciones que da jardín feis para ejecutar este modelo en local vale hay unas instrucciones aquí en concreto más interesantes que son los que salen en la línea 5 6 y 7 que es donde digo el nombre del modelo digo el tokenizador que se va a usar y en la línea 7 es donde por así decir cargo el modelo no hay un automóvil for causal lm no sé qué que le digo el nombre del modelo y se instancia el modelo en la variable model vale cuando ejecuto este código python hay una descarga muy parecida a la que había cuando usaba olama vale o sea como refresco olama es un software para ejecutar modelos vale ahora estamos haciendo usando el api de jardín feis que es con código python y una librería de jardín feis ejecutar modelos vale es como una alternativa puede ejecutar modelos con olama o puede ejecutar modelos con código python y una librería de jardín feis que se llama transformers la librería si se llama transformers vale pero cuando yo ejecuto este código pasa algo muy parecido a cuando me he bajado antes los el modelo con olama se bajan distintas capas en este caso sí que me dicen los nombres de los ficheros en concreto no y dice que es pues un fichero json otro fichero json txt los json y los txt son básicamente texto plano con formato pero ahí no hay código vale no principio no va a haber código dos ficheros los más grandes son ficheros safe tensors vale que ya lo hemos hablado vale os diré un poquito más pero aquí así a priori no parece que haya código tampoco parece que me he bajado ficheros con datos y luego dos ficheros extensos. Los ficheros, bueno, aquí, bueno, miro los modelos que tengo aquí. Es que tenía bajados muchos de distintas pruebas que estoy haciendo. Bueno, encuentro este modelo concreto en mi ordenador. O sea, también cuando ejecuta este código me he bajado algo, estos ficheros. Analizo lo que me he bajado y lo mismo, muy parecido, ¿vale? Hay una serie de ficheros que les llama blogs porque son, digamos, tiras de bytes. Que, además, bueno, al final cada uno de estos ficheros representa, estos ficheros con estos nombres raros, 1, 8, 3, E, B, 8, 1, bla, bla, bla, representan ficheros concretos que son los que me he bajado, ¿vale? Los J son txt y extensos. Que justamente son los mismos que en Hacking Face podemos ver en la pestaña Files and Versions. O sea, en Hacking Face sí que me dice directamente qué ficheros me voy a bajar cuando yo me baje este modelo, ¿vale? Vale, hay un fichero interesante. Y porque yo empiezo a mirar estos ficheros a ver si encuentro código por ahí. A ver si en el caso de Hacking Face es diferente del caso de Olama y sí que me estoy bajando código. Entonces, veo un fichero interesante que es un Json que pone también arquitecturas y pone, aquí llama la arquitectura Granate for Casual LM. Este fichero me da la sensación, sí, dime, Ernesto. A ver, una cosa. Es que está tan interesante que sé que teníamos que hacer una parada, pero no quiero quedarme en la mitad. ¿Os parece que sigamos el tirón? Y ya hasta que acabe el frase, si tú estás bien, no estás cansado, ¿no? Sí, sí. Yo tengo cuerda. Pero, Ernesto, estamos de acuerdo, ¿no? Sí, sí, sí, sí, de, no, no, no es para borrar, sinceramente. Lo siento, compañero. Por eso digo yo que del tirón. Estamos de acuerdo. ¿Vale? Si estáis de acuerdo, todos del tirón. Vale, gracias. Que estás, me estás manteniendo ahí la tensión. Sí, está super. Adelante, adelante. Vale, pues, nada, este fichero lo miro y también tiene la arquitectura, ¿no? Granate for Casual LM. Y parece un fichero parecido, o sea, parece que aquí los metadatos que antes tenía dentro del fichero gguf, aquí están fuera. O sea, en uno de los ficheros que me he bajado de Hacking Face, los metadatos están en un fichero que se llama config.json. Bueno, aquí apunta a otra cosa, pero, porque estos son los ficheros Safe Tensors, ¿vale? Luego volvemos al config.json, ¿vale? Los ficheros Safe Tensors que me he bajado, aquí está la descripción en Hacking Face. Dice, Safe Tensors is a new simple format for storing tensors para almacenar los pesos de manera safe, ¿vale? De manera segura. En oposición a PQL, que ya lo hemos comentado, ¿vale? Que PQL es el formato estrella malo, digamos, que inicialmente se utilizaba. Y, además, dice que es muy rápido y tal. Vale, Safe Tensors parece que realmente hay poco que rascar, ¿vale? Pero los metadatos que ahora no están dentro de Safe Tensors, como sí que estaban en gguf, están fuera. Entonces, la arquitectura la volvemos a tener aquí. Entonces, parece que, incluso, ahora estamos usando otro software, no estamos usando Olama, el concepto de arquitectura de modelo es importante. Y es lo que marca el código que va a usar el modelo, ¿vale? Y aquí dice que es Granite for Causal LM. Vale, entonces, en los ficheros que me he bajado tampoco está el código. ¿Esta arquitectura qué pasa? Ahora sí que me la bajo de internet o esta arquitectura está en algún sitio. Igual que antes estaba dentro de Olama, este código específico está en algún sitio. Bueno, empiezo a rastrear el código fuente y esto me lleva a la librería Transformers. Entonces, me voy a la librería Transformers, que es open source. Y aquí también resaltar, o sea, toda esta investigación es posible porque esto es de código abierto y yo puedo ir a los repositorios y bajármelos y hacer grep y navegar por los directorios. Si todo esto fuera código cerrado, pues, a ver, podría haberlo hecho con ingeniería inversa y tal, pero primero mis conocimientos ahí habría tenido que estudiar bastante para poder haberlo hecho en código cerrado. Pero con código abierto puedo bajarme los directorios y estudiarlos y entenderlos, ¿no? Entonces, me voy a la librería Transformers y me bajo el código, ¿vale? Me bajo el código y hago lo mismo que hice con Quenchu. Aquí busco la arquitectura Granite for causal LLM dentro del código fuente de la librería Transformers. Y hay 20 referencias en ficheros concretos, ¿no? Aquí, de hecho, dentro del código fuente de Transformers hay un directorio Models, que hay un directorio Granite y tiene unos ficheros Python Modeling Granite que efectivamente definen la arquitectura de los modelos con arquitectura Granite. O sea, volvemos. Vamos a tener un patrón muy similar a cuando teníamos Enolama. Enolama, teníamos todo este código propio dentro de un fichero lama.go. En este caso, está estructurado de otra forma, parece que mejor, que es a nivel de directorios y código concreto. Entonces, el código de la arquitectura Granite está en un fichero, pero dentro de la librería Transformers, ¿vale?, que es código Python. Entonces, miro este modeling Granite y, efectivamente, aquí vemos el código que define el modelo. El vocabulario, las capas ocultas, aquí, pues, sí, las, digamos, esas cabeceras que se ponen en los modelos, ¿no?, al final, que es lo que entrenas cuando haces fine tuning y tal. Bueno, pues, aquí se ve toda la arquitectura del modelo en concreto Granite. Y miro dentro de este directorio, models dentro de la librería Transformers y veo, pues, lo mismo, muchísimos, bueno, 298 arquitecturas de modelos, 298 arquitecturas de modelos dentro de la librería Transformers, ¿vale? Entonces, tenemos una situación muy parecida. Realmente no me bajo código cuando me bajo el modelo de Hacking Face, sí que me bajo código cuando me bajo la librería Transformers. Es decir, cuando hago un pip install Transformers, me estoy bajando el código fuente de todas las arquitecturas de todos los modelos que tiene la librería Transformers. Y cuando DeepSeq, que, por ejemplo, hasta hace muy poquito, igual todavía pasa, el último modelo de DeepSeq, el más grande, no está soportado por la librería Transformers. Y esto es porque, pues, no se ha aportado el código de ese modelo aquí. A lo mejor es que no se ha querido hacer open source, por ejemplo, y por eso no se ha aportado. Porque si lo aportas aquí, ya estás abriendo el código. Pero es eso algo que nos podríamos encontrar. Podemos ir a Hacking Face y ver que dice que la librería Transformers no soporta un modelo concreto y es porque el código no está aquí, ¿vale? Pero, entonces, aquí volvemos a lo mismo. Es decir, sobre los riesgos de los modelos que te bajas, no te puedes bajar un modelo. Es difícil, hasta ahora, y ahora veremos otros casos, los casos, digamos, en el límite. Es difícil bajarse un modelo. Si es de Hacking Face y es Safe Tensors, cuando tú te bajas el modelo, es una seguridad muy alta, ¿vale? Safe Tensors y te bajas el modelo. Y cuando trabajas con OLAMA, igual, por el tema del GGUF. Pero hay riesgos porque hay código, ¿vale? Hay código. Lo que pasa es que el código, en vez de estar en el propio modelo, está en la propia librería. Pero código hay. Y tendrá vulnerabilidades que, si no han salido y no están saliendo, saldrán en algún momento, ¿vale? Ahora, mientras veía esto e iba revisando los distintos modelos que había en Hacking Face, me di cuenta que había unos modelos especiales que les llaman modelos a medida o custom models en Hacking Face. Esto es un caso de Hacking Face. Y vamos a ver qué significa esto. Este, por ejemplo, fue el caso de un modelo de Microsoft, el modelo CI 3.5. Yo veía en esta lista de ficheros que te bajas que tiene ficheros Python directamente. Se suponía que los ficheros Python estaban en la librería Transformers, no que estaban aquí y yo me los bajaba cuando me bajaba el modelo. Entonces, esto incluso se lo comenté a una persona, es decir, es una persona que trabaja también en seguridad, que respeto muchísimo y también está metido en el tema de inteligencia artificial. Y le pregunté, oye, ¿y estos modelos que tienen ficheros Python? Y dice, hombre, yo creo que esto será para hacer configuración o algo del modelo, pero no creo que se ejecuten, ¿no? Lo digo porque, o sea, todo lo que queda por investigar aquí, ¿no? Es decir, que yo le preguntaba a esta persona que está muy metida en esto y tampoco encajaba estos ficheros Python, ¿qué hacían aquí? Él pensaba que no se tenían que ejecutar. Y digo yo, bueno, pues voy a verlo. Me voy a bajar este modelo y voy a ver si puedo hacer algo para ver si estos Python se ejecutan. Porque eso significaría, si yo realmente me bajo unos ficheros Python que están subidos en Houdinface y se ejecutan, entonces, ahí ya tenemos que esos ficheros Python podían tener malware y ejecutarse en mi ordenador, que es lo que yo estoy buscando desde el principio. Esa posibilidad existe. Entonces, bueno, están estos ficheros .py y veo el código. Este es el código que dice Houdinface que tengo que ejecutar para ejecutar este modelo. Y veo una cosa ya que. que le hace saltar las alarmas. Es un modelo de Microsoft, que no es un modelo de una organización desconocida. Pero hay algo que me llama la atención. Tengo que poner eso para cargar este modelo, Trust Remote Code True. O sea, específicamente tengo que decir que me fío del código remoto. Claro, entonces hay código remoto, ¿no? Hay código que en los otros casos no lo consideraban así y, además, no me pedían que me fiara. No hacía falta porque venía con la librería Transformers. Pero ahora hay unos ficheros .py y coincide que me está pidiendo el código que me fíe de código, ¿no? Bueno, vamos a verlo. Me bajo el modelo, no, ejecuto este código. Y me dice, me fijo en los mensajes y me da un mensaje que me dice, make sure, asegúrate de comprobar, de double check, que el código que te estás bajando no contiene código malicioso. O sea, ya me está avisando, se está quitando hiding face, se está quitando las culpas, ¿no? Dice, asegúrate de que tú compruebas de que el código que te estás bajando, que es este fichero .py que hemos visto en el listado, no contiene código malicioso. Y luego dice, para evitar bajarse versiones nuevas, puedes bloquear la versión y tal. Pero el caso es que se está bajando el fichero .py y, además, me está avisando que me fije que no contiene código malicioso. Como todo el mundo sabe, aquí el usuario para lo que está haciendo y dice, no, no, yo no me fío y va a comprobar el código. Pues, no, eso no pasa, ¿no? Entonces, bueno, aquí ya vemos que sí que existe esta posibilidad, ¿no? Existe esta posibilidad de bajarse, pero ¿realmente lo ejecuta o no lo ejecuta? Me está avisando. Todo parece decir que sí. Veo que efectivamente se baja. Cuando yo ejecuto este código, miro otra vez en los directorios y los .py están ahí. Y lo que hago es modificarlo, ¿no? Yo esto lo hice para un evento, de la fecha está mal, era 2025, ¿vale? Era al principio de este año. Meto un print dentro de uno de estos ficheros .py, lo ejecuto y la primera vez no pasa nada. O sea, parece que el fichero .py no se ha ejecutado. Hago un poco un análisis más detallado y es que había una especie de caching de los ficheros Python y tal. La cuestión es que encuentro dónde están cacheados y efectivamente modifico el .py y se ejecuta el código. Es decir, conclusión, ¿me estoy bajando código Python? Sí que me estoy bajando. O sea, hay por lo menos un caso o unos tipos de casos donde yo me puedo bajar ficheros Python directamente de Hacking Face que se ejecutan en mi ordenador. Es cierto que tengo que poner que me fío del código, pero es que esa variable que dice me fío del código viene por defecto en el código que te recomienda Hacking Face, que normalmente lo que hacemos ahí es copiar y pegar ese código y ejecutar. Entonces, sí que hay un riesgo significativo de bajarse un código que puede ser malicioso y que haga algo malo en nuestro ordenador cuando se ejecute, ¿vale? Hay ciertas medidas, ya digo, este parámetro que tienes que poner a true, pero si pensamos desde donde partimos, que es que pensábamos que lo que nos bajábamos cuando nos bajábamos un modelo solo eran floats hasta donde hemos llegado, que es que sí que existen modelos en Hacking Face que tienen código fuente y se ejecuta tal cual en nuestro ordenador, pues, es algo significativo. Entonces, busco más información de estos casos. Existe un concepto que es el de Custom Models en Hacking Face que dice, justamente viene a confirmar cosas que he estado, digamos, imaginando o he hecho una hipótesis. Dice, si el código no se ha incorporado a la librería Transformers y tú quieres compartir tu código, puedes usar Custom Models, que es directamente subir estos ficheros al repositorio de Hacking Face. Es decir, si no te lo hemos aceptado en la librería Transformers o tienes prisa, tú mismo puedes subir estos ficheros Python al código. Y en tu fichero de configuración config.json es donde lo indicas, ¿no? Estos ficheros Python que luego se van a ejecutar. Y ahora, damos ganancia. ¿Queen AOL tiene algún tipo de blog o es un blog de clientes? RICARDO ZTARAYA.: Sí. Es más, es un blog comunicano de cualquier tipo. Por lo menos con imagen de indajado. Entonces, cuando yos tú han explicado el tema, si tienes alguna respuesta, dale al signo, damos click en el botón Unfinder. Si el blog que habías purificado no es suficiente, puedes utilizar el signo del blog. Eh, 1, con 1 de 8 billones de parámetros, pues, lo mismo. También, eh, lo mismo, también parecido. Hay que hacer un trust remote code, igual a true, para poder ejecutarlo. Te vuelve a avisar. Es decir, no es un caso aislado. Hay más casos de estos custom models que se suben a Hugging Face, que tienen código Python, que se ejecuta en nuestro ordenador cuando los bajamos. ¿Vale? Un poco la conclusión que yo saco de esto es que, porque ahora comentaré un poco más y hablaré con los ficheros pickle, pero un poco la visión que yo empiezo a ver aquí es que tú, cuando te bajas un modelo, por lo menos, diolama, parece que todos son gguf. Pero, cuando te los bajas de Hugging Face, te puedes estar bajando modelos en diferentes formatos. Y, según el formato, tienes más riesgo de bajarte algo malo o no. ¿Vale? Entonces, gguf, riesgo bajo. Saved tensors, riesgo muy bajo. Custom models, riesgo medio guión alto. Pickle, riesgo alto. ¿Vale? Hice un poco, pues, estas conclusiones. ¿Son los modelos solo pesos? ¿Solo datos? No. Los modelos tienen una arquitectura y esa arquitectura está definida por un código. Ahora, ¿dónde está el código? Puede depender según el motor de inferencia, en olama dentro de un fichero olama.go o en Hugging Face dentro de un directorio transformers. Pero los modelos tienen una arquitectura y una arquitectura que se deriva en un código y luego unos pesos. Los modelos se componen de las 2 formas. Los modelos no son solo pesos. Y como tienen código, pues, hay aspectos de seguridad. Que se puede discutir, oye, pero es que si el código está en olama, el problema de seguridad no es del modelo, es de olama. Vale. Bien, es decir, pero el problema lo tendrá olama cuando está ejecutando el código que representa la arquitectura de Granite o que representa la arquitectura de Lama o la arquitectura de Quentu, ¿vale? O sea, es decir, hay riesgos asociados a ejecutar modelos locales con estos motores de inferencia porque hay código que se ejecuta. Y no es el mismo código para todos los modelos. Cada modelo tiene código diferente. Y esta pregunta, ¿no? Y también era una de las preguntas iniciales que yo me hacía. ¿Pueden contener malware los modelos que descargamos de internet? Pues, lo que he dicho, ¿no? En GGUF difícil, en Safe Tensors muy difícil, en modelos custom factible. Yo no he probado a subir malware directamente en un fichero de este tipo a Hacking Face. Ellos usan, pasan un antivirus, pasan 3 herramientas de seguridad. La última vez que miré pasaban 3 herramientas de seguridad y unos en antivirus. Pero yo me cuestiono si el antivirus que pasan tiene firmas para este tipo de código potencialmente malicioso. ¿Sí, Ernesto? A ver que, o sea, me parece un hallazgo espectacular y un estudio muy chulo. Pero, claro, es que incluso aquí, aunque pasen el antivirus, ¿podría, en este caso, tener simplemente un enlace a un virus de verdad? O sea, a otro virus que tienen esa descarga. No sé si me estoy explicando. Claro, podría haber, sí, como las técnicas que se hacen en GGUF que existen, que se descarga, que tienen un código que se descarga a otro código malicioso. Sí, ese código Python podría ejecutar cualquier cosa. Eso, por ejemplo, es una buena pregunta. Yo no he investigado si estos motores de inferencia ponen alguna limitación a lo que ese código Python puede hacer. Pero me extrañaría que lo hicieran. Y por eso digo, es que esa era más fácil. Es decir, yo no pongo nada. Todos sabemos cómo funcionan los antivirus de firma. Entonces, bueno, no creo que a nivel de antivirus de comportamiento, si te lo descargas en local, estén ahora mirando hacia dónde vas. Entonces, es una forma relativamente sencilla de, yo he hecho un troyano, he cogido un troyano y he hecho que no lo detectara cambiando, bueno, lo que hemos hecho todos los flores y jugando un poquito, ¿no? Cambiando un poco la configuración para que no detecte ningún antivirus. Entonces, claro, es que esto es relativamente fácil una vez que puedes poner ese código, ¿no? O sea, me parece increíble. Sí, sí, sí, sí. Sí, sí. Entonces, a ver, yo ya digo, a ver, esto, dices que es un poco lo que lo que ha comentado. Johanna, creo que lo ha aumentado Jenny, es decir, que sí, claro, probablemente lo quitarán del repositorio en cuanto se den cuenta, pero los primeros. No, no, ya están muertos, sí, sí. Y si le pones un nombre, un nombre muy, sabes, de mucha pompa, o sea, pues, lo que decía Johanna también creo, directamente te lo descargas y pasa desapercibido. O sea, piensas que estás descargándote algo bueno y has abierto un menón superinteresante, ¿eh? Ya digo, a nivel de, y a nivel empresa, es decir, porque a nivel personal, a ver, es como todo, ¿no? Es decir, tenemos una versión a riesgo concreta cada uno, dices, oye, yo me puedo arriesgar más. Pero una empresa grande que, además, esos modelos que se ejecutan en local, también los van a mirar más empresas. Por ejemplo, en tema de telecomunicaciones se está haciendo mucho tema de inteligencia artificial en empresas de telecos que quieren, además, tienen un tema por la seguridad importante. Que se plantean esto, ¿no? Dicen, oye, yo me bajo un modelo a local, ¿tengo riesgos o no tengo riesgos? ¿Tienes riesgos? Es decir, que los puedes controlar, pero de decir no hay riesgos a decir, ya tengo que entender los formatos que me estoy bajando, tengo que entender el motor de inferencia que está ejecutando los modelos para ver el riesgo que tienen o no tienen, de dónde me bajo los modelos. Es decir, hay riesgos. Y esto es como todo en seguridad. Es decir, pues, no usamos esto. No, claro, hay que usarlo, pero hay que entender los riesgos que hay que existen. Y esto viene mucho porque este es uno de los primeros formatos, que lo habéis comentado, en el que se distribuían modelos. Pickle es un formato, es una funcionalidad de Python que tú puedes convertir una clase de Python en bytes y luego esos bytes puedes volver a convertirlos en una clase de Python, ¿no? Que es serializar y deserializar. Entonces, Pickle, además, tiene una funcionalidad que cuando esos bytes se convierten en una clase puedes hacer que esa clase se instancie y se ejecute, por ejemplo, el código que haya en el método init, por así decir, ¿no? Entonces, digamos, es funcionalidad de Pickle el hecho que cuando se, vamos a decir, se deserializa, se convierte de bytes a una clase de Python, se ejecute código, ¿vale? Y esto, digamos, esto es el primer formato que se hizo, o sea, que se pasaban modelos y tenía este riesgo. Entonces, para eso se hizo SifTensors. Entonces, pues, se hizo SifTensors y se prohibió el formato Pickle en HuggingFace y ya está. No, claro, eso es lo que a mí también me llamó la atención luego haciendo esto. El tema está en que el formato Pickle se sigue utilizando, se sigue utilizando. Lo que hace HuggingFace es que pasa una herramienta que intenta detectar si el formato Pickle, no ya si el formato Pickle tiene código ejecutable, que no hace eso. Lo que hace es que mira el código ejecutable y trata de determinar si lo que ejecuta tienen métodos peligrosos. Cuando ya hay, digamos, esto es, estamos, o sea, estamos repitiendo los problemas, tal vez porque yo no tengo una solución buena. Desde luego, si la tuviera, pues, igual tendría que patentar. Pero volvemos otra vez a lo mismo de los antivirus, ¿no? Es decir, es una carrera del gato y el ratón. No de siempre, no de siempre, efectivamente. Entonces, es lo mismo, intentamos, el software de seguridad intenta determinar si este Pickle es malicioso y el formato y los malos que hacen. Bueno, pues, descubrieron una forma que el fichero, el formato de Pickle, le cambiaban unos bytes. Y, entonces, este programa que se llama Pickle Scan, que es el que utiliza Hugging Face, ya no podía analizar ese fichero Pickle, pero no daba error ni nada. Decía que no había encontrado nada, pero ese formato Pickle seguía funcionando. Y metieron malware en Hugging Face, es decir, esto ha pasado con este formato Pickle malintencionado en Hugging Face, un poco para hacer las pruebas. No era un malware ahí malísimo, sino era más bien como un experimento. Pero esto ha pasado, pues, en febrero de 2025, ¿vale? Que no es hace 3 años. Y yo, por ejemplo, mientras hacía esto, esto, por ejemplo, fue en marzo, pues, más o menos en marzo, 12 días hacía, antes de cuando yo tomé esta captura, pues, encontré un modelo, Hanus Pro, que estaba bastante descargado y demás, que tenía un formato BIN. Pickel y justamente Pickel Scan daba error, ¿no? Entonces, que el escáner de seguridad de un formato peligroso de error, pues, no da mucha confianza, ¿no? Entonces, bueno, esto también es HackingFace, está poniendo de su parte, a mí HackingFace es una empresa que me encanta todo lo que hace. Bueno, eso no quita que hay riesgos y que están intentando reducirlos, pero, bueno, aquí es el caso, ¿no? Esto es un modelo que está ahí en formato Pickel. Entonces, si sabemos esto, sabemos que poniendo el ratón encima de esos ficheros, hay unas etiquetas que pone safe, pues, hay una que vemos aquí una admiración. Pues, esos modelos tenemos que tener una especial precaución con ellos, ¿no? Si son este tipo de formatos, BIN, Pickel y, además, en los escáneres de HackingFace están fallando, pues, bueno, a ver, si necesitamos mucho, mucho usar ese modelo, pues, habrá que ver, a ver qué hacemos, pero pinta feo, ¿no? Entonces, medidas de, vale, entonces yo digo, vale, medidas de seguridad para proteger los modelos de, para proteger a los usuarios, ¿no? De los modelos y también la distribución de modelos, ¿no? Yo, si quiero distribuir un modelo, ¿qué hago para que mis usuarios tengan menos riesgo? Bueno, los usuarios pueden descargarlos de fuentes confiables o intentar, pues, lo habéis dicho, ¿no? Fijarse en qué organización es la que ha subido ese modelo. Puede ser que la organización Microsoft en HackingFace suba un fichero con malware, sí. Pero las probabilidades son mucho más bajas que si es la organización Microsoft MM, ¿no? Con unas letras añadidas o cambiadas letras o con un 1, me refiero. Organizaciones que no reconocemos. También lo que se está trabajando mucho es en ver cómo firmamos los modelos, ¿vale? Igual que los binarios se pueden firmar y comprobar la firma cuando tú la has bajado y saber que esa firma significa que ese paquete no ha sido modificado. O sea, que lo creó, por ejemplo, Microsoft o lo creó Red Hat y lo firmó y desde que lo firmó no ha cambiado, ¿vale? Eso se hace con técnicas criptográficas. Pues, también vamos a firmar los modelos, ¿vale? Aquí también os comento, esto creo que es muy interesante, hay muchas organizaciones relacionadas con el open source, pero son organizaciones abiertas que cualquiera puede participar. Es decir, es una manera de, digamos, introducirse, ganarse una reputación. Yo a nivel laboral, claro, no sé lo que hacéis, lo que estáis buscando, si tenéis un sitio, si buscáis otras cosas, pero una manera de relacionarse con empresas grandes, con empresas Microsoft, Google, NVIDIA, Red Hat, Amazon los he visto menos, pero Microsoft, Google, NVIDIA, Red Hat están en todas estas organizaciones que son abiertas, que cualquiera de vosotros podéis apuntaros a una reunión, son online, no tenéis por qué, vamos a decir, hablar. A veces podéis ver lo que se habla, luego participar, contribuir a documentos que hacen. Y son organizaciones donde se hacen, donde se hablan estos estándares. Es decir, hay un repositorio en GitHub que se llama Model Transparency, Model-Transparency, luego os puedo pasar el enlace, que es donde se está hablando mucho de cómo hacer esta firma digital de modelos. Y luego hay organizaciones, OpenSSF, AI Alliance, COSAI, COSAI, que es una organización que se llama Oasis, que también hacen estándares, han hecho muchos estándares de XML. PABLO SANGUINETTI. Florencio, perdona, ¿podrías pasar luego por este mismo chat estas organizaciones que estás comentando? FLORENCIO DELGADO. Sí. PABLO SANGUINETTI. Porque yo no las conozco y me vendría bien por echarle un vistazo. FLORENCIO DELGADO. Sí, sí, sí. Pues, está muy bien, ya digo, yo parte de mi rol, de mi trabajo es intentar participar lo máximo en ese tipo de organizaciones. Y también, por supuesto, tratar de influir hacia lo que yo creo que es lo correcto de cómo deberían ser estos estándares, pero que cualquiera puede participar. Y es muy interesante. Y también escaneos, escaneos de vulnerabilidades y de malware de estos modelos, ¿no? Yo era el primero que al principio de esto también repetía como un loro el tema de que los modelos eran solo pesos, lo reconozco, yo al principio también estuve ahí. Y pensaba que no hacía falta escanear estos modelos para ver si tenían malware, pero sí, sí. Lo que he visto es que sí que se va a necesitar que se analicen también estos modelos. ¿Vale? Vale, entonces. Sobre esto que hemos visto de la seguridad propia de los modelos, ahora tengo ya, queda poco, quedan pocas transparencias, ¿vale? Pero ya voy a pasar a hablar un poquito de lo que es la seguridad de las librerías, ¿no? Que lo hemos comentado al principio y también Johanna lo ha expresado, ¿no? Dice, oye, cuando me instalo algo en estas librerías, ¿quién mira la seguridad, qué seguridad tiene? Ya no hablo de los modelos en sí, sino el propio PyTorch, ¿no? O TensorFlow u otras, muchas. Voy a comentar aspectos de seguridad de estas herramientas. Pero antes, si tenéis alguna pregunta, curiosidad, comentario de esto del tema de los modelos. Bueno, yo solamente comentar de que la verdad es que flipé harto con lo que presentaste. Porque, claro, uno ha escuchado muchísimo este tema y cuando dicen, bueno, tú estás viendo el tema de inteligencia artificial y qué opinión tiene uno, es súper difícil saber si eran seguros o no. Era más bien lo que estaba diciendo la industria, pero creo que diste unos puntos súper interesantes sobre algo que es posible para uno identificar. No digo que todo, porque claramente falta la experiencia. Pero ya poder identificar ese tipo de cosas como tú hiciste ese barrido, ese seguimiento de en qué fijarse, que es un mínimo criterio cuando uno ya es profesional, no, yo te lo agradezco un montón. De hecho, que yo no sabía qué mirar en el código para decir, oye, sí, mira, esto podría ser. Más que lo que mencionó Johanna delante que, claro, ya ahí habíamos visto un poco la referencia respecto a los formatos con los cuales se guardaban los modelos. Pero más que eso, era como un poco ambiguo saber. Así que, no, te agradezco ese punto, pero a mí me flipó. Bueno, a ver, Matías. Sí, igual te felicito. Creo que es genial la investigación que estás haciendo. Responde a muchas dudas que teníamos. Al menos a mí me sirve mucho para ya tener una opinión más fundamentada. Porque yo te diría que tenía la curiosidad y tenía este tema de decir, oye, pero esto no está bien, esto no es seguro, esto hay que verlo. Pero como decía Jenny, no sabes tú mucho por dónde hacerlo. Y de hecho, ahora mismo yo estoy con el proyecto que tengo para la doctora. Oye, precisamente sobre la seguridad de los modelos, pero yo quiero estudiarlo a nivel multimodal. Porque ahí veo que también hay muchos agujeros. Entonces, la forma en que tú has enfrentado esto, el hacerte las preguntas, me parece súper útil. Porque yo me hacía las preguntas, pero me quedaba un poco en el aire. Y yo conversaba con Jenny y nos quedamos mirando y es como, ¿qué hacemos? Entonces, el ver cómo tú has ido paso a paso es muy útil para ya nosotras también hacer nuestras propias investigaciones sobre estos temas. Porque yo creo, como tú dices, que está todo por hacer en seguridad de IA. Y lo de los modelos y todo, yo veo que, claro, uno descarga cualquier cosa. Bueno, yo me fijo, pero sí me ha pasado de descargar cosas extrañas de GitHub, por ejemplo. Y esos links de GitHub están en HackingFace, por ejemplo. Porque a veces te dicen en los multimodales, mira, la parte de texto la suben en HackingFace, pero la parte de las imágenes te la dejo en un repo de GitHub. Correcto. Entonces, por eso te digo que a mí eso me ha causado, como yo estoy trabajando con los multimodales, me ha causado mucho, mucho reparos de seguridad y ganas de investigarlo. Y, bueno, paso el aviso, si hay oportunidades de investigación que tú sepas, let me know, please. Muy bien, muy bien. Hay otra cosa también, ahora hablando de estos temas, que puede ser interesante, que es cuando yo, o sea, al principio os he comentado que hay un momento que yo tengo la duda de si a lo mejor el código de los modelos está bajando en tiempo real de internet, ¿no? Cuando se ejecuta el modelo. Antes de ejecutarse el modelo sí se baja, ¿no? Esa arquitectura. Y, entonces, no veo eso, pero veo una cosa curiosa que no investigué mucho, pero sí que por lo menos la vi y dije, esto es esto, que es que todos estos motores, tanto OLAMA, mira, OLAMA, para ser sincero, no me acuerdo exactamente. Lo vi en HackingFace y luego lo vi en VLLM, que hice algunas pruebas también y lo vi. En HackingFace y en VLLM, aunque tú estés cargando el modelo en local y no necesite conexión a internet para nada, hace una conexión a internet. Y se comunica, en HackingFace se comunica con un dominio de HackingFace y en VLLM también con un dominio de VLLM, creo que es. Entonces, luego investigando, digo, a ver, si yo estoy ejecutando esto en local y no se está bajando el código del modelo de internet, ¿por qué hace una comunicación a internet? Y no lo hacía cuando yo le hacía preguntas, lo hacía al principio cuando cargaba el modelo. Entonces, llegué a la conclusión, porque luego encontré además ciertas opciones por ahí, que es el tema de. Telemetría, ¿no? Ahora está muy de moda el tema de la, bueno, supongo que desde hace tiempo, pero el tema de la telemetría, que es software que envía información a la nube por distintas razones. No tienen por qué ser datos tuyos, privados, pero son para ellos saber, por ejemplo, el número de usuarios que tienen, quién lo usa. Entonces, estas herramientas lo hacen. El API de HavingFace se conecta a HavingFace, probablemente para saber que alguien está utilizando su API. Y VLM envía información de, son características técnicas. Se puede desactivar, pero si no lo sabes, por defecto está habilitado. Entonces, tú por defecto estás usando un programa que está informando a su nave nodriza, ¿no? A su base, oye, estoy siendo utilizado. Y no tendría por qué porque no da ningún valor al usuario. Pero eso es también un aspecto para investigar qué envía exactamente, cuándo, por qué. Eso sí es interesante. Pero, bueno, como ya vi que no era código y yo estaba buscando el código, no seguí por ahí. Vale, pues ahora voy a comentar un poco, porque ahora y también conectándolo con lo anterior de alguna forma, hemos dicho que, vale, ahora hay código. Ya sabemos que, efectivamente, los modelos tienen código. Está el caso de los Custom Models de HavingFace, que ahí te bajas el Python tal cual. Pero, bueno, vamos a suponer que, y no lo es, no es el caso general. El caso general es que el código está en OLAMA o está en Transformers, en API Transformers, o en VLM estará en otro sitio, ¿vale? Vale, ahora este código puede tener vulnerabilidades, ¿no? La segunda cosa, cuando yo empiezo a hablar de esto, es, bueno, pero es que ese código es código que básicamente manipula matrices y son, vamos a decir, son operaciones matemáticas, por así decir, muy básicas. Y ahí es raro o imposible, vamos a decir, muy raro, que haya vulnerabilidades. Entonces, aquí la afirmación que a mí me genera curiosidad es, si este código de PyTorch puede tener vulnerabilidades de seguridad. Si el código de TensorFlow, este código que sé que yo me encuentro en OLAMA, que es diferente para cada modelo, o me encuentro en la librería Transformers, sí puede tener vulnerabilidades de seguridad. Y encuentro que, bueno, y otro aspecto que me puede hacer pensar que no es que esto es código Python, ¿vale? Es código Python. El código Python es un código de alto nivel que hace una gestión de memoria de tal manera que es difícil que tenga vulnerabilidades en la gestión de memoria, que suelen ser las vulnerabilidades más graves. Es decir, en programas de Python no suele haber, es muy raro, vulnerabilidades relacionadas con la memoria, que son las más graves en seguridad. Eso suena muy bien, pero es a cambio de que Python es mucho más lento que C o C++, porque precisamente hace esta gestión de memoria. C, C++ no hace esta gestión de memoria, se la deja al programador, ¿no? Dice, yo no la hago para ser más rápido, yo espero que tú sepas lo que estás haciendo, programador, ¿vale? Y eso es lo que nos ha llevado a años de problemas de seguridad en código de C, C++. Que se han ido corrigiendo y hay mucho trabajo detrás y gente súper profesional, pero es que es fácil equivocarse y tener un fallo de seguridad en C, C++. Pero en Python no, en Python es muy difícil, muy difícil que haya un problema de seguridad de este tipo, relacionado con la gestión de memoria. Pero, entonces, empiezo a mirar, a entrenarme un poco más y, claro, tiene un poco de truco. PyTorch es Python, TensorFlow es Python, pero también hay versiones en C, C++, pero no solamente es que haya versiones en C, C++, es que hay una cosa que les llaman operadores, que son las operaciones fundamentales que hacen estas operaciones con matrices que están por debajo, están a más bajo nivel. Entonces, si tú tienes un código Python, en PyTorch tú haces código Python, pero al final se llaman a unas funciones que están hechas en C y en C++. Esto es así por lo que hemos dicho de eficiencia, es decir, no es ningún secreto que para la inteligencia artificial necesitamos toda la eficiencia. que podamos. Entonces, claro, era raro pensar que esto se estaba haciendo con código Python. En el fondo, en las operaciones que importan, por así decir, hay código C, C++ por debajo. Entonces, lo que se ha visto, lo que se ha investigado es, bueno, primero, que este código Python, código hecho en PyTorch y código en TensorFlow, puede tener vulnerabilidades de memoria graves de seguridad porque hay código C, C++. Y se pueden encontrar, hay una técnica que se llama fusing, que consiste en generar datos mal formados o, por ejemplo, muy largos, muy pequeños, con caracteres especiales, etcétera. Y pasárselos muy rápidamente a un programa para ver si falla y para ver cómo falla. Entonces, hacer fusing de Python generalmente es una tontería, una pérdida de tiempo por lo que hemos dicho de lo que hace Python. Pero, en este caso, unos investigadores ya vieron que, a través de las funciones de Python, fuseando funciones de Python, se puede alcanzar el código C, C++. Ahí hay 2 papers que explican cómo lo hicieron ellos, Ibisyn y con FL, donde hacen fusing de estas librerías y encontraron fallos de seguridad graves en PyTorch, en TensorFlow y en otras librerías de estos lenguajes. Es decir, ¿quién mira la seguridad de estas librerías, sobre todo de las librerías más populares? Hay mucha gente encima mirando cómo funcionan y encontrando fallos de seguridad, reportándolos y corrigiéndolos. Red Hat es una empresa, por ejemplo, que hace esto con este tipo de software. ¿Significa que no tienen fallos de seguridad? No. Los tienen y no se han encontrado. Y, porque, digamos, es una ley del software. Y luego, aparte, este código va cambiando y en el código que cambia, en el código nuevo, se introducen nuevas vulnerabilidades de seguridad. Entonces, aunque aparentemente estas librerías están hechas en Python, donde es más difícil tener estos problemas graves de seguridad, por debajo hay C, C++ y pueden tener estos problemas graves de seguridad. Y, de hecho, los tienen y se han encontrado fallos. Y otra, disculpa, Florencia, ahí, no sé por dónde va a avanzar, pero justamente con esto, bueno, no sé si tan profundo o no, pero a mí, por ejemplo, lo que me ha llamado la atención es que también se está, o sea, que uno al principio podría decir, mira, tenemos un montón de vulnerabilidades, pero en la medida que va pasando el tiempo vamos a ir acotando. Pero esto también como que se va ampliando. Porque yo lo que he visto es que, además de Python, se están incorporando en librerías con TypeScript, que en el fondo es JavaScript. Y yo que he trabajado con JavaScript, el gran tema de cómo he acostado un montón que sea un estándar por todos los temas de seguridad que tienes el lenguaje y que ahora esté así como parte de las librerías, no es menor. PAULO MONTAÑAZO SÁNCHEZ POR INTÉRPRETE Sí, a ver, ese tema en general es muy difícil. Es decir, el tema de las vulnerabilidades en el software, cómo hacer que vayan a menos en vez de a más y tal, sí, no está para nada solucionado. La idea sería que realmente evolucionáramos hacia frameworks de desarrollo que sea difícil cometer fallos de seguridad, ¿no? Por ejemplo, Rust para el tema de la industria de seguridad es importante porque ya va, es muy difícil tener un fallo de seguridad grave en Rust, pero también es difícil de programar, requiere una curva de aprendizaje, no podemos reprogramar millones y millones de líneas de código en Rust de la noche a la mañana. Entonces, vamos, en ese sentido hay trabajo de seguridad. Y, bueno, y esto he hablado de fallos de seguridad graves relacionados con la gestión de memoria, pero es que, por ejemplo, PyTorch, yo estuve implicado en un fallo de seguridad que alguien identificó. Bueno, de hecho, PyTorch dice que no es un problema de seguridad, que es una feature, es una característica. Y es que PyTorch tiene una capacidad para ejecución distribuida. O sea, de alguna forma puedes tener distintas instancias de PyTorch ejecutándose en distintos servidores y que se comuniquen entre ellas para, entre todas, ejecutar una tarea. Bueno, pues, cuando PyTorch funciona así, estas instancias de PyTorch que están escuchando aceptan conexión de cualquiera sin autenticar y ejecutan lo que se les diga. O sea, ejecución remota de código sin autenticar, que es como el fallo de seguridad más grave. El argumento de ellos para no solucionarlo es que eso está pensado así por temas de eficiencia. Ciencia y porque lo que ellos dicen es que esta arquitectura distribuida de PyTorch está pensada para ser ejecutada en un entorno cerrado, donde todos los elementos, digamos, los agentes que hay ahí, todos los participantes son confiables. La seguridad a veces es muy difícil, ¿no? En este caso ellos dicen que es así, está documentado, avisan a los usuarios de que esto funciona así. A los de seguridad no nos gusta poder ejecutar comandos en una máquina sin ni siquiera autenticarnos. ¿Y qué sería? ¿Que es que la red es confiable? Pues, toda la arquitectura está de cero trust. Si lo habéis oído, pues, está en contra de esto. Por otro lado, entiendo que a lo mejor hay que fijarse más en otras cosas de seguridad antes que en esto. Pero, bueno, esto es, pues, que también transmitir que todos estos frameworks nuevos de inteligencia artificial que corren mucho, que quieren llegar a mucha funcionalidad rápido, aunque sean grandes y conocidos, eso no garantiza ni que sean open source, garantiza que sean seguros. Normalmente en seguridad siempre nos tenemos que cuestionar las cosas, tratar de entenderlas, que nos cuadren y aún así tendremos riesgos. Pero no podemos confiar ciegamente en lo que se nos dice o en lo que pensamos porque se ha demostrado una y otra vez que hay riesgos, ¿no? Sobre el tema de las dependencias, básicamente tengo eso. También quería comentar, por lo que hemos dicho antes, bueno, entonces, yo que me bajo, tengo mucho riesgo si me bajo librerías, riesgo hay, pero, por ejemplo, librerías que estén, o sea, normalmente todos estos repositorios, por ejemplo, de pip que bajamos muchas dependencias y están haciendo cosas para reducir al máximo la posibilidad de que haya problemas de seguridad. Los problemas de seguridad más frecuentes relacionados con dependencias es que nosotros nos equivoquemos al teclear la dependencia. Cuando ponemos el import, import transformers, pues nos olvidemos de la S última y pongamos import transformer. Pues a lo mejor un malo ha creado una librería transformer esperando que alguien se equivoque y que se baje la suya. Y, entonces, esa librería hace algo malo. Eso pasa a pasado y los que gestionan estos repositorios de dependencias de vez en cuando hacen limpieza y quitan todo lo que encuentran que es basura o malo, pero esto pasa. Entonces, casi más depende de nosotros porque cuando importamos dependencias, pues nos fijemos y vemos algo que no nos cuadra, pues lo revisemos y demás. También es buena práctica usar cuantas menos dependencias mejor, pero sin volvernos locos. Es decir, al final tenemos que usar las dependencias que necesitemos para trabajar. Pero, bueno, que hay por ahí dependencias que es casi de broma, ¿no? Para determinar si un número es par o impar una dependencia. Pues un poco de orgullo de decir, nos vamos a bajar una dependencia para ver si un número es par o impar, ¿no? Vamos a hacerlo de otra forma. Eso es lo único, intentar reducir el número de dependencias que usamos, intentar utilizar repositorios conocidos. Y, sobre todo, pues eso, estar más o menos al tanto de cómo se mueve la industria para detectar problemas. VIRGINIA PEDULLA MARTÍNEZ Los plugins de Visual Studio también hay que tener ojo. Los plugins de Visual Studio, hay estudios que dicen que tienen poca seguridad. Es decir, que el propio Visual Studio no controla mucho lo que pueden hacer los plugins y no pueden hacer y demás. Entonces, tienen riesgo. Yo no he escuchado, no lo he buscado a breve, pero no he escuchado casos reales de, oye, plugins con malware. No lo he escuchado, pero existe riesgo. Toma, yo insisto, que yo soy de seguridad y que no hay que volverse locos con el riesgo. Digamos que hay riesgo, no significa que no usemos plugins de Visual Studio. Yo voy a seguir usando plugins de Visual Studio porque necesito trabajar, necesito funcionalidad, pero sé que hay un riesgo. Si, por ejemplo, un día me recomiendan un plugin o veo un plugin por ahí, veo la descripción, está todo en un idioma que no entiendo, tiene pocas descargas, pues, a lo mejor no me lo descargo porque sé que puedo tener un problema. Si yo miro el plugin, tiene 4 estrellas y media, un millón de descargas, pues. ¿Puedo tener malware? Sí, pero tiendo a fiarme. Ahora, por ejemplo, eso mismo, pero estoy en un servidor, no estoy en mi máquina de trabajo. Pues, no lo hago. En un servidor a lo mejor utilizo BIM o utilizo un editor lo más plano posible. No instalo visual estudio en un servidor porque me es más cómodo, porque un servidor tiene más riesgo. Entonces, es un poco jugar con el riesgo y lo que es aceptable y no aceptable en cada momento. Pero en los típicos, las actualizaciones, que eso es fundamental. Porque hay muchos proyectos que quedan abandonados, básicamente. Y esos son particularmente, para mí, un caballo de Troya. Ese tipo de librerías que el proyecto quedó abandonado por los originales, digamos. Sí. Bueno, esto ya ha pasado, ¿no? Ha pasado en proyectos grandes open source que alguien ha trabajado como contribuidor un tiempo y al cabo de un año, esto fue como una operación súper avanzada, ¿no? Que al cabo de un año introdujo un malware y, además, un malware muy, muy difícil de detectar en el código. Bueno, es que ese riesgo existe, igual que existe en software cerrado. Es decir, en software cerrado no lo vemos, pero un trabajador, digamos, que se cuela en una empresa a trabajar y está trabajando un tiempo y luego, al cabo de un tiempo, introduce una puerta trasera en el software. Al final, en seguridad, esto lo tratamos con tener muchas capas de seguridad, no solo una. Es decir, por ejemplo, lo del ordenador. Bueno, pues, tener copia de seguridad. Sobre todo, si utilizamos Windows, tener un anti-malware. Si utilizamos Linux, pues, trabajar con el usuario que no es root, solo subir a root cuando es necesario. Es decir, más o menos intentar hacernos a buenas prácticas reduce el riesgo. Vale, y, entonces, ahora hemos hablado de la seguridad de los modelos y hemos hablado de la seguridad de las aplicaciones y las dependencias. Muy, muy por encima, ¿no? De los modelos es lo que más hemos hablado en profundidad, de las aplicaciones y las dependencias un poco. Y ahora hablamos un poco de la plataforma. Entonces, esto también es, vale, yo ya voy conociendo todos los riesgos que hay en inteligencia artificial, desde esta pata que hemos dicho al principio, ¿no? Desde que los malos utilicen temas de inteligencia artificial para hackear a los usuarios. Vale, ahora, ¿qué tiene que tener o por qué me tengo que preocupar en la plataforma, no? Si, por ejemplo, yo soy el que desarrolla VLM, ¿qué hago para que los usuarios estén más seguros? VLM o, sí, o plataformas que ejecutan modelos. Por ejemplo, sería muy útil que Olama, un poco lo que ha preguntado Ernesto, ¿no? Es decir, que cuando Olama ejecuta un modelo, no le permita a este modelo, ¿no? En el caso de los modelos custom de Hugging Face, no les permita hacer cualquier cosa que esté limitado a lo que puedan hacer. Por ejemplo, esos modelos no tienen por qué conectarse a la red ni tienen por qué acceder al disco, ¿no? Lo único que van a hacer es ejecutar operaciones matriciales, pues, que eso esté limitado. ¿Eso estaría bien? La superficie de ataque, ¿no? Que hablábamos al principio, ¿cómo reduzco la probabilidad de un problema de seguridad ahí? El tema de guardrails, ¿no? Que hablábamos, esto es más para lo que genera el modelo, pero estaría bien que si usamos una plataforma que ejecuta modelos, pues, yo tengo un software de guardrails que vigile lo que el modelo contesta. Y, por ejemplo, yo soy una empresa que despliego un chatbot para mis clientes. Pues, yo no quiero que el chatbot nunca diga, por ejemplo, un insulto o que nunca dé un DNI, un número de tarjeta, un número de cuenta. Pues, yo tengo un software que cuando el chatbot va a contestar al usuario, primero va a este software, que es el guardrails. El guardrails lee lo que ha contestado el bot y dice, no, no, espera, estás contestando con un DNI. Y lo elimina de la respuesta. Lo elimina o bloquea la respuesta o hace alguna acción que configuremos para gestionar eso, ¿vale? Pues, eso es lo que hacen los guardrails. Y eso nos lo debería proporcionar la plataforma. Tener un registro de modelos, ¿no? Igual que hablábamos del Hacking Face, o sea, el Hacking Face es un registro de modelos, pero empresas grandes que tengan muchos modelos los podrían tener en local, ¿no? Para así también se ahorran bajarse 5 gigas cada vez de Hacking. HackingFace, lo tienen en la red interna o en su instancia en la nube y tienen un registro de modelos. Y, además, el registro de modelo que tienen les hace los propios escaneos de seguridad de los modelos y demás, ¿no? Pues, esto es algo que estaría bien que proporcionara la plataforma. Almacenamiento y gestión de datasets, ¿no? El tema del poisoning de modelos viene mucho por haber manipulado datasets. Pues, también queremos tener los datasets, ¿no? HackingFace también tiene un catálogo de datasets, pero tener un software nuestro en local o en nuestra instancia en la nube. Unos 500,000 tiene HackingFace más o menos, 400 y pico mil largos. Y, luego, el tema de la monitorización, que es monitorizar, pues, que los modelos, las aplicaciones, los componentes estén funcionando bien, ¿no? Esto sería un ejemplo de una arquitectura donde están muchos de estos componentes muy desglosada, ¿no? Porque a veces estos componentes le pueden hacer 2 cosas la misma pieza. Pero, bueno, tenemos, se empieza a leer un poco por la izquierda, ¿no? Tenemos el dispositivo del usuario que accede a un frontend, esto accede a un API gateway, donde se hace una autorización, ¿no?, y una autenticación para dejar pasar al usuario o no. El guardrails que está antes y después de la contestación del modelo, ¿no? La orquestación de modelos. Normalmente una plataforma de este tipo puede tener varios modelos, ¿no? Y se decide qué modelo usar cada vez. Incluso también modelos externos, no solo locales, ¿vale? Con su model registry, el registro de modelos, que puede hacer también de puente a Hacking Face o otros registros. Y luego también eso pasa si hay un tema de RAG y demás, ¿vale? Y luego también login, monitorización. Entonces, es una imagen de una arquitectura viendo un poco todas las piezas. Ya digo, muy desglosada. alogio número de explicacion Y modelos de, o sea, y medidas de seguridad en la plataforma. He hablado un poco de funcionalidad, pero, bueno, otra que se habla mucho referente a seguridad y los modelos. No sé si habréis oído hablar de una cosa que se llama matchedS bombs. Los matchedS bombs son básicamente un inventario de los componentes que tiene un software. Sobre todo se ve la utilidad en un software cerrado. No sé, a ti te venden Excel y te dan el S-bomb, en el S-bomb dice todas las librerías que usa Excel, ¿vale? Esto intentó ser una ley americana que obligaba a que todo el software tuviera S-bombs, pero ahora con el cambio de gobierno, pues, parece que eso se ha enfriado, ¿vale? Pero el concepto sigue existiendo, más que ser una obligación, pues, el concepto existe. Y existe también para la inteligencia artificial. Se está trabajando en estas organizaciones que digo abiertas, se está trabajando en un estándar para hacer AI bombs que son, a ti te venderían o tú te bajarías o te venderían un software de inteligencia artificial y vendría con un fichero que dice toda la inteligencia artificial que usa, por ejemplo, los protocolos, los algoritmos, los modelos, de dónde vienen esos modelos, ¿no? Las características de los modelos, la firma. Eso sería ideal, ¿no? Y sería lo bueno y hacia donde debemos ir. Salió una noticia de que había un modelo de estos de imágenes que se había entrenado con imágenes de niños, ¿vale? De niños, pues, en tema de, no sé si desnudos o, o sea, se había entrenado con este tipo de imágenes y era un modelo que muchas empresas estaban utilizando porque era un modelo genérico de imágenes. Lo que pasa es que alguien se dio cuenta que se había entrenado también con ese tipo de imágenes porque los habían cogido de, no sé qué, repositorio o no sé qué historia. La cuestión era que las empresas no sabían, había empresas grandes que estaban usando ese modelo y tenían muchos problemas. Primero, si habían creado otros modelos basados en ese, no tenían esa conexión. A lo mejor la habían perdido. No lo sabían, seguro. No tenían esa información a mano. Y luego tampoco sabían en qué aplicaciones estaban usando esos modelos. Entonces, esto de los iBombs, el provenance y el lineage, está relacionado con eso, ¿no? La visión es que las empresas tendrán la posibilidad de tener mapeado en qué aplicaciones están usando qué modelos, qué modelos han entrenado en base a qué modelos. Y ahora estamos en el principio, ahora las empresas están probando con 2 o 3 modelos. Pero cuando pasen los años y tengan cientos de modelos o en las arquitecturas de agentes, que va a haber muchos modelos pequeños, a ver, ¿cuántos modelos estoy utilizando? ¿Dónde? ¿De dónde vienen? ¿Quién los hizo originalmente? ¿Con qué datos se entrenaron? Perdón, tienen datos que pueden tener problemas de licencias, tienen datos que pueden tener problemas con imágenes. O si sale una noticia como esta, de repente se ha descubierto que este modelo está envenenado y tiene datos malos. ¿Dónde estoy utilizando ese modelo? ¿Para qué lo he usado? Incluso ha pasado, ¿no? Y, oye, a clientes, pues, paséis este modelo en el pasado que sepáis que hay este problema. O sea, una gestión de lo relacionado con esos modelos. Escaneos de seguridad, Pick&Scan. Escaneos de la evaluación en sí de la seguridad de los modelos. LMEval es una herramienta open source que hace esto. Gara, que es otra herramienta muy buena. Y luego ahí, pues, también comento sobre temas de Garbail, seguridad en ejecución y demás. Muchos aspectos, muchos aspectos para investigar. Conclusiones hacia alto nivel de lo que hemos estado viendo. Los modelos de IA que consumimos tienen riesgos, lo sabíamos, pero lo hemos visto un poco más científicamente. Una cosa primera, descargarlos de fuentes fiables. También el software que utilizamos para crearlos. Y que se está trabajando mucho en esto. Es decir, las empresas que hacen estos tipos de desarrollos, no solo Red Hat, todas las empresas están trabajando mucho en todos estos aspectos que hemos comentado, pero estamos un poco al principio. Y aquí es, pues, lo que os comentaba al principio, ¿no? Ahora se habla mucho del tema de los agentes, MCP. No lo voy a comentar con detalle porque ya estamos hacia el final y quiero también si hay temas de preguntas, pero MCP es este protocolo para manejar herramientas y también tiene sus riesgos de seguridad. Y A2A que es un, A2A es un estándar que ha definido Google. Y es curioso cómo funciona. Yo eso tampoco lo tenía claro antes de entrar en Red Hat y, soy sincero, todavía lo estoy aprendiendo. O sea, Google desarrolla este estándar y se lo regala a una organización que se llama, en este caso, creo que ha sido ACN, no sé si es, a ver, ACNCF. CNCF es una organización que tiene muchos proyectos relacionados con contenedores, con Cloud Native. Y esto lo desarrolla una empresa y luego se lo proporciona una organización que trabaja, digamos, de manera abierta. Y esa organización es la que adopta el proyecto y lo mantiene en el tiempo. O sea, ya no está asociado solo con una empresa, sino que un conjunto de empresas trabajan en ello para mejorarlo. Entonces, ese es el caso de A2A. ¿Vale? Y eso es. Preguntas, curiosidades, dudas. PAULA MONTAÑA ZAMORRODO Sí, yo tengo unas cuantas que no te quería interrumpir porque está tan interesante que no quería cortar el hilo. Pero, a ver, primero, uno de los modelos que nosotros usamos bastante en Computer Vision y que tuvo problemas de seguridad fue YOLO. YOLO 11 en diciembre del año pasado tuvo un problema de seguridad grave, un ataque bien sofisticado con el repositorio GitHub. Entonces, eso se hizo porque hicieron un fork de un release. Y al fork del release después le pusieron un pool con el código malicioso. Y lograron pasar la revisión que hicieron al principio, antes de la publicación, pero sí había pasado una revisión. Entonces, hubo gente que lo descargó. Y, claro, notó, empezó a reportar los problemas porque empezaron a tener problemas con la CPU que se le disparaba como si estuvieran haciendo la minería de cripto. PAULA MONTAÑA ZAMORRODO Exactivamente, seguramente sería eso, sí. YOLO 11 Claro. Entonces, ahí, bueno, se descubrió. Pero lo gracioso dentro de todo es que se sacó, bueno, una versión que se supone que estaba bien, pero luego a los días el atacante volvió a liberar 2 versiones más maliciosas. Entonces, la verdad es que era bien sofisticado el ataque porque lograba burlar bien fácilmente las medidas de revisión que tenían. Y Ultralytics es una empresa enorme. O sea, no estamos hablando aquí de cualquier empresa que tenía cualquier modelo. Entonces, para mí fue importante ese caso porque refleja que, en realidad, por mucho que uno se fíe de empresas grandes, el riesgo siempre está. Ahora, generalmente lo tajan rápido. Y, claro, cuando tú, ese es el riesgo de descargarte así como lo último de lo último, por eso yo siempre espero que la versión se estabilice, que la prueben otro, pero trato de no probar siempre las primeras versiones porque pasan cosas como esta. PAULA MONTAÑA ZAMORRODO Sí, eso reduce el riesgo. De hecho, si se puede esperar, es una medida que sí, efectivamente, reduce el riesgo. Por lo que tú dices, que cuando alguien ya lo prueba, pues es más fácil que encuentre el problema. No, no lo reduce a cero. Pero no, lo sabemos porque hay fallos que pueden estar ahí o problemas largo tiempo sin detectar, pero lo reduce, lo reduce mucho. Sí, el otro tema que iba a comentar es eso que comentaste de las imágenes de niños en el modelo. Bueno, ese es un tema ahora que está surgiendo, o sea, no las imágenes en sí, sino que hay grandes modelos, sobre todo esto se da con imágenes, grandes modelos alimentados con cientos y miles de imágenes y que te pueden dar buenos resultados. Esto se ve más que nada en redes neuronales. Te pueden dar muy buenos resultados cuando tú buscas algo específico, pero el problema es la explicabilidad. Yo, por ejemplo, que estoy en el grupo de investigación con temas de salud, estoy haciendo, bueno, estuve haciendo una siamesa por temas de detección de cáncer. Entonces, el tema está que los modelos preentrenados con estas miles de imágenes daban buenos resultados, pero a ti a nivel médico no puedes confiar en ese modelo porque tú no sabes en realidad las imágenes que tiene y puede tener cosas como las que tú dices y en el fondo son imágenes que no son apropiadas o que pueden tener vallas o tener miles de problemas. Entonces, siempre es mejor y siempre se valora el que tú logres desarrollar una arquitectura propia donde tú sepas con qué estás alimentando el modelo, precisamente con temas de explicabilidad. Hay industrias en las que no importa tanto el tema, pero en salud, por ejemplo, es fundamental y siempre se va a preferir un modelo donde tú lo entrenes con tus imágenes que están curadas, que están verificadas, a que lo entrenes con estos modelos general. Y lo último, el tema de los protocolos, que también yo lo encuentro súper interesante y, bueno, simplemente que una de las dudas que a mí me surgió cuando apareció el, porque del MSP nos han hablado bastante en el máster, así que yo creo que estamos todos más familiarizados, pero el A2H, no, eso es algo que yo me enteré por mis contactos y por temas profesionales. Y lo primero que yo dije, bueno, ¿cuál es la diferencia? O sea, ¿son equivalentes? Porque yo pensaba que eran equivalentes y no, no son equivalentes porque el MSP es mucho más bajo nivel y conecta en el fondo a una gente con las tools y te da ciertos niveles de seguridad al estar ese contacto, esa comunicación estandarizada. Pero el A2H es mucho más ambicioso porque ya son colaboraciones multiagente y ya está para flujos complejos, es más alto nivel. Y yo creo que va a ser como es posible que no lo usemos a futuro porque yo veo que de a poco todo se está agentizando porque los agentes son la nueva web. Entonces, me parece súper, súper interesante y creo que, claro, Google lo liberó porque en el fondo es como un anillo para dominarlos a todos, o sea, claramente. A ver, lo que se habla también es que puede ser que MCP, que ahora lo que tú dices, son cosas distintas. Uno es para establecer cómo los agentes se comunican entre ellos y MCP es más para un usuario, o sea, pregunta algo, ¿no? Y entonces MCP con la ayuda de un modelo averigua qué herramienta es la que puede usar, ¿no? Y luego los servicios de MCP ejecutan esas herramientas. Entonces, A2A es más para la comunicación entre agentes, más como si fuera el protocolo de red para esa comunicación entre agentes. De hecho, no tiene en principio, A2A no se habla con LLM para nada, es más como los agentes se hablan entre ellos. Pero se habla de que MCP tal vez se mueva, como esos son desarrollos muy rápidos y todo cambia muy rápido, se mueva para cubrir la misma necesidad que cubre A2A. Entonces, aunque ahora mismo no se solapan, puede ser que en el futuro MCP sustituya A2A. Eso no se sabe, o sea, es algo como que es lo que se está hablando, ¿no? O sea, yo me lo he encontrado porque estoy trabajando en la seguridad de A2A y alguien que está trabajando en MCP me ha dicho, oye, el proyecto está pensando en moverse hacia eso. O sea, que. Bueno, interesante. Pero, claro, es algo que como está tan nuevo, en realidad yo creo que no hay expertos en esta área. Estamos todos tratando de entender un poco. Hombre, los que lo desarrollan y los que lo han definido, sí, pero que eso es un grupo muy pequeño de gente, sí. Claro. Bueno, yo segundo, entonces, como saliendo un poco del tema, pero aprovechando tu expertise, ¿cómo nos recomiendas a nosotros? Pues, somos todo ahora un grupo de profesionales que salimos del tema de inteligencia artificial con distintos intereses y cómo nos podrías orientar tú bajo tu experiencia en cómo entramos en este mercado. Sí. A ver, yo creo que lo que las empresas, a ver, parto de la base de que os diría que no lo sé porque yo entro, por la parte de la seguridad, entro en la parte de inteligencia artificial, ¿no? Entonces, yo en ese sentido soy afortunado porque entro con una palanca que es la ciberseguridad. Pero lo que yo veo de lo que buscan las empresas, la necesidad que tienen. tienen, es que hay mucha necesidad de casos de uso y de personas que hacen cosas con la inteligencia artificial. No solamente conocen, que yo creo que eso se puede quedar abstracto a lo mejor por una persona, una empresa que está contratando, sino que lo que hay mucha necesidad de casos de uso. A nosotros nos piden mucho que lo usemos, que usemos inteligencia artificial, que hagamos casos de uso, que la usemos para optimizar cosas repetitivas que hagamos. Entonces, yo creo que una manera es, por ejemplo, aquí también está muy valorado, se le da, digamos, ha habido un incremento, por lo menos yo desde mi punto de vista, yo antes siempre leía artículos de blogs, sí, artículos de blogs, básicamente para entrenarme de los cambios en seguridad. Para temas de inteligencia artificial, leo muchos papers, he leído más papers en el último año, en los últimos seis meses, que antes en toda mi vida, porque la mayoría de conocimiento relacionado con IA y seguridad de la IA está en papers, más que en artículos de blog. Entonces, yo veo que aquí la investigación está más cerca de las empresas que, por ejemplo, la investigación en ciberseguridad, yo nunca la he visto tan cerca de las empresas como está la investigación en inteligencia artificial y que lo valoran, o sea, Red Hat valora, quiere, quiere que investiguemos y que publiquemos papers. Entonces, yo creo que, claro, tomar Red Hat como ejemplo, claro, no lo sé, no lo sé, porque claro, no sé cuánto se puede parecer a otras empresas, una empresa tiene un tamaño, pero aparte, incluso para ser una empresa americana, es distinta de otras empresas americanas, pero vamos, yo creo que sí que tiene sentido, es decir, que os hagáis con una cartera de cosas que podáis enseñar, es decir, de proyectos que podáis enseñar, que no se eche para atrás el tema de la investigación frente a la empresa, es decir, yo creo que es muy buena idea investigar, escribir papers o tener repositorios de GitHub como conclusión de una investigación y que luego eso sea vuestro currículum más que el currículum escrito, es decir, mira, este es mi repositorio de GitHub y he hecho estos proyectos o estos son los papers que yo he publicado o he colaborado y luego también el tema de las asociaciones, ¿vale? Que si no os lo paso ahora, porque también yo justo inmediatamente tengo una cosa, pero os lo voy a pasar para que Ernesto os lo pase, el tema de asociaciones relacionadas con seguridad e inteligencia artificial y os invito de verdad que participéis y luego también, es ambicioso, pero que no es de miedo y os invito de verdad que lo investiguéis, si tenéis la posibilidad y os llama la atención, buscar proyectos de código abierto tipo Ptorch, ¿por qué no?, otros que os gusten, Hacking Face Transformers, investiguéis cómo colaborar, cómo contribuir, es decir, ver los issues que tienen abiertos, enteraros si hacen reuniones o no, intentar hacer un pull request solucionando un issue que os salga mal y os digan, oye, es que esto ya está trabajando otra persona, ah, vale, es que no sabía que, ¿cómo me entero yo de que hay otra persona trabajando así? Ah, vale, perfecto, pues coges otro issue y hacéis un pull request, si sois programadores o queréis aprender o os gusta, es decir, código, software abierto, repositorios de GitHub, asociaciones abiertas relacionadas con inteligencia artificial, papers y proyectos, yo creo que esas cosas sirven muchísimo para luego, teniendo la oportunidad de una empresa que tenga la necesidad de busque, que os, vamos, tengáis más valor que otras personas que no hayan hecho esas cosas. Pues genial, Florencio, oye, yo si no hay más dudas ni descanso ni nada, darte las gracias, de verdad, yo creo que hemos sido oponentes de primera, pero tú realmente te diría que de lo mejorcito, eh, gracias, de verdad, suerte en la charla que hay en Amsterdam y nada, pásame la información cuando puedas, sin prisa, esta semana y vamos hablando, gracias. Pues nada, lo dicho, muchas gracias, tenéis que venir correctamente al principio, sino que sin problema Ernesto os lo pase y cualquier cosa podemos hablar. Muchas gracias a todos y gracias por la participación. Gracias, crack, hablamos pronto.