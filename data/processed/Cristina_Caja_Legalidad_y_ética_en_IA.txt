# Master Class Master Class - Cristina Caja. Legalidad y ética en IA-20250711_165558-Grabación de la reunión transcript # Transcripción generada automáticamente por OpenAI Whisper # Archivo original: Master Class - Cristina Caja. Legalidad y ética en IA-20250711_165558-Grabación de la reunión.mp4 (459.5 MB) # Procesado en 20 segmentos el 2025-07-12 12:45:00 # Segmentos transcritos exitosamente: 20/20 # Segmentos fallidos: 0 # Cobertura estimada: 100.0% Vamos a poner la presentación y vamos a compartir, ¿dónde está?, aquí, para pasar, aquí y aquí, ¿vale? Y ahora lo que necesito, espera, necesito compartir pantalla, por si no, ¿vale? Porque si no comparto pantalla, tampoco servirá de mucho, ¿no? Recupeis, que lo tengo todo controlado, que pensáis. Compartir la pantalla entera y menos lío, pantalla entera. ¿Vale? Aquí compartiendo pantalla, ¿cómo se nota el tiempo hermoso de... de barrio, eh? ¿Vale? Y... ¡Voilà! ¡Por fin! Bueno, antes que nada, quiero presentaros a Cristina, darle las gracias, ya estáis ahí a las doce de la noche más o menos, hoy se nos lleva al aeropuerto a las ocho, siete y media, para que no se asompeis, claro, porque tienen... no han venido de Canarias. Además, está en la parte jurídica, Oiza, tiene varios artículos escritos sobre interés judicial y, bueno, agradecérselo y para mí es un placer que esté aquí en este, porque además en la parte jurídica... la parte ética tiene tela, pero la parte jurídica aún más. Estábamos antes en una reunión debatiendo cómo tenemos que hacer estas auditorías, qué tenemos que considerar, si es de altos riesgos o si no, la ley europea, la RIA o IAACT, también estamos hablando del esquema nacional de seguridad... Hay varios temas que ahora mismo están todos en el aire, ¿no? Y es un placer tenerla ahí a su compañero que está ahí en el fondo apuntando, pero no os va a evaluar él, ¿vale? Date las gracias y ya todos son tus juguidos. Bueno, primero deciros que yo vengo del mundo jurídico, como os ha comentado Ernesto, y no tiene nada que ver con el mundo de la ingeniería, matemáticas y demás. Es decir, que no todo es sencillo y no todo tiene una respuesta. Entonces, bueno, pues supongo que hay muchos ámbitos de inteligencia artificial que sabéis seguramente muchísimo más que yo, pero lo que yo estoy estudiando es el tema de la normativa y qué podemos hacer para que esté regulado de la mejor manera posible. Entonces, llevo muchos años siendo jueza en Canarias y desde hace un tiempito pues empecé a indagar sobre la inteligencia artificial, a escribir artículos, a ver cómo podíamos también desde el mundo jurídico introducirlo, pero también desde el mundo de la universidad, ¿no? Entonces aquí pues he venido con un compañero de la universidad y estamos escribiendo artículos pues para ver cuáles pueden ser las bases. Yo no sé, desde el punto de vista del derecho, si conocéis la normativa y cómo se está regulando el tema de la inteligencia artificial o de eso no sabéis absolutamente nada, ¿no? Sabemos que existe la normativa. Ahora hace poco estuvimos debatiendo por el tema de la normativa europea. Sabemos que existe, yo por ejemplo preguntaba también por la implicación del gobierno porque sacó un sandbox para aplicaciones de IA y yo decía, porque ellos están invitando que si tú haces algo con IA, lo pongas en ese sandbox y ellos te van a decir si cumples o no cumples. Entonces yo decía, esto es voluntario, esto es normativo, entonces yo tengo muchas dudas. Claro, bueno pues vamos a intentar entre todos solucionar estas cosas, pero ya os digo que no es muy sencillo dar una respuesta acá y decir esto es así porque estamos en un momento histórico en el que todavía pues no hay una base cierta y estamos esperando esa ley de inteligencia artificial que todavía no hay ni proyecto, pero bueno se está pensando ya en ella y se empieza en España cuando sea. En principio lo que tenemos es lo que acabé de comentar, es un reglamento. El reglamento no se llama ley, se llama reglamento de la Unión Europea. Esa es la única regulación que tenemos en cuanto a inteligencia artificial, pero luego tenemos otra normativa que ahora voy a comentar que también nos sirve un poco de límite a todo lo que tenga que ver con la inteligencia artificial por cada de la protección de datos. Entonces eso sería un poco las bases y luego tenemos interpretaciones que se están haciendo de ese reglamento de inteligencia artificial. Entonces mi presentación consta de tres partes. La primera es un poco poneros en contexto de lo que tenemos ahora y de cómo funciona el tema jurídico, que seguro que lo sabéis, pero vamos a, os voy a comentar alguna cosilla. Después os hablaré de la parte ética y por último pues de la normativa que existe hasta ahora, a julio de 2025. Bueno pues empezamos, la inteligencia artificial va a definir decisiones, trabajos, salud, ya lo estamos viendo cómo es comparable a la revolución industrial y digital, pero vamos a ver que nos va a afectar absolutamente a todos, en todos los sectores, dependientemente de quién nos dedicamos, pues nos va a afectar el impacto de la inteligencia artificial. ¿Qué ocurre con todas, con toda la normativa? Que siempre llega tarde. Ya sabéis que primero las cosas ocurren, los problemas ocurren en la práctica y luego buscamos la solución. Entonces en base a los problemas que van surgiendo pues vamos a ver qué normativa podemos hacer para regular y esa normativa entendemos que tiene que ser con una base ética, una base moral. Por una parte vamos a ver cómo puede potenciar las capacidades humanas, pero también vamos a ver en sentido negativo cómo puede consolidar esas desigualdades y esos sesgos. Y a mí lo que más me preocupa, y no sé si a vosotros también, es el tema de la responsabilidad. Para mí esta es la clave, la que puede erosionar la autonomía si no se regula bien. ¿Por qué? Porque podemos llegar a tener la tentación de delegar nuestra responsabilidad a una inteligencia artificial. Entonces hace poco hemos dado una ponencia sobre el tema de inteligencia artificial, chatbots y salud mental. cómo cada vez se está utilizando más para preguntar qué hago con mi vida, ¿no? Entonces, ¿qué hacemos con esto? Realmente, porque en algún momento alguno dije, venga, voy a preguntarle qué pasa, ¿qué puedo hacer? ¿Qué pensáis sobre esto? ¿Alguien lo ha hecho? De los que estáis aquí, ¿alguien lo ha hecho? Yo no me gustaría preguntar así. Sí, sí, sí, sí. Bueno, yo sí como prueba, porque precisamente veía en las noticias que había muchos jóvenes que estaban ocurriendo, en vez de pagarse el psicólogo, le consultaban a HGTP Cosa y que incluso había casos de un chico, una familia, que se había suicidado. Entonces, que ahí quedaba un tema así como, ¿y esto qué fue? Eso es. El problema que podemos tener es que, claro, si nos da un mal consejo, nos puede llevar hasta el límite, ¿no? Entonces, como ya el CHGPT, si empiezas a preguntar determinadas cosas, te dice, ojo, hay un teléfono de protección del suicidio. Empieza a darte un poco de información como límite, porque nos puede llevar a un resultado que no queremos. Pero es verdad que es que ahí, ahora no hay tampoco ningún tipo de regulación y por una parte dice, bueno, cualquier momento me puedo sentar, puedo preguntar, esté fácil acceso y todos podemos caer en la tentación, ¿no? Pero cuidado con esto. Pero no solamente eso, sino, yo soy jueza. Imaginaros que le pregunto a la inteligencia artificial qué va a hacer con el caso que se me ha planteado. ¿Os gustaría que vuestro caso lo decidiera una inteligencia artificial? No. No, pero sí me gustaría utilizarlo para darme otra perspectiva de cómo estoy analizando. De hecho, es para eso la mayoría de las veces que lo utilizo, para ver qué opinión, qué perspectiva tiene con referente a un tema, con referente a un punto concreto. Así que me da igual tener otra perspectiva. Pero tú estás ahí, ¿verdad? Obviamente. Claro, me estás ahí filtrando la información. Human in the woods. Claro. Exacto. Es que te da una visión que te puede dar ideas que tú a lo mejor ni habías pensado. Entonces, para eso, hola, puede ser positivo, pero delegar y decir, venga, que lo haga la inteligencia artificial, parece un poco fuerte. Por poder se puede hacer, ¿no? O sea, por poder, ahora, en este momento, podemos hacerlo. Entonces, yo siempre pongo un ejemplo, no solamente por esto, sino por otra cosa, pero otras historias. Siempre decimos, no, es que hay una normativa detrás. Siempre decimos, bueno, pues hay una normativa que me protege. Ojo, pero que hay una normativa detrás, primero está nuestra propia responsabilidad. Entonces, hay una normativa, lo sabemos, que dice, bueno, que nos protege frente a los robos. Sabemos todos que no hay que robar en casas ajenas. Pero todos, lo primero que hacemos es cerrar la puerta cuando nos vamos. La responsabilidad primera es nuestra. No dejamos las puertas abiertas, como hay una normativa que prohíbe el robo. No. Bueno, pues lo primero que tenemos que hacer es pensar que, en el caso de la inteligencia artificial, los primeros responsables somos nosotros. Los primeros que estamos metiendo unos datos que a lo mejor no deberíamos meter somos nosotros. Entonces, no pensemos que la normativa va a ser tan extensa como para protegernos. Tenemos que nosotros, desde nuestra responsabilidad, pensar lo que queremos hacer y lo que no. Plantea preguntas éticas y filosóficas sobre la decisión. Y ahora os voy a explicar esto. Ojo con lo que decía, con lo de delegar. Este fenómeno no es nuevo. Hemos asistido en otras áreas del conocimiento a la aparición de avances científicos que desbordan esos marcos normativos existentes y generan intensos debates morales y jurídicos. Y aquí pongo un ejemplo o dos ejemplos. Por ejemplo, la fecundación in vitro. Cuando surge todo el tema de la fecundación in vitro, la fecundación del óvulo fuera del cuerpo, y todavía sigue este debate, el tema de la fecundación de una manera artificial. Aceptada el día de hoy, pero genera unos dilemas éticos. Estamos hablando de que hay una regulación. En el año 2006 se regula la fecundación in vitro. No se permite estar en el óvulo, fecundarlo fuera del óvulo, introducirlo. Bueno, crear vida en un laboratorio está bien. No estamos hablando de un tema jurídico, estamos hablando de un tema ético. Está bien. ¿Qué hacemos con los embriones sobrantes? Es que empiezan a plantearse una serie de dudas cuando surge esta tecnología y tampoco puedes decir, venga, paso, no quiero saber nada. No, es que ya puedo hacerlo, o sea, puedo clonar. Puedo elegir los ojos alquileres. Pero ¿está bien hacerlo? Igual ocurre en la inteligencia artificial. Hay límites al derecho a reproducirse. Y en el caso de la gestación subrogada, ¿sabéis el tema de las madres de alquiler? ¿Está permitido? Aquí no, allá sí. En el caso de los niños. En el otro lado del océano, sí. En muchos países está permitido. Por eso hay gente que, como aquí no está permitido, se van a otros lugares. ¿Y qué pasa con esos niños que nacen allí, de padres españoles? Que luego vienen para acá y... No se pueden registrar. Sí, había el caso de una persona de la televisión involucrada en esto. Hay casos de famosos, no famosos, pero es un problema y están continuamente teniendo resoluciones sobre esto. Va en contra del orden público. Se dice, va en contra del orden público. Entonces, no se puede registrar. Son problemas que van surgiendo porque hay avances y porque nos están surgiendo en la práctica una serie de problemas sin solución al principio. Explotación o libertad de la mujer, cosificación del cuerpo femenino, comercio de seres humanos. La técnica avanza más rápido que el derecho y se generan esas zonas grises legales y éticas. Y siempre legal va unido a ética. Se tensionan valores y ahí es el primer punto de partida. No hay regulación en España de inteligencia artificial, pero sí tenemos protección de derechos humanos. Y si hay normativa que protege los derechos. Entonces, no es que estemos totalmente desprotegidos. O sea, partimos de que si vulneran nuestros derechos, ¿qué hacemos? Derechos fundamentales. ¿A dónde vamos? ¿Qué hacemos? Hay una serie de mecanismos y hay un tribunal en concreto, que es el Tribunal Constitucional, al que podemos acudir. Entonces, si hay una vulneración como consecuencia de la IA, podemos acudir al Tribunal Constitucional. Y después, también los tribunales ordinarios, en cada una de sus secciones, civil, penal, laboral, administrativo, si hay una lesión de derechos fundamentales, hay unos procedimientos concretos, rápidos, para poder proteger esos derechos fundamentales. Entonces, partimos, inteligencia artificial, ¿vale? Resultado, no lesiona derechos fundamentales. Hay lesión de derechos fundamentales, hay mecanismos. Yo quería saber si esos mecanismos legales son mecanismos costeados por el Estado. Es decir, si tú sientes que tus derechos están vulnerados, ¿tú tienes que tener dinero para contratar un abogado que haga esto o tú vas y te asignan un abogado? ¿Cómo funciona eso? Están los abogados particulares que tú pagas. Vas a un despacho particular y pagas y luego está el abogado de oficio. El abogado de oficio es el que, si cumples una serie de requisitos, es decir, tu economía no alcanza, determinada cantidad, tu situación familiar y demás, vas al colegio de abogados, te hace un plan y te dice si tienes derecho o no a que el Estado te dé un abogado de oficio. Se llama abogado de oficio. Puedes además acudir a cualquier tipo de procedimiento, incluso tribunales constitucionales. Como digo, libertad, dignidad, justicia e igualdad. La IA pone en cuestión uno de los pilares de nuestras sociedades democráticas, la atribución de esa responsabilidad basada en la autonomía personal. Delegamos decisiones humanas a la IA, el punto de partida. La IA toma decisiones en salud, empleo, justicia, consumo… ¿Qué buenos usos creéis que se le puede dar a la inteligencia artificial? Detección de enfermedades anticipadas antes que… Patrón de análisis de información con una profundidad que por mano le tomaría siglos. Lo que he comentado antes en el tema de la fecundación in vitro, la selección de empriones, se puede seleccionar uno u otro. Bueno, pues ya la IA lo está haciendo muchísimo mejor, o lo puede hacer muchísimo mejor, que un hombre. Entonces, tiene unas ventajas muy potentes. Ahora, la pregunta que nos vamos a hacer. ¿Quién responde? Que esa nos la hemos hecho todos. ¿Quién responde cuando es una IA la que ha decidido? Debería tener un humano a cargo, porque es lo mismo que hemos hablado. Si tú usas la calculadora para hacer un cálculo y te equivocas, vas a demandar a la calculadora, obvio que no, eres tú el humano utilizando la herramienta, entonces debería ser siempre un humano responsable. Y capas de responsabilidad también. Eso es. La que tiene vos, la que tiene tu proveedor, la que tiene el ingeniero. Tenemos que ir estallendo el caso concreto y decir, a ver aquí, ¿quién se ha equivocado? Si es desde el inicio hasta el usuario. Entonces, la responsabilidad última sería nuestra, pero bueno, yo si delego y veo que es fiable y me dan una serie de garantías, alguien tiene que responder. Entonces, cuando el médico utiliza una IA para dar un diagnóstico, imaginaos que tiene el dilema, quizás yo creo que tiene esta enfermedad, pero es que la IA me está diciendo que tiene la otra. ¿Quién es vosotros que decidiríais? El es el que tiene la última palabra. Porque le estoy usando justamente, usando la analogía de Ivana, le estoy usando la calculadora para hacer el cálculo. Puedes tener tú la incertidumbre de esto o esto. Si decís que no está bien el diagnóstico de la IA y te equivocaste tú, pues te equivocaste tú. Siempre tiene que ser humano. Yo creo que hay acciones que sí se pueden hacer con la agéntica o lo que viene a futuro, pero hay cosas que no debería dejarse tan a la ligera. Justo no nos cae bien que venga alguien, una jueza. Si una jueza es alguien que está, que tiene un juicio y se puede crear y creer en el juicio de esta persona. Ahora, si a la IA se modela con la característica y el juicio de esta jueza al 100%, ¿cuál de los dos? Claro. Imaginaos, yo cuando resuelvo, lo que hago es coger una base de datos y decir, vamos a ver cómo han resuelto jueces anteriores para tener un poco la idea de cómo puedo resolver. Imaginaos si la IA tiene acceso a todas las resoluciones de todos los jueces y me dice, pues he hecho una barriga y esta es la mejor solución. Bueno, hay métricas para... Porque yo sé que se habla mucho sobre el tema de las decisiones de la IA versus los humanos, pero los humanos igual tienen una alta tasa de errores y muchas veces se considera o no se considera error dependiendo de las circunstancias porque de repente pueden resolver un caso, por ejemplo, un juicio que normalmente se decide de una forma, pero dado que se hizo un revuelo público, dado que es representativo, se cambia el criterio por esas circunstancias. Entonces yo creo que... Por la presión social, ¿no entiendes? O las políticas, las distintas circunstancias. Entonces yo creo que, por ejemplo, en el caso de la IA, uno de los puntos que yo no he escuchado mucho es que se evalúen métricas en las cuales todos nos englobemos aquí. Una máquina como un humano tiene cierta cantidad de errores probables y nosotros llegamos a un acuerdo de que si la probabilidad de error que comete es hasta un 80%, ya es posible. como bueno, si cometió el 20% son los errores, ok, ya, es parte de lo que uno podría considerar, pero hoy en día no tenemos métricas que todos aceptemos para humanos y para máquinas, entonces claro, somos súper rigurosos con el tema, si la máquina se equivoca es como ¡uh, horrible! pero a lo mejor el porcentaje de errores de los humanos es mucho más alto y lo perdonamos, ¿por qué? porque humanos, solo cuando no nos convenga, pero si somos nosotros los que estamos al otro lado, ahí ya no le perdonamos al humano, entonces... Replica un poco en tener un culpable, ¿no? al que apuntar... Ese es el tema, que si no me equivoco, respondo, pero si se equivoca la máquina, tenemos un problema. Para mí en áreas críticas siempre tiene que haber un buen cargo, sí, y tú tienes que utilizar la herramienta en un punto de libertad, ahora, sí, a ti lo más probable es que te ayude a utilizar la herramienta bien y nosotros desde el punto de vista de la IA creo que tenemos que ser los primeros de defender un uso responsable de la IA y siempre tiene que haber un humano desde mi punto de vista, pero eso también son paradigmas filosóficos que ahora están todos, y algunos piensan que no, que todo se debe automatizar sin control humano, yo creo que hay ciertas áreas, ciertos procesos pequeños que sí que se pueden automatizar pero siempre tiene que haber un humano responsable porque si no, no sé, creo que nuestra sociedad sería algo para mí muy adverso, muy frío y creo que muy injusto, creo que la parte humana es algo que la IA no puede reemplazar, la IA probablemente nos mataría a todos, si dependiera de... ¿Gregorio tiene la mano levantada? Gregorio, si quieres comentar, es que no se ve aquí, lo estoy viendo yo... Dale, perdón, sí, sí, pues con respecto a eso digamos que, pues, la cosa es que digamos que en las últimas siempre habría como un humano que estaría pues poniendo un modelo detrás, entonces en las últimas el humano pues le haría más o menos peso a algunas decisiones, igual siempre habría algún tipo de sesgo, o digamos mi comentario un poco, por ejemplo, un juez falla y tiene muchos elementos, entre otras cosas, digamos, también el estrato social de la persona o digamos el grado de poder que pueda tener dentro de la sociedad, no se juzga lo mismo, digamos, una persona que está en bajo rango o un general de un ejército o un político o incluso digamos cuando está en un hospital hay un director científico que mira algo que se llama tejido social, es decir, dos chicas que estén jóvenes, que tengan embarazo de alto riesgo, se miraría a quién se atiende, si hay una sola cama, a quién tiene más tejido social, entonces un poco digamos que siempre, no es como que el humano lo haga mejor, la inteligencia lo podría hacer mejor, es que en últimas el humano se responde por la decisión, pero siempre que habría un humano balanceando el asunto, habría algo que, como se entrenó el modelo, y eso se puede manipular. Evidentemente yo tengo mi filosofía de vida y bueno, pues lo que yo decida a lo mejor hay otra persona que no lo decide yo. Un juez de Estados Unidos no es decir lo mismo que un juez de Europa. Claro, efectivamente, o yo tengo una creencia o yo tengo, me acuerdo con el tema de cuando surgió hace ya bastantes años el matrimonio homosexual, un juez se negó porque dijo que no creía y entonces bueno, pues se empezó un debate sobre el tema de la objeción de conciencia, el juez tiene objeción de conciencia, es decir, puede acelerarse a juzgar cuando entiende que va en contra de sus propias creencias, eso es muy potente, porque el médico puede negarse a realizar un aborto, un farmacéutico, en su momento salió una sentencia, una dispensa de preservativos, le dijeron que sí tenía que dispensarlos, pero un tema de una píldora abortiva le dijeron que no tenía por qué hacerlo. Entonces hay otras profesiones que tú te pones en una lista, en este caso está bien regulado, te puedes negar, si eres médico, al tema, pues, claro, es que no creen en eso, no lo pueden obligar, todo regulado, igual que la eutanasia, te puedes también negar, pero un juez no. Quedó súper claro que el juez es un ente que no tiene conciencia, en ese sentido, que no tiene confianza, entonces tú tienes que dar siempre una solución y no te puedes negar a ello. ¿Por qué? Porque se entiende que tú lo que tienes que hacer es cumplir, no tienes que poner tu persona, no tienes que poner tus sentimientos, no tienes que pensar si tienes una hija que le ha pasado no sé qué y resulta que es lo mismo que le ha pasado la que tienes enfrente y voy a por ella, no, tienes que pensar que hay una normativa que cumplir y ya está, ya es cumplido, simplemente ha rajado tabla la ley. Entonces, bueno, pues, es lo que se intenta, es hacer lo más objetivo posible. Una pregunta, ¿qué piensas tú de qué? Porque tú nos estás preguntando qué pensamos nosotros de la IA, yo me pregunto qué pensarías tú si la sociedad quisiera tener transparencia sobre lo que la IA haría para evaluar tu decisión como jueza, es decir, para evitar corrupción, para evitar que venga un político y que a él se lo juzgue distinto y la sociedad dice, ok, tú como juez eres responsable, pero yo también quiero saber, tener de forma transparente qué haría la IA de forma tal de que tú, tu juicio puede ser distinto a la IA porque tú eres la jueza, pero tú vas a tener que justificar ante todos nosotros muy bien tu decisión, ¿qué pensarías tú si la sociedad te exigiera algo así? Claro, es que, vamos a ver, yo cuando hago una resolución a mí me obligan a argumentarlo, entonces me puedo equivocar o no, pero tiene que ser conforme a la ley, ¿vale? Siempre, y si no está argumentado, por ejemplo, pongo dos líneas, me pueden recurrir y el tribunal de arriba me dice, ojo, no has argumentado, no puedo decidir por qué quiero, tiene que ser conforme a este artículo, me puedo equivocar o no, ¿vale? Pero tiene que ser por esto y por esto, entonces, no es que piense yo, es lo que dice la normativa, que todos los ciudadanos tienen derecho a saber por qué se ha decidido determinada, por qué se ha dado determinada respuesta y en el caso de la IA igual, el derecho a la transparencia, lo dice el reglamento y lo dice también el tribunal. El reglamento de la protección de datos, que es el reglamento de la Unión Europea, que es como la base por la cual luego también se ha publicado la ley de protección de datos en España. Lo dice Clara, que todos tenemos derecho a saber de dónde procede, antes lo hablábamos, todas las decisiones que se van tomando. El problema que tenemos es que a veces no sé si vamos a saber de dónde procede. Qué difícil, entonces ojalá y nunca lleguemos al escenario que plantea Joana, porque argumentar contra la IA ya se ha mostrado que es imposible. Pues ahí es donde va a estar el problema, que podemos delegar, sí podemos delegar, pero siempre y cuando podamos demostrar de dónde procede eso. Más que nada porque si no nos pueden manipular, llegar a poder a alguien que no le interesa o que le interesa una cosa u otra y estamos absolutamente vendidos. ¿Estamos excusándonos en la máquina? ¿Erosiona nuestra dignidad? El artículo que más me gusta de la Constitución es el artículo 10, la dignidad humana, que es el que digo siempre, y son los que tienen que defender por encima de todo y ahí es la base del resto de los derechos. Y si delegamos decisiones que van más allá de las tareas mecánicas, aquí está el problema. Las tareas mecánicas nos pueden venir súper bien con inteligencia artificial, pero cuando sea más allá la IA requiere una respuesta normativa anticipada, que ya sabemos no regular implica un vacío ético, o sea no solamente regular implica no tener una normativa, sino que no tenemos esa base ética necesaria. Y el reto, como digo, no es frenarle progreso, sino asegurar que se respeten esos valores humanos, y en eso estamos. Bueno, vamos a pasar a la siguiente diapositiva. El último punto es donde más a veces se hace crítica a Europa, porque mucha gente dice que Europa solo legisla, pero no innova ni progresa. Pero somos los únicos que hemos legislado sobre esto. Era Master… Y ahora era Master… Valencia. Ahí. Bueno, la segunda parte sería la ética. Ética, inteligencia artificial, los principios rectores según la Comisión Europea. La ética aplicada a la inteligencia artificial busca garantizar su desarrollo y uso conforme a los derechos fundamentales, los valores democráticos. Vamos a ver cómo la Unión Europea establece una serie de principios rectores. Tenemos por una parte el reglamento de la Unión Europea, y luego vamos a ver cómo desde la propia Unión Europea se han dado como una serie de directrices de cómo tenemos que desarrollar esa inteligencia artificial y cómo debemos interpretarla. Lo primero, la base fundamental, como digo, es el respeto a los derechos fundamentales. Dignidad humana, privacidad, igualdad, autonomía. No debe violar las libertades individuales y no debe causar discriminación. Todo esto es lo que nos dice. Compatible con la Carta de Derechos Fundamentales de la Unión Europea. Si alguno se acuerda de haber leído la Constitución, o en el Instituto, no sé si se leía, bueno, pues la Carta de Derechos Fundamentales de la Unión Europea es como nuestra Constitución española. Cualquier Constitución, ¿no? La de los derechos fundamentales. Prohibición, por ejemplo, de sistemas de puntuación, que luego lo haremos, de puntuación social que limiten libertades. ¿Sabéis lo de la puntuación social? De los lugares del SIDI, ¿no? Que te dan privilegios si te portas bien y te los quitan si te portas mal. Bueno, pues eso en principio está prohibido. ¿Sabéis qué es el reglamento? Que lo comentaré cuando leo la normativa. Hay cuatro niveles, entonces se entiende que determinadas actuaciones que tienen que estar prohibidas sí o sí, otras que tienen que ser de alto riesgo, que hay que regularlas de una manera mucho más exhaustiva, y luego riesgo mínimo o ningún riesgo. Entonces, dependiendo de en qué situación nos encontremos, se va a exigir una mayor regulación. ¿Se concilia con lo de los conductores? Porque ellos tienen un puntaje y si no les quitan derechos del SIDI. Sí. ¿Es eso que dice Leni? Sí, lo que hacen es, en determinadas ciudades de China… No, no, acá. Cuando tienes la licencia de conducir, si vas cometiendo infracciones, te van quitando un tema de puntaje y después ya te limita la circulación por eso. Eso sí que está permitido, ¿pero por qué? Porque está dentro de la misma área, ¿vale? O sea, a mí solamente me están sancionando por algo que yo he hecho dentro del propio ámbito de la conducción. El problema en estos lugares de China es que, si me ponen una multa, ya no me dejan coger un avión. Ah, ya. Te duplican la sanción, digamos. No, que está conectado con absolutamente todo. O sea, si tú no has pagado una deuda al banco, pues ellos pueden, en la marquesina de los autobuses, poner tu cara. Entonces, eso es lo que está prohibido. El que haya una conexión absoluta de todas tus acciones sociales. Otra cosa es que dentro de eso, del ámbito de la circulación, si tú has cometido infracciones, pues te quiten puntos. Eso sí está permitido. Bueno, aquí era un esquemita de, por ejemplo, el historial de crédito para que veamos que todo lo que ven, la información personal, los comportamientos, los hábitos, las relaciones interpersonales, todo esto lo conectan y después pueden quitarme el servicio de internet, exigirme coger un transporte público, no me dan un crédito o un préstamo, contratación laboral, pueden incluso decirme que no puedo acceder al terminado trabajo y la prohibición de viajar fuera del país. Eso es muy serio. Yo le pregunté a el jefe, preguntando por la RIA, por la ley europea, y uno de los ejemplos que me puso fue, no se puede poner puntuaciones a los seres humanos, no como hacen en China. O sea, fue una respuesta del propio Chávez-Pérez. Imagínate qué condicionante. Pone que tiene esa capacidad. ¿Eso DeepSeek? ¿Seguro que no te lo habéis encontrado? Vale, pues lo primero entonces, y a que no vaya en contra de derechos fundamentales, segundo, la supervisión humana siempre. Las tareas automáticas solamente en el caso de bajo riesgo. Pero cuando hay un bajo riesgo… Siempre supervisión humana. Las decisiones automatizadas deben ser revisadas, corregidas, anuladas por personas. Garantizar que sea reversible, porque si hemos equivocado esto no está bien, volvemos para atrás. Seguridad y control democrático. Prevenir los errores automatizados, estoy refiriendo a la intervención humana. Y aquí el ejemplo de un diagnóstico médico asistido por una IA, pero siempre que haya un facultativo al lado que diga, yo asumo, en el caso de que haga caso a la IA, asumo el error. Nosotros hemos encontrado muchas negligencias médicas, mucha gente que haga errores humanos. Médicos que se han equivocado. ¿Contra quién voy? Vas contra el médico, contra el seguro del médico, contra el hospital, contra la seguridad del hospital, pero sabes con quién quieres ir. Es el tema de salud yo lo encuentro particularmente importante, porque yo creo que la IA tiene un potencial de hacer llegar la salud a todo el mundo. Los sistemas de salud están estresados en todas partes por la cantidad de recursos que hay para llegar a la población. La IA es muy útil en eso, pero tú no puedes dejarla sin supervisión, por favor. Entonces, yo lo que he estado trabajando precisamente es cómo hacer modelos explicables para salud. Y no es tan fácil y es necesario que los médicos también entiendan un poquito más, y yo creo que en todas las áreas del conocimiento va a ser lo mismo, especialmente la sensible, de que yo tengo que entender que si yo utilizo un modelo genérico, no es lo mismo que si yo hago un modelo personalizado. Y nosotras lo estamos viendo en investigación porque una cosa es que tú trabajes para predecir el cáncer con una red que está alimentada con modelos de imágenes, de miles de imágenes, que tú no sabes qué metieron ahí. Y otra cosa es que tú tengas una red que tú haces con imágenes especialmente curadas y que tú sabes de dónde viene. La explicabilidad de cómo se toman las decisiones es fundamental. Entonces, claro, el médico dice, no, es que la IA no sirve. Ojo, no es que no sirva, sirve, pero hay que hacerla bien, hay que hacerla explicable, hay que hacerla con cierta garantía. Y eso es lo que yo siento que por falta de conocimiento técnico a veces, la gente toma decisiones dicotómicas. Que sí, que no, que yo estoy a favor, que yo estoy en contra. No se trata de estar a favor o en contra, tenemos que aprender cómo trabajar con la IA de una forma que como humanos nos ayuda a prosperar y no lo contrario. Es que en este aspecto yo creo que la clave aquí es, siempre pasa igual, la tecnología, hay una explosión tecnológica, una revolución industrial en un momento, ahora hay una explosión tecnológica. ¿En qué momento apareció esta GPT? Esto lo explicamos al principio. En el 22. ¿Qué día? ¿Lo sabéis? Octubre. No, noviembre. 30 de noviembre de 2022. No hace ni 3 años. Consiguió en 2 meses 100 millones de usuarios, que estábamos comentando antes. Claro, es que ahora mismo, tú has dado como un punto que se está hablando en el último mes. Lleva más tiempo, pero realmente aparece más, el artículo está que es la IA explicable. Esto es algo que tiene que explotar porque lo necesitamos. Necesitamos explicar justo lo que dice la ley también. Bueno, no es una ley, es el reglamento, pero que luego se convertirá en algún momento en ley. Tiene que poder explicar, pero claro, si realmente detrás de la IA hay un ser humano, tampoco hace falta explicarlo. Abrúmelo, porque tampoco sé si haría falta explicarlo o no. Es que siempre que tú dejas al humano solo, se presta corrupción. Entonces, no, tiene que ser explicable. No, no, no. Digo la IA porque al final, ¿tú sabes cómo funciona una calculadora? Tú puedes comprobar el resultado de una calculadora, pero tú no sabes. Al final, el resultado de la calculadora, pero la calculadora por dentro. Si tienes buen punto. Yo creo que el tema es que, claro, completamente explicable no, porque no le puedes pedir a todo el mundo que tenga la profundidad de entenderlo, pero sí creo que un poco más explicable que hoy en día, en que unos pocos tienen todos los datos. A mí me gustaría que acabáramos el máster, los que estamos aquí y nos gusta la IA y somos tecnólogos, que lo acabáramos sabiendo que la IA es pura estadística. Al gusto no. Es decir, al final son matrices, matemáticas y es el resultado. Es decir, efectivamente. Y tú puedes, por definición, no pueden haber dos resultados iguales. Es decir, incluso preguntando lo mismo. Bueno, esa fue la primera práctica que hicimos. Preguntamos todos lo mismo a ver qué nos dicen y a cada uno le decimos una cosa. Sí que puede haber una programación por encima que haga que sea más o menos parecido. Entonces, de cara a la explicabilidad, que es un tema súper interesante y que creo que vamos a escuchar bastante en los próximos meses, y para la parte de justicia tenemos mucho que hablar de eso, para la parte legal, hay un punto donde no va a funcionar nunca. ¿Por qué? No vamos a ser capaces de explicarlo totalmente nunca. ¿Por qué? Porque al final, por sentido de la propia tecnología como está diseñada, hay una parte estadística que va por pesos que no vas a poder explicar. Creo yo. Así que, ¿por qué has tomado ciertas decisiones en los puntos básicos, pero la decisión final? Yo creo que no. Ni un punto de vista. Es un melón. Es que una cosa es explicabilidad y otra interpretabilidad. Entonces, yo creo que, claro, lo que tú dices, te encuentro la razón totalmente, pero sí que esto que, si tú sabes los datos bases de los que fue entrenado el modelo, es muy distinto a que tú puedas confiar o no. O sea, puede que tú no entiendas del todo la decisión del modelo, pero tú tienes ciertos protocolos que hacen que lo que salga de ahí, tú dices, yo confío, porque está hecho de una forma. Pero distinto es a que tú le metas de todo y digas, yo no sé si puedo confiar en esto. ¿Sabéis cuándo acabo yo? Que doy alguna charla aquí otra de temas de inteligencia artificial. Una de las últimas conclusiones. Conclusiones que pongo siempre, ¿sabéis cuál es? La IA miente. Es una de las conclusiones que cabe. ¿Pero por qué? Porque al final cualquier prueba básica que hagas, de hecho, buscando temas de la ley, me dijo un artículo concreto que no existía, que no estaba ahí. No, no, te lo relaja entero. Y además te dice, no, estoy segura y es inequívoco. Si queréis os lo pongo. ¿Qué digo? Pero vamos a ver. Al final, y esto tiene un peligro en la parte legal, porque a mí me vienen muchas veces amigos míos que tienen empresas y tal, y me viene algún CEO de otra compañía, no solo de esta que hablamos, porque él sabe, están prescindiendo de los abogados. Y no estoy exagerando. O sea, me vienen y me dice, yo ya no estoy prescindiendo, sobre todo, de la parte más de Hacienda y todo esto, el abogado especializado en esto. Están redactando, me lo decía él, un amigo mío que tiene una empresa, son 10 empleados, facturan unos 2 millones de euros al año. Que ya es una empresa, una pyme, pero bueno, ya tiene eso. Y está prescindiendo. O sea, donde antes se gastaba, tenía unos abogados con los que consultaba a Zalimán, pues ya se los ha quitado de encima y se arriesga. Y yo le digo, claro, imagínate. Que me diga que ya utiliza, porque como no tengas una base jurídica, es imposible salir adelante. Y el otro día me asustó y le dije, tú eres consciente de las decisiones que estás tomando. Yo me enteré de la existencia de echa GPT, que estaba diciendo antes, porque mis alumnos empezaban a utilizarla y no lo sabía. Entonces, si eso fue en noviembre, yo me enteré en enero, que me empiezo a recibir trabajos. Y veo que me empiezan a hablar de tribunales en otros países. Y en esto y en la detrásera, pues, me dicen, ¿qué es? ¿Qué es? ¿Y dónde han sacado? Entonces, claro, indagar al tribunal penal de no sé qué. Y yo diciendo, pues, esto no existe aquí. Y además, eso, como clones, todos iguales. Bueno, pues, ahí ya me di cuenta de, pues, esto es algo raro. Empecé a preguntar y al final, pues, resultó que habían hecho con esa GPT. Entonces, en el tema jurídico, y lo digo porque escribimos artículos con base y a veces le preguntamos a la inteligencia y demás, hay que tener muchísimo cuidado porque la ley cambia mucho y la inteligencia lo coge y no sabe si ya está denomada la ley o no. Es que es pasado. Esto es tremendo. Mira, hay una cosa, la tutela. No sé si sabéis lo que es la tutela. La tutela, ¿no? Por ejemplo, teníamos un abuelito que tenía Alzheimer y aquí había que buscarle, pues, un representante. Entonces, se le ponía un tutor. Entonces, se nombraba por el juez, un tutor. Y esta persona, como no podía decidir, se le quitaba completamente la capacidad de decidir en todo, ¿vale? No podía ir a coger el examen del banco, el tema de salud y demás. Se nombraba un tutor. Bueno, pues, esto en el año 2021 hay una ley que dice que esto no puede ser porque Europa ya nos está diciendo que la tutoría no debe existir, ¿no? ¿Por qué? Porque hay unos derechos que nunca, jamás, por muy mal que estemos, nos los pueden quitar. Se modifica la ley. Bueno, pues, hemos empezado a escribir, ¿no? Últimamente sobre estos temas y la inteligencia artificial te sigue diciendo, el tutor, el tutor. Y no hay manera de decirle, ya está derogado, o sea, esto ya no existe. Entonces, claro, si tú no tienes esa base y estás al día y sabes algo, pues, te puedes equivocar. Te puedes equivocar y mucho. En el Chrome sí le puedes decir, dándole un ejemplo. Yo lo he hecho porque cuando me da información incorrecta, yo digo, pero que no. A mí me dice, por ejemplo, esa tecnología no existe o ese modelo no existe porque está entrenado con datos hasta fines de 2023. Entonces, yo busco una base, un artículo o algo serio donde diga lo que yo digo y le digo, mira, tú estás equivocado porque esto sí existe y aquí hay información al respecto. Y le doy acceso a internet a la hora de buscar. Porque también si la tienes restringida el acceso a internet no te lo va a encontrar. Estás educando tú. Y, claro, por eso digo que tú si tienes tu propia IA y le vas metiendo información, pues, al final te va a dar respuesta. Hay otro tema que también hemos hablado para hacer un artículo y es que esto ya, por ejemplo, pornografía infantil, ¿vale? Pornografía infantil está regulado, ¿no? Está apenado por la ley. Pero, ¿qué pasa si lo que yo difundo son imágenes de niños hechos por inteligencia artificial? ¿Está apenado por la ley? No sé yo qué diría. Estamos hablando solamente de personas que están difundiendo esas imágenes creadas por inteligencia artificial. Son datos sintéticos. ¿Vas a apenar a alguien por datos sintéticos? Vale. ¿Cómo interpretes esos datos sintéticos? Pues, si son gráficos, pues, ahí es donde cae la moral. Claro, pero la moral, hay muchas cosas. A mí me deja el novio, ¿no? Como me decía una amiga, es que tenía que estar apenado por la ley. Ya. Pero no está apenado por la ley, no. Si te dejan casi del altar, sí, los gastos de boda, ¿no? Pero apenado por la ley no está. Bueno, pues, en este caso, le pregunté, digo, bueno, yo sabía la respuesta porque empecé los artículos del Código Penal. Digo, voy a preguntarle, se inventó completamente el artículo de arriba abajo y me dijo, el artículo ta, ta, ta del Código Penal dice que la porografía de Zandí, aunque sea por imágenes, perfectamente redactado. ¿Qué es esto? ¿Qué es esto? Y además le puso un bis. O sea, un artículo que no existía para nada. Bueno, pues, hay que tener cuidado con todo esto. Y en cuanto a lo que se ha dicho de la penal, sí está apenado por la ley, ¿vale? O sea, sí está previsto porque habla también de ficción. ¿Por qué? Sí, sí. No lo dices exactamente así, pero se puede interpretar. No sé si es así. Gregorio, ¿querías comentar algo? Perdón, te había visto la mano levantada. Sí, es un poco que quería complementar un poco. Quizás, de pronto hay que hacer mucho hincapié y pienso que es una de las cosas que podrían fortalecerse es entender realmente que estos modelos funcionan con base en probabilidades. Es decir, el AIA no miente, ni se equivoca, ni te va a decir todas las veces la verdad. o sea, por bueno que seas o por bueno que hagas el modelo. Eso está basado en probabilidades y tiene unas probabilidades de error. O sea, actuaría como cuando uno toma una decisión, tienes un 95% de acierto y un 5% de fallo. ¿Qué eso puede pasar? Y, por ejemplo, digamos yo que soy estadístico, por ejemplo, en mi país, no sé cómo es acá, yo no tengo una tarjeta profesional porque no sigo un código de reglas porque no me pueden a mí imponer de pronto que yo le hago a una compañía, mira, esta decisión o este producto se va a vender y yo me baso en probabilidades. Entonces, la persona que toma la decisión asume las probabilidades que tiene y con base en eso toma la decisión. Es decir, yo no puedo ir a una compañía a demandarme porque yo le dije tu producto se va a vender y no se vendió porque eso está contemplado dentro de las probabilidades. Y así funciona un poco la inteligencia artificial. Es un poco lo que debería entender cualquier profesional que esto está basado en probabilidades para saber cómo toma, digamos, el tema de la respuesta. Ahora, muy precisamente, yo le pregunto si es una ley, pues él me va a dar palabras más frecuentes. O, por ejemplo, cuando yo le diga que si hay un artículo sobre algún tema, pues él va a buscar los autores que más publican sobre el tema y va, por probabilidad, a decir que es de apellido Smith o algún anglosajón, digamos, del apellido porque muy seguramente muchos publicaron y tienen ese apellido porque se basa en probabilidades. Un poco entendiendo eso, ya puedes tomar como las decisiones informadas. También está un poco cuándo se alimenta el algoritmo. Si, digamos, la legislación cambia y la base con la que se alimentó está un poco rezagada, pues tampoco me va a dar si la ley ya está erogada. Entonces, un poco, digamos, que eso ahí aportaría. Gregorio, ¿ya has puesto el mute? Sí, lo he puesto. Os juro que no sirve. Una cosa importante que creo que, efectivamente, estamos hablando de un tema estadístico, pero también los que estamos aquí sabemos cómo funciona la ley, espero, a estas alturas. Entonces, podemos tener modelos que sepan de leyes específicas. Es decir, eso se puede hacer. Esto ya lo hablábamos el otro día también, ayer, el miércoles, un millón quinientos mil modelos publicados en HimeFace, ¿vale? Pues puede haber un modelo específico que dentro de la probabilidad esté con un fine tuning o como consideremos, o con un modelo RAG, pero que sepa y que no te va a dar exactamente las mismas palabras. Incluso yo creo que sí que te las podría dar, si me metes en este caso de una base de datos vectorial, estoy casi seguro que te las podría dar, para este tema. Entonces, ¿dónde vamos a acabar en este aspecto? Pues para modelos súper específicos, legales, donde se hable solo de ese tema de legales. Y esto es otro melón que podríamos abrir, donde al final habrá modelos específicos para cada uno de estos, no son casos de uso, sino son dominios de conocimiento, ¿no? Yo creo que ahí es donde vamos a atender. Exactamente. Yo me refiero a los modelos médicos que están personalizados, yo creo que es eso, para confiar necesita, y la legislación, más encima, cambia de país en país. Entonces, tú no podrías tener, así como en otras áreas, un modelo global imposible. No, no es así. Pero si en el entrenamiento de la IA específica, en la parte médica, por ejemplo, yo como otro entrenador de esa IA, cometo el error en las imágenes, que hace que el diagnóstico sea errado, la responsabilidad terminaría siendo únicamente mía. Tuya, claro. El entrenamiento, básicamente, ya la IA no tendría responsabilidad, sino que tendría yo el responsable por el mal entrenamiento. De todas maneras, tenemos que saber que esa es otra cosa. Nosotros estáis haciendo el máster y, bueno, nos hemos ya metido en el tema de la inteligencia, algo sabemos, pero es que hay gente que no sabe absolutamente nada y se podría hacer por cierto. Eso también es importante, educar en este sentido y decir, oye, que la inteligencia artificial es esto. Esa es otra, y a lo mejor no interesa que se sepa, pero es que es así. Entonces, tú como jueza, ya has practicado en la parte de IA, o sea, tenéis que saber también cómo funciona una IA para poder juzgar. De momento, en el tema de justicia, no preguntas, todavía no se ha hecho absolutamente nada. O sea, en ese sentido. Yo tengo compañeros que están a año y medio. Me parece que juzgar algo que no sabes cómo funciona, en plan, entiendo que juzgas el áudito o el contexto de lo que haya pasado, pero si no sabes cómo mínimo funciona la estructura de algo, la parte más complicada, tú dices que aspirar a un juez se basa en la ley, en la parte subjetiva que tenemos nosotros, pero si no sabes cómo funciona o el contexto de qué ha pasado ahí, a mí me parece un poco... Pero ahí tiran de especialistas, ahí tiran de los péritos, los especialistas tecnológicos, entiendo. Eso te iba a preguntar, ¿y cuál entonces es el perfil del périto de IA? Digámoslo así, o sea, porque en este caso... Péritro Melón. Determinar si el error es del entrenamiento, si es un tema de información de la IA, ¿cómo se hace el périto? O sea, ¿cuál sería el perfil del périto de una IA? Esa es otra. A ver... Claro, a ver... Yo he estado mucho tiempo en un juzgado penal y los delitos leves, no sé si sabéis lo que es, pero son los que están en un penado solamente con multa, ¿vale? Que no son grandes delitos, pero que son cosas que nos pueden pasar a todos, ¿vale? Le das un día de discoteca, te pasamos aquí y ya está, una pelea y ahí apareces, ¿no? Vale. Antes, claro, te presentaban en un WhatsApp o te presentaban una prueba y te podías fiar. Ahora no te puedes fiar de nada. Te presenta una imagen y te quedas así, y diciendo, ¿y esto será verdad? ¿Será mentira? Porque cualquiera puede manipular una imagen, más o menos un audio. ¿Sabéis que me ha amenazado el traer el audio? Y antes era, claro, el simple audio. ¿Ahora qué hacemos? Melón. Otro melón. Yo hablé con la policía, que está especializada en delitos informáticos, y estaban, no sabían, no sabían, pues fue el año pasado, pero estaban muy diciendo, a ver ahora, ¿cómo afrontamos esto? Entonces, yo, alguna persona, y claro, yo hablé con una persona experta en esto, y yo diría, yo tampoco sabía, o sea, me resulta complicado a mí, y cada vez esto va más. Entonces, también ahí va a haber un problema. Por eso eso es clave en este caso, que no sé si eso se va a regular en algún momento, pero que los sistemas de IA que ofrezcan imágenes pongan de alguna forma marcas de agua o metadatos. Sí, generalmente. Pero no lo hacen, porque ya no lo he visto. Porque no hay multa todavía. Claro. Entiendo que para regular o no se están basando no necesitan saber cómo funciona, sino, a ver, ante una agresión puede haber, tiene que haber dos personas o dos implicados, entonces yo tengo que, si no conozco este, yo alúo en base a este, yo alúo en base a lo que yo conozco, no necesito saber cómo funciona el cerebro de un asesino para saber qué hizo algo en contra de alguien de la sociedad y por ende debe ser penado. Creo que por ahí es donde están tirando de regular la IA y tengo entendido que, por ejemplo, el ChatGPT, las imágenes que está haciendo el ChatGPT, él está aplicando un filtro amarillo, como que no es muy perceptible, pero ya se puede, se puede, exactamente. Pero tienes que saber eso. Y el tema es que puede ser, puede ser el ChatGPT, pero tú puedes tener un modelo creado por ti y tú mismo. Lo que pasó con una, no sé si fue en Inglaterra, sé que hubo, se difundieron imágenes de una persona de la política de este país desnuda y no era... En España ha habido un caso muy grave con niñas. En Nueva Zelanda, ahí va, en Nueva Zelanda. En España ha habido un caso muy grave con niñas, en el colegio. En Estados Unidos ha habido muchos de esos casos de niñas que, bueno, los compañeros de clase empiezan a difundir imágenes de... Es una broma, es una broma, pero que puede herir mucho la sensibilidad, claro. No me ha tocado ninguno así de cerca, pero claro, ya empiezan a empezar a daros más. Pero es que es eso, de cómo no te enseñan a ti también determinados también trucos, etcétera, y partes de que no sabes absolutamente nada, pues... Porque ahí estamos con los casos de corrupción política que es últimamente de los audios. O sea, claro, ahí se podría llegar a, ahora mismo, ¿eh? A decir, no, no, Ábalos o... No, no, concretamente, ¿eh? No, no, es que eso, ya lo he dicho, yo no me reconozco. Yo no soy. Podría decir, eso está hecho con inteligencia artificial y que se exima un poco su responsabilidad. No, no, tendrías que llamar a una persona experta, claro, un perito, para catalizar ese audio. Pero si le hacen las dudas razonables... Exacto. No, no, a mí eso sí es que me ha pasado. No, no, si es que no soy yo. De la organización no soy yo. Eso sí me ha pasado. Te quedas así, y a veces es que es la... Es que eres tú. No, no puedo decirlo. Bueno, justo en base a eso, Dinamarca está proponiendo una ley que te haga... Como que tú eres dueño de tu propiedad intelectual digital y de tu persona digital. Es que ya lo tiene. Primera legislación del mundo de los neuroderechos. Bueno, bueno, entonces no habría que replicar eso. Realmente da igual que tengas... Al final el problema es, si tú eres capaz de demostrar que eres tú o no eres tú, hasta ahora era difícil hacer pasar una imagen por otra persona o una voz. Tenías que tener sistemas potentes y gastarte mucho dinero, aunque seas experto. Ahora cualquier persona con un portátil basiquillo es capaz de hacerlo. Entonces, claro, yo no me había planteado y no sé ahí cómo afectar a la velocidad de la justicia. Eso es, eso es. ¿Cómo puede retrasar? Al principio va a ser un poco... A ver qué dormimos con esto. Te digo, porque es imposible que la ley, por mucho esfuerzo que se ponga, vaya a la par con la velocidad tecnológica. Es imposible. Es el meta-universo. O sea, vamos a ver las gafas. Si se comete un delito... Ay, ya pasó. Con una niña que tuvo abuso digital. Exacto. ¿En qué plano queda eso? ¿Cómo, cómo? ¿Abuso digital? Eso no lo he leído. Una violación. ¿Una violación? Imagínate, una violación con... Ah, sí, sí. El tema es que psicológicamente... Una meta-violación. ...la experiencia es real. Y tú, pues, quita las gafas, que si no es una violación, tú haces así, te quitas las gafas y... Oye, que una niña. Ostras, tú no habías leído sobre el caso. Y encima sientes y no sé cuánto. Entonces, ¿qué hacemos? Está apenado, ¿no? Está violado, pero... Sí. Pero psicológicamente lleva sentido. Eso también es lo equivalente que has dicho de quitarte. Es lo mismo decirle a la chica. ¿Y usted sí que jodió y lloró cuando le estaban violando? No. Que se quiten las gafas. No, que quiero decir que... Que se queda paralizada. Que en ese momento, no me lo había planteado nunca. Es decir, me estáis diciendo... En digital... Claro, no te va tan fácil como decir... ¡Ostras, algo raro! Pero es un hombre grande. Es una chica de 12 años que está jugando un juego. O sea, claro, en ese momento no atinó. Está inmersa en la experiencia. De hecho, no está ni consciente que ella está fuera de eso. Porque está tan inmersa que ella lo está viviendo. Y el trauma pasó, porque no atinó. Era una niña. Tiene sentido lo que estáis comentando, pero que no me había planteado. A mí me resultaba tan fácil como... Revisar el ordenador. Pensar en qué hacer. El cambio está ahí. He dicho que el bullying digital está basado en eso. Uno también dice... Bueno, pero sales de las redes sociales y... Eso es más difícil. No, no. Pero es que va cambiando el paradigma. Porque hay ciertas cosas que uno lo siente. Como que eso es... Oye, pero ¿cuáles son tus redes sociales? Pero ya los chicos que están acá... Mi vida son las redes sociales. No, me puedo ir. Y después, cuando estén en el mundo digital, va a ser lo mismo. Pero es que toda mi vida está ahí. ¿Cómo me voy a salir? Entonces, te va absorbiendo la vida digital de una forma que es distinta a lo que uno piensa. También porque entendáis mi postura, que me ha sorprendido esto. Todos hemos jugado a las peleas. Tú te has peleado en el mundo digital. No sé, el Street Fighter. Bueno, yo soy más viejo, a lo mejor, que vosotros, ¿no? O sea, más mayor, con más experiencia. Y a la IA, con más experiencia. Tú te has pegado, has peleado. Nunca te has sentido agredido. Por eso, cuando me habéis dicho ese caso, me lo leeré. O sea, buscaré información. Cuando me habéis dicho ese caso, para mí sería como una pelea virtual. No te hace daño una pelea virtual. Te puedes rabiar porque estás perdiendo. Hay otros temas psicológicos, en otro tipo de delitos, hay que hacer... Yo lo que voy a hacer con los que vengan es, chip insertado, y no te lo puedes sacar, y no puedes desconectarte... Exacto, y ahí da un punto muy... Pero es que ese es el punto cuando tú te proyectas hacia ahí y empiezas a entender por qué la norma no puede esperar a que eso pase. La norma tiene que ser lo más general que pueda tomar esos casos, porque se demora tanto en actualizarse que si tú piensas solo en el primer caso, ¿cuántos años más van a pasar de 3, 4 avances para que llegues a algo que... No lo que decir ahí. Estamos en una crucijada en la historia de la humanidad. Nunca ha habido tantos acontecimientos disruptivos en la historia como ahora. Es imposible que la ley se adapte a... Es que mañana se le va a ocurrir algo nueva. Entonces es que es normal que la ley no dé abasto. Es que no hay manera, hay manera. Algo habrá que hacer, no sé qué, pero algo habrá que hacer. ¿Sabes qué? Yo creo que la ley... Bueno, mi opinión como ciudadana, obviamente, no soy quien para dictar nada, pero mi opinión como ciudadana es que la ley debiera enfocarse en lo humano, en proteger la dignidad, los valores, los derechos humanos, independiente de la tecnología que se utilice para vulnerarlos. Tiene que ser lo suficientemente general, como decía Jenny, porque como tú no vas a poder ir a la par del caso de uso, no vas a poder ir... Bueno, y cuando se haga a través de un gemelo digital, y cuando se haga en el mundo virtual, y cuando sea... No, no puede ser. Tiene que ser tan general la protección de la dignidad, los derechos del ser humano, que da lo mismo la rapidez de la tecnología, nosotros tengamos como ciudadanos una herramienta con la que defender. Aunque ya en la normativa es verdad que lo primero que se hace cuando surge un problema, como te dije antes de la solución, somos nosotros los que decidimos. Entonces, cuando ya hay diferentes sentencias encaminadas, resolviendo de una manera, es cuando ya se plantea el hacer una ley sobre eso. Entonces es verdad que no hay una desprotección total, sino que están los jueces para dar una solución. ¿Os acordáis lo que pasó con los animales? Que hay casos que a veces dices, madre mía, hay problemas que yo no sé... A ver, no quiero ofender aquí a nadie con lo que va a salir, yo tengo perro, ¿vale? Tengo perrito, así que... Yo estoy en un juzgado de familia decidiendo una custodia de quién va con el padre, la madre, fines de semana... Y se me plantea, ¿qué hago con el perrito? Me viene uno y me dice, ¿qué hago con el perrito? ¿El perrito para quién va? ¿Para la madre? ¿Para el padre? ¿Va con los niños? Claro, que un juez se ponga a decidir qué hacemos con un perrito. Y cuando digo perrito, digo gatito, o sea, todo tipo de mascota. Lo que sea. Lo que sea, la tarántula que tienes en tu casa y te encanta, ¿no? Un gergo, me da igual. Pero es que, claro, dices, ¿esto cómo puede ser? Bueno, pues al final hay una normativa que te dice que tú tienes que decidir, y además ya no es que haya algún juez que haya decidido, es que ahora tenemos, nos viene al juzgado y nos viene el padre, la madre, y nos dice, tengo dos hijos, dos gatos y un perro. Y tengo que decidir quién va con uno, quién va con otro y la pensión de alimentos. Para los niños y para los perros y los animales y demás. Entonces, bueno, pues claro que la sociedad va cambiando. Esto hace 10 años. Iba a pensar que yo iba a ponerme a regular el tema de los perros, ¿no? Ni como propiedad. Antes se decía que era como un mueble. Ahora se dice que es un semoviente. O sea, es una cosa así que ni es un ser humano ni es un mueble. Es un semoviente. Semoviente. Semoviente. Ese es el término que utiliza el Código Civil 233. Bueno, pues seguimos con esto. En el caso del CHIP, hasta donde renuncio yo a algún tema personal por la implantación de un CHIP. O sea, no sé, porque no conocemos exactamente cuál sería el contrato en el cual yo digo, bueno, listo, yo acepto ese CHIP, pero no sé qué me limito dentro de lo que yo a futuro pueda reclamar en la aceptación de la implementación o implantación de un CHIP. Bueno, pierdo el derecho a una cosa por implantar un CHIP. Y eso no hace que pueda demandar, por ejemplo, a la empresa que está fabricando el CHIP. No sé qué va a dar. Es porque no conocemos las características del contrato al momento de aceptar una implantación de ese CHIP. Tiene que estar muy bien regulado porque es una institución importante y potente. Yo no creo que se obligue a eso. Yo creo que va a ser voluntario. Si tú lo piensas, está implicando en personas que tienen discapacidad y que una implantación a este nivel puede hacer que tú recupere sensaciones, movilidad. O sea, yo creo que la gente que lo tenga va a ser porque quiere tenerlo, porque le trae una mejor calidad de vida. No creo que venga... Bueno, ya sé, ¿verdad? Si no nos liamos con esto de los CHIPS, aquí hay temas más interesantes. Esto es muy interesante, pero seguimos. Seguimos. La tercera base, transparencia y explicabilidad, que ya lo hemos dicho de todas maneras antes. El usuario debe entender cómo y por qué decide la IA. Eso es lo que dice la Unión Europea. Documentación, lenguaje claro. Esto también es muy importante. Y vías de revisión. El lenguaje claro. Si alguno ha tenido algún contacto con la justicia en algún momento por lo que sea, claro, no somos. Entonces, importante. El lenguaje claro es incompatible con sistemas opacos o caja negra. Y después, el ejemplo del chat bancario que explica por qué no me concedo un préstamo. Tengo que saber por qué. ¿Se puede utilizar? Sí. Pero tengo que saber cuál es el origen. Cuarto, justicia y no discriminación. Levitar los sesgos. Dos, entrenamiento para no provocar discriminación. Auditar. Importante hacer revisiones continuas del impacto sobre todo social y ético y una evaluación. Y la selección del, por ejemplo, selección de personal. Sistema de contratación que filtra sin sesgo por género o raza. Vale. Quinto, responsabilidad y rendición de cuentas. Posibilidad de reclamar y corregir. También importante. Tiene que, por una parte, regularse. Y por otra, saber a dónde acudir si no estoy de acuerdo. Con todo, ¿no? Me da un resultado el juez, ¿no? Una sentencia. Y puedo poner un recurso. Pues igual, en todo lo que se haya utilizado la inteligencia artificial, me tienen que informar de qué canal. para que yo pueda pues luego reclamar. Identificación clara de los responsables, este es el otro punto, eso lo dice la Unión Europea, que toda la regulación de la inteligencia artificial tiene que basarse en estos puntos que estoy diciendo y uno de ellos es eso, saber si es el proveedor, es el que desarrolla, los usuarios o las administraciones públicas. Marco normativo adaptado al entorno digital, responsabilidad civil y penal, también saber a qué nos atenemos, cuáles son las normas, no es lo mismo civil y penal, no es lo mismo cometer un delito que te puede llevar a la cárcel que cometer cualquier acto, que sea pagar una cantidad económica. Y plataforma, por ejemplo, un ejemplo que permite apelar a una decisión automatizada que me ha excluido de una beca, que esto puede ser, no me da una beca y tengo que saber por qué. Sexto, seguridad y robustez técnica y aquí pongo los tres puntos, por una parte tiene que haber normativa de prevención, sistemas seguros, predecibles, resistentes, durante todo el proceso siempre control, protocolos de control de calidad, revisión continua y por último las consecuencias, reparar cómo me he equivocado, la inteligencia artificial no ha dado el resultado adecuado, proteger frente a los fallos y ataques y pongo aquí tener un plan B, saber qué hacer. Por ejemplo, un algoritmo en coche autónomo que se detiene ante una anomalía, que esto también sería posible. Bienestar social y sostenibilidad y al servicio del bien común, la equidad respetuosa con el planeta y no solo eficiencia económica sino también impacto humano, que es lo que tú comentabas, la inteligencia artificial al servicio del ser humano pero no pensando en un tema económico sino sobre todo el tema de la protección de los derechos. Y ha usado, por ejemplo, para optimizar el consumo energético en edificios públicos reduciendo la huella ecológica. Ponemos el siguiente, que sería ya la tercera parte, que es el reglamento, entonces voy a explicar qué normativa tenemos hasta ahora, vamos a ver cómo el reglamento de la Unión Europea todavía hay partes, porque está entrando en vigor poco a poco y hasta el año 2026 no entrará en vivo en su totalidad. Eso quiere decir que todavía, aunque existe, se recomienda su uso pero no hay una consecuencia jurídica, que sería el reglamento, sería este. Vale, entonces este sería el primero. Gracias. A día de hoy, qué es lo que tenemos. Y después las otras PowerPoint hablan de las acciones que están prohibidas, las que son de alto riesgo y las que son de riesgo mínimo. A nivel internacional, qué es lo que tenemos. Para que veáis un poco el panorama, normativa internacional está la UNESCO, Organización de Naciones Unidas, un instrumento internacional que se ha adoptado por 193 estados, miembros, que proporcionan un marco ético global, un marco ético. No hay una consecuencia de incumplimiento, simplemente pues hay un compromiso de tener esa normativa ética en relación a la inteligencia artificial. Derechos humanos como eje central, la IA debe respetar y promover los derechos fundamentales, inclusión y diversidad, evitar los sesgos, garantizar la representatividad cultural y de género y la sostenibilidad, que vaya pues contribuyendo a los objetivos de desarrollo sostenible. Y por último, la prohibición de los usos noctivos. Estas son recomendaciones, como digo, a nivel internacional. Luego, la OCDE, conjunto de recomendaciones no vinculantes, adoptadas por los países miembros de esta organización para la cooperación y desarrollo económico, que busca la IA fiable, transparente y responsable. Entonces, tenemos por una parte la normativa internacional, ahora veremos la de la Unión Europea y, por último, la nacional. Esto es voluntario, ¿no? Esto, voluntario. Pero es que hasta el reglamento, en principio, son recomendaciones. Y acentrada, ahora te digo, lo que sí trae consecuencias, ahora vamos a ver qué normativa sí trae consecuencias en el caso de que no se cumpla. Inteligencia artificial centrada en el ser humano, transparencia y explicabilidad. No voy a detenerme porque es repetir otra vez lo mismo. Estamos viendo que siempre todas las normativas, esto es lo que se pretende, transparencia, solidez y seguridad, responsabilidad, equidad e inclusión. Y por último, el Consejo de Europa, he puesto no confundir con el Consejo Europeo. El Consejo de Europa no tiene nada que ver con la Unión Europea. Es una institución distinta. Y hay un convenio sobre derechos humanos e inteligencia artificial. Como veis, sí que existe, ¿no? Desde hace tiempo ya se está hablando y existe una serie de principios que debe regir, aunque digo que no hay consecuencias y a veces si no hay consecuencias no cumplimos, ¿no? A veces, como siempre. Es que, claro, apelas a la responsabilidad, pero luego, pues, si no hay una consecuencia... Actualmente, en proceso de adopción, se trata del primer tratado internacional. Es un convenio que está en ello y, como digo, el objetivo es asegurar que el desarrollo y el uso de la inteligencia artificial respete el convenio europeo de derechos humanos. Vamos a ver cómo se está negociando para hacer algo vinculante con inteligencia artificial. Bueno, aquí pongo las diferencias entre el convenio y el RIA, el reglamento de inteligencia artificial. Y, bueno, pues, vamos a ver cómo está hecho con el CGPT. Pues, hay algunos errores que no lo había visto. Tratado internacional vinculante. Aquí está el reglamento de aplicación directa. Vale, importante. Se llama, por eso quiero que quede claro, Unión Europea. Reglamento. Reglamento significa que directamente se aplica en todos los países. Es directamente aplicable ese reglamento. Otra cosa es que se adapte en España a esa ley que vamos a tener. Ha abierto a los países miembros y no miembros y aquí solamente aplicable a la Unión Europea. Derechos humanos, democracia y Estado de Derecho, esto es lo que quiere el convenio. Habla mucho más genérico, como veis, habla de derechos humanos. En cambio, el reglamento lo que hace es que habla de los riesgos concretos que vamos a hablar ahora en relación a la inteligencia artificial. El objetivo es garantizar que la inteligencia respete los derechos fundamentales y aquí regular el desarrollo y uso seguro de la inteligencia artificial. Aquí más centrado en lo público, mucho más genérico y aquí en los operadores, los proveedores y demás. Requiere ratificación y aquí directamente aplicable que sería lo que digo el reglamento. Y este que está en fase de firma y ratificación y este que fue aprobado el 13 de marzo de 2024 y que, vamos a ver, que creo que es la siguiente. Bueno, ahora lo veremos qué es lo que ha entrado en vigor y qué es lo que no. Vale, dentro de la Unión Europea, el reglamento de inteligencia artificial, el llamado RIA, reglamento del Parlamento y del Consejo del 13 de junio de 2024, regula el desarrollo y el uso de la IA. Garantizar segura, ética, transparente y, sobre todo, compatible con los derechos fundamentales. Y lo que hace es dividir el riesgo inaceptable, los prohibidos, el alto riesgo, el limitado y el riesgo mínimo que ahora vemos. La diferencia entre un reglamento y una ley. No existe el concepto de ley como instrumento dentro de la Unión Europea. Se llama reglamento, reglas o reglamentos. Y entonces, si eso, en España tendríamos que crear la ley en base a ese reglamento. Sería lo siguiente que estamos haciendo. Entonces, el reglamento, por eso es tan genérico, porque luego tiene que adaptarse. Es igual que el reglamento de protección de datos, reglamento general y luego lo que hace España es la ley europea. Exacto. Y, claro, después tenemos la segunda parte. ¿Quién va a sancionar en caso de incumplimiento? ¿Quién va a sancionar? ¿Quién va a poner sanciones a parte de los jueces? Bien, en el caso del reglamento de protección de datos hay una ley española y tenemos un organismo concreto, que es la agencia que la tenemos, que es la que va a imponer sanciones. Ahora pongo algún ejemplo de eso. 1 de agosto del 2024 entra en vigor, pero vamos a ver cómo en febrero no se hace nada, es cuando existe esta prohibición efectiva de IA de riesgo inaceptable, del primer nivel. En mayo es cuando se empiezan a dar unas directrices de cómo debemos cumplir la IA, que todavía algunas están retrasadas, todavía no se ha completado. En agosto de 2025 es cuando empezamos ya con las obligaciones y sanciones y el año que viene la aplicación de los sistemas de alto riesgo, que ahora veremos, en agosto de 2026. Y en el 2027 la plena aplicación de las técnicas de sistemas de alto riesgo. ¿Usted sabe, en términos generales, si algún proyecto se vio que tuvo que echarse atrás frente al inicio de la prohibición efectiva? ¿Si hubiera utilizado un alto riesgo inaceptable? Claro, y que se haya apagado ese proyecto porque ya no hay ningún proyecto, pero… No, aquí tengo un debate, yo tenía un debate con Florencio. Sabéis que somos un poco frikis y hablamos de estos temas en lugar de cuando nos tomamos una cerveza. Que, claro, yo le decía, porque yo se ha entendido en el reglamento, que este reglamento es aplicable tanto fuera como dentro de la Unión Europea, porque si afecta a ciudadanos europeos. Y Florencio me decía que no, que no tenía mucho sentido, que fuera aplicable, porque él pensaba en Estados Unidos, ¿cómo ibas a perseguir si esto se hacía en Estados Unidos? Yo no sé cómo perseguirlo. Lo que le decía es… Exigir que los datos no se muevan de acá, como en la ley de protección… Pero no solo los datos, sino que no… Por ejemplo, se utiliza un PIB genético desde Estados Unidos, como Estados Unidos, porque el caso podría ser China perfectamente, para puntuar a los ciudadanos europeos. En España, imagínate que tú tienes el reloj este por tema de tu salud y ellos te puntúan en Estados Unidos. Entonces, tú vas a hacer algo allí y te dicen, tú aquí no puedes entrar, porque yo sé que tú tienes VIH, ¿no estoy mentando? O pueden hacer juntar a los proveedores, por eso la amenaza de Trump y todo eso. Ya, pero entonces este reglamento sería aplicable en Estados Unidos si nos afecta a los europeos. Yo entiendo que sí. Ahora, de todas maneras, el reglamento deja muy claro a quién se aplica. Entiendo que tiene que ser… Es un tema que tenemos internamente y yo creo que sí. Pero es que, en realidad, la normativa de la Unión Europea se suele aplicar más allá del ámbito de la Unión Europea. Claro. Y no afecta a los ciudadanos. Exacto. Yo vi una entrevista, una chica que era de la Unión Europea, con un chico de IA, y es por el tema de que los proveedores, las grandes empresas, dicen, ah, quieren limitar, yo me retiro del mercado europeo. Es como en el poker, cuando tú dices, y es como tratar de forzar que las sanciones se tiren atrás. Y tú te retiras seis meses. Ellos no van a dejar el mercado europeo, claramente. Pero es como se hace en Estados Unidos las cosas. Entonces, ellos tienen que anunciar que se van, los productos los lanzan en todo el resto del mundo, acá no, para presionar por la legislación. Pero esta chica decía, lo que hay acá en Europa, pena, casos de uso nocivos para los ciudadanos europeos. Nosotros no estamos prohibiendo que las empresas lanzen sus productos. Entonces, cuando un gran proveedor, un Google, dice, me retiro del mercado europeo, porque la ley no me deja operar, eso es falso. Nosotros solamente sancionamos cuando vemos que un producto, caso de uso de ese producto, atenta contra los derechos fundamentales de los ciudadanos. ¿Por qué? Porque en el área de la IA está ese discurso, y ella dice… que era una mala publicidad, de que Europa solo regula y no hace nada y que en el fondo perdemos competitividad en la IA por el hecho de solamente poner trabas a las empresas y al desarrollo tecnológico. Y esta chica decía, no es así, porque no estamos limitando la IA en este momento, no hay nada en Europa que limite el desarrollo tecnológico, no se limita el que las empresas innoven y bla, bla, bla. Entonces, quería saber tu opinión. Eso es un debate también, un tema de límites con innovación, claro, cuantos más límites pues al final limitan el tema de la innovación. Se me ocurrió otra cosa cuando estabas hablando y es de cómo va a evolucionar de manera distinta en la UE y fuera el tema de la inteligencia artificial, si hay una regulación concreta aquí y allí no. Estamos empezando, pero no sabemos dentro de unos años cómo va a desembocar y que mucha gente se va a querer ir fuera. Lo que tú decías respecto a cómo seguirlo, yo creo que lo único va, al menos en un primer inicio, que es lo mismo que ha pasado con el tema de los datos. Yo lo vi por el tema de que estaba trabajando con academias que hacen online curso y todo. Y tenían este tema del procesamiento de los datos y decían, la academia no sigue la norma europea. Entonces, si tú eres europeo no entres porque no se cumple esa normativa. Pero te dejan el tema de que tú como ciudadano deberías no entrar, pero ya tú entras y te sientes vulnerado. Yo me imagino que ahí es donde yo, europeo, pongo en mis tribunales y ahí se verá si está en los reglas, si esto podría crecer y ver si con ese país donde esa empresa está posicionada tienen algún convenio que puedan hacer llegar esa norma. Pero si no, eso queda en papel muerto. De todas maneras, por ser realista, hagamos lo que hagamos a nivel de poner reglamentos o normativas. Por desgracia, la Unión Europea está tan lejos de lo que están haciendo los estadounidenses a nivel de inteligencia artificial que vamos a seguir dependiendo de ellos, pero igual que dependemos de ellos por las armas y toda la tecnología. Estamos muy lejos, por desgracia. Los únicos que están un poquito más avanzados son los franceses con Mistral, que es la única empresa privada muy subvencionada por el Estado francés que está poniendo algo, intentando llevar algo a nivel. Pero bueno, yo sí que no sé si vosotros trasteáis un poquito con Mistral, deberíais, pero es que está a otro nivel. No podemos comparar. Entonces yo creo que estamos muy lejos. Podemos al menos ser los mejores protegiendo. Y mira que debate esto en las redes de en Europa solo protegemos a la gente. Solo nos dedicamos a hacer leyes y no a la parte de investigar. Pero como hemos llegado tan tarde, al menos mi punto de vista es completamente que protejamos bien a nuestra gente. Y a ver. Por eso, utilizar instrumentos, de nuevo, como existe una protección, podemos reclamar en el caso de que no de la talla. Estaba ocurriendo el otro día que tuve un juicio de una persona que compró para una empresa una máquina de hacer sandwich. Bueno, pues no estaba homologado con el sello de la Unión Europea, lo había pedido de China y, claro, yo tuve reclamo. Entonces, claro, decía, no, no, es que no funcionaba bien. Claro, se puede reclamar a la persona que te lo ha vendido, pero al fabricante, olvídate. Entonces, claro, que también en eso tenemos que tener cuidado porque tenemos que tener una serie de garantías. Lo que tú dijiste me trajo un tema que es muy legal e importante. Hace poco estaba yo trabajando con modelos y apareció este tema de que tal empresa, Facebook, decidió retirar sus modelos de la Unión Europea por la legislación. Entonces, los modelos no estaban oficialmente disponibles y yo estaba ahí en Facebook, podía descargarlo, pero no usarlo. El tema es que yo quería trabajar con ese modelo porque si no me quedaba atrás era lo que yo quería hacer. Entonces, al final había ahí me empecé a informar y me encontré con algo que yo encuentro interesante legalmente. Este reglamento y todo esto va contra los proveedores, pero no contra los ciudadanos, al menos como está ahora. Entonces, yo como persona, bueno, yo todavía no soy ciudadana acá, pero como residente acá, a mí no me puede nadie sancionar si yo con una VPN o con esto voy bajo el modelo en otro lado y lo uso, pese a que ese modelo no está autorizado en la Unión Europea. Entonces, yo averigüe porque dije lo que quiero hacer, lo puedo hacer o no. Yo tengo muchas de esas dudas, cómo se me ocurren estas cosas. Entonces, averigüe y no, nadie me puede sancionar por bajar el modelo. Entonces, claro, tú con una VPN te conectas a Estados Unidos, bajas el modelo porque es lo más avanzado y tú lo quieres usar, lo quieres trastear. Y yo decía cómo me voy a quedar atrás de esto de no poder usarlo. Entonces, yo lo usé, lo utilicé y todo el tema y claro, no hay ninguna prohibición al respecto. Pero ese es un tema interesante de que en algún momento nos van a prohibir, porque en ese momento yo me voy. Si me prohíben usar la tecnología más avanzada, yo me quedo atrás como profesional. Entonces, yo ahí me iría. Claro. Como estudiantes, por ejemplo, utilizar para hacer trabajo, ser más inteligente. Mucha gente es… ¿Lo veis bien, mal, regular? ¿Previrlo, no previrlo? No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, yo encuentro que se debe hacer lo que sí se debe guiar, se debe enseñar. El tema ético nos va a perseguir con cualquier cosa que hagamos. No solamente chatGBT, sino hacemos un esfuerzo para que nosotros, como sociedad, éticamente parece que fuera lo más loco, pero es que nos va a perseguir esto con cualquier cosa que nosotros creemos y que vamos a seguir creando. Por algo es un problema que viene desde los filósofos mucho más atrás, hasta los días de entonces, pero que frente a cada avance, digamos, no, no, que la solución está en no usarla, claramente nos vamos a ir quedando atrás. Y bueno, aquí lo hemos comentado mucho, para mí el tema no se puede negar la tecnología, porque tanto nosotros como nuestros hijos, bueno, los que los tengan, van a ir quedando atrás frente a un mundo que avanza hacia allá. Entonces el tema está en cómo capacitamos, cómo creamos nuevas formas de ver el valor, por ejemplo, de ser profesional, de ser pensamiento crítico. de valorar además nosotros como sociedad las cosas que sean como humanos, o sea, de alguna manera ir protegiendo también la habilidad humana o dándole alguna forma de incentivo, no castigo a no usar la tecnología, sino que incentivo a que oye, esto no existe... Cuidado con esta, mira, por ejemplo, yo he tenido que firmar ahora, yo llevo dos trabajos final de máster y he tenido que firmar porque he revisado que uno de ellos, que es el que va a presentar, esto estoy mirando porque está aquí, va a presentar ahora la semana que viene, uno de ellos no tiene más de un 30% de trabajo, digamos, no me acuerdo específicamente, pero el rollo es que no ha copiado ni el 30% y tengo que firmar que yo lo he revisado. Ya conocéis mi opinión sobre el tema, como si está hecho al 100%, es decir, al final ojalá lo pudiera hacer al 100% porque para mí lo importante no es cómo redactes, lo importante es que tú has revisado esa redacción y que yo cuando vaya a leerla diga, bueno, está correcto, esa es una. Y la otra es, ahora coge más peso, cómo explicas las cosas, que es de lo que hemos hablado, en fin. El día de la... realmente el TFM, se va a caer el TFM, si viene una persona, le hacemos cuatro preguntas, es que este no sabe el código, cómo lo ha hecho. ¿Y dónde está desalineado? ¿Eh? Que me dices tú, ten cuidado, ¿dónde se desalinea mi punto de vista con lo que tú me dices? No, no, que al final la clave ahí es que, lo que yo entendía de lo que estabas diciendo, es que tienes que apoyar, pero siempre tienes que ser tú el responsable de lo que acabas escribiendo. No sé, si vas por ahí, a lo mejor te he cortado antes. Sí, no, es todo complicado. O sea, me refiero yo de que no está en la herramienta claramente, está en el valor que nosotros le damos a lo que sale de ahí, tanto como en la parte educativa, como en la profesional. Y si tú valoras que profesionalmente es lo que escribió ahí, el problema es como lo evaluaste tú, pero en la responsabilidad de la persona que creó eso, es distinto. Si tú tienes que evaluar, o sea, si tú das un trabajo, que si yo hago un prompt fácil y me sale la tarea, o sea... Hay un caso legal que a mí me hizo mucha gracia, que no sé si lo conocéis, o por el dato, porque estábais fuera, en este caso, Ana Rosa Quintana escribió un libro. Bueno, escribió, lo pongo así, escribió. Y resulta que se ve que se lo habían escrito y era una copia de algo. No me acuerdo muy bien, pero a mí me llamó la atención que dijo, no, no, es que me lo han escrito y era un error porque copié una parte o algo así, comentó. Legalmente, en ese caso, era muy claro que había copiado por lo que fuera. Pero esto va a pasar mucho. Había encargado a alguien que se lo hiciera, creo. Sí, algo así. Y esa persona no, pero esa persona se había copiado de otro. Ella ni siquiera te lo había leído. Ella es la víctima. Dijo, ok, publica. Y ahora, los que os ayudáis con IA, mentira, lo de tu explicación, los que nos ayudamos, porque yo me ayudo con IA, solo faltaba. Claro, tienes que ser consciente de alguna forma, saber, y eso es muy difícil, si está copiando información de otro lado. Esa es la complejidad. Y esa es la clave que realmente sería, digamos, yo creo que aquí sí que sería, no sé si jurídicamente o no, es dentro de la universidad complicado el decir, oye, es que esto, vale, lo has hecho con IA, pero es que la IA lo que ha hecho ha sido copiárselo del trabajo final de máster de Juan Carlos, que está aquí al lado. Y ellos dicen, ostras, eso sería cagada. Y ese es el punto. Lo que se debe buscar no es que se limite, siento yo, el que se use una herramienta. Para mí no tiene sentido. Lo que se debe buscar es que tu aporte sea original. Lo que se debe buscar es ese punto en que tú o lo expliques presencialmente, los fundamentos, como lo que tú decías, yo todo lo que hago tiene que estar fundamentado. Eso debiera ser para cualquier profesional. Si se ha copiado. Podrías copiarlo al 100%, porque si no estarías plagiando el trabajo de otro y eso sería verificable. Ese es el tema. Entonces tienes que pedir cosas más difíciles y más originales. Si tú haces trabajo fácil y que tú con un problema lo sacas, para mí, sinceramente, el fallo está en cómo estás enseñando o qué estás enseñando. Igual me he explicado mal. Yo creo que lo que para mí es complicado, y ahora estoy reflexionando sobre lo que os había dicho, y me tengo que mirar. Cuidado. Claro, yo que se haga trabajo con la IA lo veo bien. Y tú supervises y dices, vale, yo lo que he puesto ahí está bien. Lo que no me había planteado hasta ahora, y para mí es nuevo de hace tres minutos, es claro. Pero si la IA te ha proporcionado información copiada de tu trabajo, eso es lo que yo no he validado y sí que tendría que haber validado. ¿Me estoy explicando? Es casi imposible hacerlo. Hoy en día es un problema. Hay investigaciones, Pepe, que han denunciado. Yo vi un caso que denunciaba plagio de otros investigadores. Y esos investigadores, el tema es que ellos se lo preguntaron a la IA. Es decir, no era su intención plagiar su investigación, pero como se apoyaron en IA, terminaron plagiando, sin saberlo, por creer en la IA. Estoy hablando justo de ese caso. El tema es que la IA plagió, y ahora nosotros estamos plagiando el plagio. Claro, porque es que, entonces, ¿ladrón que roba a ladrón? El tema es, de nuevo, lo que dijo Yeni. Joana, perdón. La IA es la nueva calculadora. Así que es una nueva calculadora muy potente. Entonces, a evaluarlo. Y yo estoy de acuerdo contigo. Es una herramienta que tú puedes usar. Te puedes apoyar en ella. Ahora, explícamelo y dime. Yo creo que hoy en día debe tener más peso en, aprendiste del proyecto, entendiste del proyecto. Ok, ahora, ¿tú sabes de dónde salió ese proyecto? Porque, claro, si te apoyas en la IA... Hay una ley y una normativa, no es ley porque es Internacional de la Universidad, donde tú no puedes copiar. Que eso es distinto. Es decir, el problema es que la IA te ofrezca un trabajo que sea copiado de otro proyecto. un compañero. Ahí es donde entra el juez y evalúa y dice mira pero esto es lo mismo que esto. Pero también yo como responsable de ese proyecto, estoy hablando de mi caso, yo como responsable tengo que afirmar que he revisado que ahí no se han copiado. Ya no es lo que te da sino que tienes que tener la base de lo que estás haciendo. Me refiero yo no le digo a la IA, vale, el TCM porque estamos ahora en ese contexto. He hecho esto, hazme la memoria. Sino que tú pues con una base de conocimiento le pones un PROM lo que sea pero claro con un contexto de que el contenido que ya tú tienes y que tú le estás dando a ella y que ella te está pues alargando, mejorando la escritura. Pero claro, tú ya le estás dando la información que tú quieres, no estás... Una cosa es consultar y otra cosa es decirle hacedme todo el TCM. Y ya en base a eso pues... Estamos hablando, por ejemplo, yo os digo temas internos. No puedes tener, digamos, copiado más del 30% de... 30 es el... En este caso es el trabajo final de máster. En general. En general, son 30%. 30% también el turnitin. ¿Eh? El turnitin. Sí. Vale. Pues ahí tuvimos un problema nosotros, bueno, este año y el año pasado. Claro, pues cuéntanos, cuéntanos. A ver, la alumna me dijo, he copiado, bueno, la autorizada mía me dijo, he copiado todo del trabajo del fin de grado. Y bueno, pues ahora cuando lo pasamos por la plataforma, vamos a ver qué pasa. Dijo 5%. 5%. Claro, problema. El año anterior no había todavía, o dos hace dos años, no había ese desarrollo de inteligencia artificial y un alumno, le pasamos el turnitin y dio un 5%. Con tan mala suerte de que uno del tribunal, pues, conocía el libro del que había copiado que no estaba detectado por el turnitin y dijo, lo has copiado aquí. Y nos sacó, dijo, es que está clavado. ¿Qué hacemos? Porque es que, claro, la normativa dice que si no supera el 5% o el 30 o lo que sea, tenemos que dar por valido, ¿no? Entonces, creo que hay un debate de, ¿hacemos caso al turnitin o realmente le hemos pillado, ha copiado y entonces no le podemos dejar pasar el trabajo? Si es plagio de un libro, un cuento que está claro, ¿no? Porque lo difícil de la inteligencia artificial es que te puede plagiar de distintas obras un poquito. Y ahí ya es muy difícil decir, esto lo sacaste de acá y de acá. Pero si tú dices que encontraron que todo lo había sacado de un libro, pues, me está claro que es plagio. No es inteligencia artificial, fue de un libro, sí. El tema es que la normativa lo que dice es que si no supera, o sea, que si no tiene ese tanto porciento, el porcentaje ese, pues, claro, lo damos por válido. Y yo, claro, diciendo, es que tenemos un problema. Porque si estamos utilizando una herramienta y le estamos diciendo que no tiene que superar y lo ha, por lo que sea, le ha dado. ¿Se ha quedado la herramienta de alguna forma? Claro, no sabemos lo que ha pasado. Entonces, ahí se creó la historia. Y este año con la alumna igual, o sea, al final tuvimos que darle por válido. Y luego, claro, resultó un poco injusto porque luego tuve otro superador, que es antiinteligencia artificial. Y, bueno, pues, le presentó el trabajo manuscrito. Yo estaba diciendo, esto no me lo creo. Tienes que presentármelo. No, no, lo copió. Lo hizo mal. No, lo dirigimos. Sí, sí, fue tremendo. Entonces, le decías que yo me lo sé. Le digo, ya, bueno, pero te lo sabes. Pero es que tienes que ponerlo o no en condiciones. Lo subió a la plataforma y el día de la exposición lo utilizó a electrónicos, o sea, con máquinas, con escuelas y ahí lo dijo, ¿no? Digo, o sea, ¿cómo nos nota que la otra? Porque no lucía, o sea, absolutamente nada. Entonces, claro, dices tú, ¡guau! Y luego, respecto a lo de... Yo cuando me di cuenta que estaban utilizando inteligencia artificial, yo lo que hago, porque claro, los trabajos no son trabajos de estos de... Trabajo de fin de grado, sino... Háblame de la usucapión. Pero te lo hace en un minuto, ¿no? La inteligencia artificial. Entonces, desde hace dos años les obligo a hacer el trabajo. Sé que no lo han hecho ellos, pero tienen que en su casa grabarse y, como si fueran presentadores, exponérmelo, pero bien expuesto, para que yo, por lo menos, diga, vale, pues porque no se los han grabado. Esa es la clave. No, no, ni yo. Depende, porque ahorita eso lo hacen ellos también. O sea, depende. Estaba pensando, no sé si son ellos o no. Sí, sí, sí, ya hay casos. Sí, ya hay. No, pues... Sí, ahora la tecnología está, pero hay todavía una distancia de costo, en que no todo el mundo se puede pagar su dinero digital para hacer... Pero dentro de nada. Hace año, o uno, yo diría que ya está al alcance de todo el mundo. Y a Bram, mirando a Pete desde el hospital, que le pidió 800.000 euros a una mujer y la enamoró desde el hospital. Y está súper mal hecho, ¿eh? Muy, muy mal hecho. ¿Hice alguna conversación? No, y ahora es como ese que hice. ¿Ella hizo? Yo tengo mi género digital. Ah, ¿tú sí lo tienes? Y parece que soy yo, de verdad. Es que ella hizo una presentación, al principio no hacen cinco minutos. Sí, pero yo la hice ahí con un personaje de esta tecnología, que fue en tu ramo, de hecho. Pero ahora no, ahora ya tengo mi género. Con mi voz, con mi cara. Y lo más gracioso es que hablan todos los idiomas. Lo que sí, en inglés, yo hablo inglés, pero cuando la pongo mi género digital a hablar en inglés... Sí, algunas cosas, pero tiene un acento un poco indio, porque la voz te la capta, pero hace giros raros. Fíjate si tenemos ya una persona de la clase que se ha preparado para hacerlo. Sí, sí, ¿no? Sí, sí. ¿Me presentáis? No. Es TPM. Si es TPM, ¿te permitieran darlo online? Por ejemplo, como una profesora dijo que le tocó lo de la pandemia y tuvo que ver su test y se quedó. Doctora online. Si hubiese sido online, probablemente sí, un gemelo digital podría... ¿No se pone nervioso? ¿Con nerviosa? No, pues, los podcast no. Es que ahora es que... O sea, con Notebook LM en lo del podcast es increíble. Y me encanta. Al final es una herramienta que utilizas para estudiar y tú le das un temario y es capaz de hacer ese... Yo hice una demo el otro día en el Colegio de Economistas de esto justo y es capaz de, con un tema aleatorio, que tú le pongas el que sea, y te hace un podcast. Un debate, ¿no? Un debate, pero es que además graduando la voz, con distintos tonos, una mujer y un hombre. Y esa es la broma. Hace de todo. Y dices, ¿cómo es posible? Y gratis. Siempre es gratis al inicio, hasta que te hace adicto. Sí, sí, sí, sí. Esto viene de... No, no, no. Nada, seguimos. Este sería el calendario. Vale, y esta es la normativa de la Unión Europea que decía que es así, que es sancionable, que es el Reglamento General de Protección de Datos del año 2016. Entonces, es de la Unión Europea, pero digo que es una ley española, que lo que hace es prácticamente decir lo mismo que este reglamento, protege a las personas físicas en lo relativo al tratamiento de datos personales. Entonces, todo lo que tenga que ver con protección de datos, aquí tenemos una normativa a la que acudir. Licitud, minimización, esta actitud. Esto también es importante. Hay un caso que estuve el otro día leyendo, de una cosa que a mí me pasó hace tiempo. Yo me fui a Roma y cogí un apartamento y el dueño del apartamento me pidió que le mandara el DNI, no sé si eso ha pasado alguna vez, delante, por detrás, un selfie, de mi marido, de mis hijas y yo no sé, del perro casi. Entonces, toda la información. Bueno, pues una resolución muy reciente, diciendo que eso no puede ser, o sea, por la minimización, o sea, es una vulneración de la protección de datos, muy potente. Y lo de España acá, entonces, ¿es real o no es real? Porque dicen que habían sacado una norma en que los hoteles así podían pedir cuenta corriente, el NIE, toda la información, una cantidad de información muy grande. No, hay una regulación, una ley, que si te la lees te dice exactamente lo que te pueden pedir. La dirección y tal. Es verdad que el DNI, la parte de tu cara, sí que tienes que presentarlo, pero es por un tema de la policía, que tienes que estar vigilado, tienen que ver si estás en alguna base de datos. Entonces, eso sí, pero la parte de atrás del DNI no tenemos obligación. Y sobre todo lo importante creo yo que es que no lo pueden escanear. Exacto. El problema está en escanear. Eso es otra cosa. Es que luego hay unas consecuencias, pero yo soy la primera que... En principio me dijeron que esto no, pero luego lo hice, porque si no no me da... Claro. Entonces, esas cosas, pero esto es una resolución muy reciente. Hay muchas cosas cuando te pedían cosas así, porque yo era un poco reticente y le ponía marcas de agua, le ponía notas, esto solo está autorizado para este... Claro, claro, claro, sí, sí, para que no... Es que luego viene el problema de tener una cuenta o con un teléfono móvil, porque tienes todos tus datos y es tan fácil como una persona más particular, que es quien no se lo está dando a una institución o a una organización, no, es un particular que se está quedando con datos con los que puede abrir eso, una cuenta o comprar un móvil. Acceso, rectificación siempre, ¿no? El tema de la IA, poder acceder, rectificar mis datos, oponerme, la supresión y la portabilidad. Y luego este artículo, que es uno de los más importantes, el artículo 22, derecho a no ser objeto de decisiones automatizadas con efectos jurídicos sin intervención humana. En este artículo hay una serie de excepciones, la primera de ellas es dar el consentimiento, pero si no, no puede ser tomar decisiones de cosas que me puedan afectar de manera automática sin la intervención humana. Otras normas europeas, pues directrices, vuelvo otra vez a lo mismo, que vuelvo a decir que tiene que ser fiable, que tiene que ser justicia, supervisión, etc. Relación con el RIA en sistemas de IA que usan datos personales. Bueno, serían, como vuelvo a decir, no dejan de ser directrices. Y luego vamos a ver la normativa española, la Ley Orgánica de Protección de Datos Personales y Garantía de los Derechos Digitales. Esta es la ley orgánica que regula específicamente la protección de datos y añade además algo que no aparece en el reglamento de la Unión Europea, que son nuestros derechos digitales. Como por ejemplo, el derecho a la privacidad. ¿Qué es ese derecho a la privacidad? Que si hay algo que no te gusta a ti, que se publicó en su momento, puedas pedir que rectifiquen y lo saquen. Vale. Imagínate que ahora te dedicas a escribir algo sobre ti o lo que sea a día de hoy y dentro de diez años vuelven a sacarlo. ¿Eso es posible? ¿Vuelven a sacarlo? Sí, vuelven a sacarlo. Alguien rescata algo de ti y lo vuelve a escribir y lo vuelve a poner otra vez en redes y vuelves a salir. No estoy segura, pero me da la sensación que a lo mejor, aunque lo hicieran, puede que no esté penable que lo hagan. Pero tú sí que podrías pedir que lo sacaran. Pero no sé, la verdad, si se puede o no se puede hacer. Vale, pues eso también está regulado. El tema del consentimiento. Hay que renovar el consentimiento. Lo que no puede ser es que una cosa que haya sucedido hace tiempo, volvamos a sacarla dentro de X. Porque yo del consentimiento hace 20 años. Contaba que hubo un caso de un asesinato en los años 80 y una persona mató a dos militares y sacaron el tema hace nada en un periódico de Extremadura. Sacó otra vez la noticia, rememorando, y salió la foto de él. Y en ese momento, además, en el periódico salieron las fotos de las víctimas muertas y más. La cosa es que esta persona ya estaba casada, tenía hijos, no había contado absolutamente nada de su pasado. un poco pues reclamar y decir que ahora él no había dado el consentimiento. Entonces el periódico lo que dijo y la periodista es que en su momento él había consentido y de hecho fueron a prisión y le hicieron una entrevista. Entonces que él había consentido que eso saliera en el periódico en el año 85. ¿Eso validaba que ahora volviera a salir? Pues se dijo que no. Bueno, pues todo eso hay que saberlo. Ley orgánica. El reglamento de la Unión Europea del año 2018 y esta ley en España también del año 2018. Perdona, pero no me queda claro. ¿Los consentimientos tienen fecha de caducidad? No es que tengan fecha de caducidad, porque no dicen tres años, pero sí es verdad que hay una interpretación que tú lo que no puede ser es que desde el consentimiento ahora y luego dentro de 20 años se entienda que tú todavía estás de acuerdo. Se podría reclamarlo y ella sería el tenor de después verlo. Estoy pensando, digo, la cantidad de niños cuando se hagan grandes que van a reclamar a sus padres porque les han sacado las maderas y le digo, madre mía. Yo digo, si las maderas de pobre son hijos, las fotos que publican de los niños... Si el niño que salió en la portada de Nevermind de Nirvana demandó porque... Pues igual. Pensé lo que quería de dinero. Le enseñan la pichurga cuando quería la pasta. ¿Qué incluye esta ley? Las reglas sobre datos de menores, tratamiento por parte de las administraciones públicas y, como digo, incluye, que eso no está en la Unión Europea, el derecho al olvido, la educación digital y la desconexión digital. Introduce derechos como la neutralidad, la educación y la desconexión en el ámbito laboral. Entonces, importante para la IA, regula el tratamiento automatizado de datos, el perfilado y el uso de algoritmos con implicaciones jurídicas. Y después tenemos el Real Decreto de Creación... Este organismo, que se llama Agencia Española de Supervisión de la Inteligencia Artificial, es el primer organismo público que tenemos, europeo, específico para la IA. Tiene su sede en La Coruña y una serie de funciones. Si entráis en la página vais a ver que ahora no hay actividad, no hay nada, pero que dentro de un tiempo probablemente sí lo tengamos. Yo siempre os comento temas de Buscan Trabajo, de las empresas... Esta mañana hemos estado hablando con Cristina justo de eso, que están buscando gente. No sé si alguno de vosotros lo ha mirado, no sé si necesitas tener la nacionalidad, pero es una cosa... Yo cuando me lo ha dicho digo, ostras, yo no había caído. Igual sí que están buscando algún perfil como el vuestro. Entonces, echar un vistazo a ver en esta, en concreto, la AESIA. A ver si lo necesitan. El año pasado buscaban todos los perfiles y todavía si entráis en la página yo creo que están pensando... Echarle un vistazo, a lo mejor tienen algún newsletter donde os podéis apuntar para recibir en caso de que tengan alguna oferta, que a lo mejor ahora no la tienen y dentro de seis meses la vuelven a sacar. Pero es interesante que estéis... Además, en estas agencias una vez que metes la cabeza, con un perfil que conoces de inteligencia artificial puede ser muy interesante. Una cosa, perdona, nos quedan unos treinta minutillos para ir luego tranquilos. ¿Cómo lo ves? ¿Muy apurado? Voy a dar un poquillo de... Vale, vamos a los puntos claves si quieres. Esto lo termino y ya empiezo con el siguiente. Eso, en la agencia esta y también en la de protección de brazos. Van a seguir surgiendo porque van a necesitar personas que sean expertas en inteligencia artificial sobre todo gente que esté en organismos públicos. Supervisa, por tanto, la inteligencia de alto riesgo, coordina la aplicación que se cumpla este reglamento y fomentar el desarrollo ético y transparente de la IA. Servir como punto nacional de referencia de la gobernanza. Hay una oficina en la Unión Europea y en los diferentes países se están creando este tipo de agencias y luego va a haber una coordinación desde arriba. La nuestra está aquí en La Coruña. De hecho, la página está en gallego. Estrategia Nacional de Inteligencia Artificial. Publicada por el gobierno de España en diciembre de 2020. Es una estrategia que lo que quiere es lo mismo. La protección, la promoción, el uso. Se marca dentro del plan digital del año 2026. Se pasa un poco rápido, pero que sepáis que también por aquí se están haciendo cosas. Otros marcos de referencia. Guías, proyecto de carta de derechos digitales y una normativa sectorial. Vamos a ver, y eso lo vamos a ver desde ya, cómo las diferentes leyes, aunque reúnen otra cosa distinta, vamos a empezar a introducir inteligencia artificial. Va a haber seguramente modificación de diferentes normativas concretas y ya vamos a decir, inteligencia artificial, ¿no? Algo relacionado con eso. Bueno, pues aquí ya vemos la ley de contratos del sector público que regula esa contratación. Basada en IA, Estados Unidos en el año 2017. Y normas sobre ciberseguridad como el esquema nacional de seguridad. ¿Existe un proyecto de ley? Sí, actualmente no se ha presentado ese proyecto de ley de inteligencia artificial aquí en España, pero el gobierno ya ha anunciado su intención de elaborarlo. Entonces, ¿qué pasos han dado? Tenemos una base que sería la estrategia nacional de inteligencia artificial, la creación de esta oficina que he comentado y los planes normativos que tiene el ministerio. Entonces, se va moviendo y dentro de más o menos tiempo vamos a ver el resultado. Vamos al siguiente y ya solamente falta hablar de los cuatro niveles que establece el reglamento en cuanto a los riesgos. De todas maneras, el más importante yo creo que es el primero. Si no da tiempo a todo, no pasa nada. Este, sí, este. Riesgo inaceptable, alto, limitado y mínimo. Empezamos por el primero. Bueno, primero, ¿quién interpreta el riesgo? Que eso lo que he dicho, todavía estamos con algo muy genérico, pero vamos a ver cómo empezamos ya a tener interpretaciones. La Comisión Europea, los considerandos que decíamos de reglamento. Después me equivoco siempre en la ley y digo artículo y todos me dicen son considerandos, digo. ¿Qué son considerandos? Pues es parecido, pero no. Bueno, pues está la Comisión Europea, después está el grupo de expertos y comités que también están interpretando qué es lo que quiere decir el reglamento, Parlamento y Consejo de la Unión Europea, la doctrina académica, las ONG, vamos a ver cómo hay instituciones que se van moviendo. Aquí cito a juristas como Cotino, que lo tenéis aquí en la Universidad de Valencia y tribunales, el TEJU, el Tribunal de Justicia Europeo, que también va a empezar a interpretar. ¿Por qué? Porque es un reglamento de la Unión Europea y al final es el Tribunal de la Unión Europea el que tiene también algo que decir. Y, por último, lo que os he dicho, cuando hay una vulneración de derechos fundamentales o de derechos en general, los tribunales de cada uno de los países. Ideas claves, que un sistema no esté prohibido ni clasificado como alto riesgo no implica que esté permitido, su legalidad depende de otras normas, lo que he dicho, conectar diferentes normativas, no centrarnos en no, no hay regulación, ¿vale? El RIA no otorga por sí solo una base legal para el tratamiento de los datos personales si se requiere una base jurídica adicional, consentimiento, el interés público y que busca proteger los derechos sin frenar la innovación. Eso es lo que quiere y por eso es tan amplio. Aquí por poner una puntualización que creo es muy importante, entonces, si nos tuviésemos que imaginar las leyes aplicables, ¿no? Sería una circunferencia RGPD, dentro el EUPD-GDD y dentro ya estaría, realmente no estaría como tal la RIA, pero digamos que es más concreta aún, ¿vale? Pero la madre del cordero aquí, por lo que hablábamos, es la RGPD, sería algo así, sería el equivalente a lo que vemos muchas veces, lo que explicábamos al principio de AGI, no sé qué, pues algo parecido, ¿sabes? Entonces, el marco global es RGPD dentro del EUPD, porque es la española en concreto, y ya dentro la parte que en algún momento estará la traslación del reglamento, que no digo ley, ya me voy, el reglamento del RIA estará trasladado, y sería otro dentro más de ese, ¿vale? Porque tengáis en concreto. Se llegará el momento en que tengamos la Ley de Inteligencia Artificial Española y la Ley de Protección de Datos, y son las dos con las que vamos a jugar. Habrá cosas que tengamos que acudir a esta y otras que se regularán por la ley de inteligencia. Bueno, prohibido, manipulación, daño significativo y prohibiciones. El artículo 5 del reglamento lo que dice es, hay acciones que están prohibidas. Y dices, ¿tu prohibido es prohibido? Pues no, prohibido es con excepciones. Entonces, vamos a ver cómo alto riesgo sería prohibido con excepciones y hay determinadas actuaciones que parecen prohibidas pero que luego van a pasar al segundo nivel, son de alto riesgo. Bueno, pues lo primero la manipulación, es un término, manipulación. ¿Cuándo nos están manipulando? ¿Vale? Queda muy abierto. ¿Cuándo realmente, bueno, pues no estamos siendo conscientes de lo que estamos haciendo? Pues complicado pero también es cuestión de interpretación, ¿no? Cuando nos están vendiendo un producto y nos están dejando libertad para decidir o no y cuando realmente nos están quitando la conciencia y casi casi nos están diciendo, cómpralo, bien. Exacto. Bueno, importante respecto a este primero, daño potencial. Basta con que sea previsible, esto es muy importante. Lo que se pretende es que se compruebe que hay un daño, daño verificable. Hay una posibilidad de daño, no quiere decir que tenga que haber una víctima, ¿vale? Sino que hay una IA que me está manipulando o que puede manipularme, ¿vale? Daño potencial, basta con que sea previsible y evaluado prospectivamente. La manipulación se distingue de la persuasión porque atenta contra la autonomía. Como digo, es una cuestión de interpretación. El daño puede ser físico, psicológico o económico. Se prohíben sistemas que superen el umbral de significatividad del daño previsto. Vale, aquí ponía un ejemplo el entrenador emocional en el aula. Una empresa tecnológica que diseña un sistema de inteligencia artificial que detecta expresiones faciales, tonos de voz, etcétera, de cada estudiante. Entonces sabe si tiene frustración, miedo. El objetivo es personalizar la enseñanza, mejorar la motivación. Entonces, las preguntas que nos podemos hacer es si esto es legítimo, ¿no? ¿Puede considerarse como una forma de manipulación emocional? ¿Existe daño potencial significativo? ¿Cómo distinguir persuasión de manipulación? ¿Cómo debe actuar el centro educativo? Como no da tiempo, no vamos a hacer el debate, pero os voy a poner las posibles respuestas, ¿vale? En el PowerPoint. Prohibición, esto es lo que estabas contando. Sí, es social. Corín. Prohibido evaluar personas según su conducta social o rasgos personales. Si causa un perjuicio o discriminación, si se usa fuera del contexto, si es desproporcionado. Y lo que dije antes, cruzar datos fiscales con redes sociales para negar. Estamos dentro del mismo ámbito. No pasa nada, en principio, pero sí lo utilizo para otros ámbitos, sí. En el TEDxChina se le aplica a los niños también la guía para saber si están atentos en clases, el reconocimiento. También, reconocimiento. Para saber si están atendiendo clases o no. Entonces, digamos que al estar en el aula no es una opción para los niños atender o no. Deben estar atentos. Lo vamos a aplicar aquí el año que viene. Bueno, pues este sería un segundo ejemplo. Os lo dejo. Seguimos. Después, el reglamento prohíbe sistemas de IA que evalúen personas con base a su comportamiento social. Siempre que puedan producir efectos adversos significativos y aquí pongo los artículos desde el anterior. Tercero, prohibición de predicción delictiva por perfiles, también interesante. Prohibido predecir delitos basándose sólo en perfiles personales. Todos los rubios son malísimos, no. Lo que sí se permite es, si yo antes he cometido delitos, hay una base y ya hay una predisposición a que lo vuelva a cometer. Eso sí, por hechos, pero no porque soy mujer, hombre o lo que sea, ¿vale? Se requiere base fáctica verificable, conductas pasadas, datos empíricos y se protege el principio de presunción de inocencia, pues tenemos derecho. A mí me hace mucha gracia lo de la presunción de inocencia porque yo siempre lo digo, digo el que denuncia tiene que decir la verdad y si tú denuncias un delito, siempre tienes que ir por la verdad por delante. En el caso de que no lo hagas, pueden deducir para su testimonio y te pueden denunciar a ti. Pero el que ha cometido un delito puede mentir y lo primero que se dice es, usted puede mentir. Entonces, la presunción de inocencia implica que yo no tengo por qué decir la verdad. Prohibición, ya. Ese sí que es un buen menón. Cuando lo comentan mis alumnos, ya estamos una hora hablando de eso. Justo, tal, tal, tal. Lo más que nada, porque a mis hijas les digo, no hay que mentir. Ahora, el día que vas a juicio, ese sí puedes. Yo me tomo nota, eh. Vamos a borrar esta parte del video. Alguien le dijo las puestas y yo tenía que mentir. Digamos que, sin que sea un delito esto, podría tener problemas con la identificación de simuladores en el caso del DFM. Identificación de simuladores. Por ejemplo, la idea del DFM es poder sacar la línea de que en realidad, por la encefalografía está enfermo, está potencialmente enfermo, va o tiene un inicio de enfermedad por las características de contracción y de la parte cognitiva. Y también identificar simuladores que dicen estar enfermos y que no se identifica en la retomografía y en la encefalografía. En el caso, son personas que acuden a ti. No, no, porque son mediciones de la empresa. Entonces, por ejemplo, vamos a tomar un grupo de personas. Entonces, el grupo de personas están. Las personas enfermas por enfermedad, por ejemplo, síndrome del cuerpo o manito rotado. Entonces hay una característica de contracción y una característica encefalográfica que va a demostrar activación de zonas de dolor. Las personas que son simuladoras son las que en teoría no están enfermas, pero simulan la enfermedad y están en todo su manejo clínico. Pero la característica de contracción electromagráfica y la característica encefalográfica no debería generar la marcación para decir están enfermas. Entonces ahí se identificaría el simulador. Entonces, en ese orden de ideas, podría haber... El simulador es el estafador, en realidad. O sea, el estafador es el tipo que dice me duele el hombro y no me duele. Viendo esto, uno podría decir, venga, hay un riesgo de que ellos pagan de si no vengas. O sea, algo que jurídicamente ellos pagan de si no vengas. Es que están... Vulnerando mis derechos. Están vulnerando mis derechos. Como TFM lo puedes hacer. Esto es un tema más de un estudio, de investigación y tal. Ahí tienes mano ancha. Ahora, el problema es si la empresa toma una decisión. Y yo entiendo que esto sería para estudiarlo. Publicar. Una cosa. Bajo el TFM, al final, tú puedes ponerlo. Anonimizar la empresa. Y tú puedes hacer un estudio sin que tenga consecuencias para el trabajador en sí. Yo he hecho ese estudio. He intentado que un 20% de los trabajadores simulaban que estaban enfermos cuando no lo estaban. Y no pasa nada. Tú no puedes poner nombres. Y la empresa, si toma una decisión en base a ese estudio que has hecho tú, es responsabilidad de la empresa. Pero tampoco legalmente. Una jueza, legalmente, no va a tener... Tiene que ir a por otras cosas legales. No a por qué me ha dicho una herramienta que tal. Eso no va a valer para nada. En ese caso. Y tú no vas a tener problemas legales por hacerlo. Pero, ojo, tienes que anonimizarlo todo. Yo entiendo que eso sí que lo tienes claro. Y ya está. Bueno. Prohibición de crear base de datos de reconocimiento facial con imágenes extraídas masivamente. Solo se permite recolección selectiva y justificada. ¿Vale? Eso también va a estar prohibido. Eso de hacer base de datos teniendo en cuenta todo lo que estamos subiendo en las redes. ¿Vale? ¿Eso va a estar prohibido o está prohibido? Prohibido. ¿Ya está o está? Ya está, ya está. O sea, eso está prohibido. Lo vamos a borrar. He puesto en el chat la imagen de las fechas. En el chat nuestro de la reunión. ¿Vale? Veréis que pone el calendario exactamente lo que está prohibido. Y, efectivamente, esto está prohibido el 2 de febrero de 2025. Prohibición efectiva de la IA de riesgo inaceptable. ¿Vale? Inferencia emocional en trabajo y educación. Prohibición de usar IA para inferir emociones en contextos laborales o educativos salvo para fines médicos y seguridad vital. En principio, que nos pongan una cámara en principio no está prohibido se excluyen encuestas o sistemas sin uso de datos biométricos. La empresa que instala el sistema de inteligencia artificial y en las reuniones dice con la excusa de mejorar el bienestar emocional y de prevenir conflictos pues empieza a etiquetar a los empleados como estresado, aburrido en tiempo real. Pues esto no se puede. Categorización biométrica prohibida. No se puede clasificar individualmente por raza, religión, orientación sexual. Solo afecta si se usa biometría para inferir datos sensibles y no afecta análisis colectivo ni usos técnicos correctivos. Esto está relacionado con el siguiente a ver bueno Bueno, pongo este ejemplo. Una empresa privada instala cámaras en la entrada de un gran recinto y el sistema, usando biometría facial y corporal, intenta inferir datos como la orientación sexual. Se puede, en general, al grupo en concreto si se va a permitir, cuando es algo muy genérico, pero cuando está individualizando, pues entiende que hay una desprotección. Entonces, esto es lo que está prohibido. Las personas dicen que no han dado su consentimiento. Esto también es importante. En la extensión es cuando das el consentimiento. Entonces, aquí pongo, ¿está permitido clasificar a las personas? Pues no. Prohíbe expresamente usando la biometría. Y está relacionado con esto, que es el reconocimiento biométrico en tiempo real. Las cámaras que nos van captando a tiempo real prohíbe uso policial en tiempo real en espacios públicos, a distancia y sin consentimiento. Y solo está permitido en casos concretos que aparecen en la diapositiva siguiente. Entonces, lo que no podemos es estar localizados a tiempo real. Aquí, por ejemplo, dice una manifestación, la policía instala cámaras, pues para evitar conflictos y demás. Prohibido. ¿Pero la Policía Nacional no lo utiliza, por ejemplo, en el aeropuerto? El reconocimiento es facial, pero no para buscar… O sea, lo que quiero decir es que… ¿A qué te refieres? Bueno, ahí uno está dando consentimiento de una manera… Las cámaras las tenemos en todas partes. Pero permitido solamente si hay un interés público sustancial. Es estrictamente necesario por búsqueda de víctimas, prevención de atentados, identificación de delincuentes graves, penado con más de cuatro años. Entonces, sí, mira en todas las cámaras. Podrías aplicar IA para mirar en todas las cámaras. Tienen que ser solamente los casos específicamente previstos en ese reglamento. El resto no puede. Pasamos al siguiente. ¿Es estrictamente necesario? ¿Qué es estrictamente necesario para usar? ¿Quién define esa…? La justicia. No, pero cuando ya se ha triplicado. La policía. Vamos a ver. Si no ya, lo dejamos aquí y yo os paso las diapositivas, ¿vale? Vale, sigue. Eso es cuando ya se ha triplicado. Te pongo un ejemplo, ¿vale? Me viene la policía al juzgado y me dice, mire, es que hay una niña que ha puesto las redes sociales que se va a quitar por la ventana. ¿Me da autorización para entrar en la casa? Perdona, ¿a ti te parece que es riesgo? ¿Tú crees que la policía me tiene que pedir permiso a mí? ¡Cierra la puerta! A eso es a lo que estamos refiriendo, ¿vale? ¿Quién decide esto? Pues en el caso concreto, si hay un secuestro de un niño, la policía lo que tiene que hacer es activarlo. Lo que no puede ser es que pidas autorización o digas, no, pues si acaso está prohibido. Los dos casos son distintos porque el segundo, para mí, está muy justificado porque el niño fue secuestrado. Hay un delito y tú buscas al responsable y recupera al niño. Pero el primero no ha pasado nada aún. Hay una amenaza o un mensaje en las redes sociales. No, pero entendí que amenaza con tirarse, eso es lo que entendí. Pero no ha sucedido. Tú no sabes si se va a tirar, no sabes si el mensaje lo envió ella y si era una joda, era una broma de algún amigo. ¿Solo por tener la amenaza? Te tiran la puerta de tu casa, estás en una situación... Ya no hace más esa broma. Lo que digo es que para mí son casos distintos porque el delito está consumado. En el otro sospecha. El secuestro no es tal secuestro. Al final resulta que se había liado la cosa pero era el padre que se llevaba al niño de excursión. Sí, ha pasado. Yo he hecho varios casos de llamadas. El 911, que el niño llamó porque la mamá le estaba pegando y era que no le dejó comer nada. Y la policía llegó a la casa. Claro, pero es que en ese caso la policía... Hay un riesgo. La cosa es justificarlos. Luego tienes que hacer un informe diciendo que en ese momento no podía hacer otra cosa. En ese caso, por pedirme permiso, la niña se tiró por la ventana. Es que eso no puede ser. Lo vamos a dejar aquí. Yo os paso esa diapositiva. ¿Diapositiva? ¡Grigorio, qué tal! Vale pues... ...muchas gracias, espectacular. Ha sido corto, pero ha sido un esfuerzo, a darla en presencial... La próxima sea en Canarias. Lo mismo le enseño yo, de tener la próxima reunión allí. Y nada, tenemos que hacer una foto oficial ya que estás aquí, para acabar. No sé si Grigorio puede salir desde ahí, o... ...hacemos algo de la Universidad Europea que tengamos. Yo creo que podemos ir variando la escenografía. Sí, sí. ¿Sí? ¿Vamos a la otra? Vale. Vale, pues ya... ¡Grigorio, hablamos! Muy bien, ¡chao! Subtítulos realizados por la comunidad de Amara.org